# ç¬¬äºŒç« ï¼šé¡¹ç›®ç»“æ„åˆ†æ

## ğŸ“š æœ¬ç« ç›®æ ‡

é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œä½ å°†æ·±å…¥ç†è§£ï¼š
- nano-vLLMçš„å®Œæ•´ç›®å½•ç»“æ„
- æ¯ä¸ªæ–‡ä»¶å’Œæ¨¡å—çš„å…·ä½“èŒè´£
- å…³é”®ä»£ç çš„é€è¡Œåˆ†æ
- æ¨¡å—é—´çš„ä¾èµ–å…³ç³»å’Œæ•°æ®æµ
- é¡¹ç›®çš„å¯åŠ¨æµç¨‹å’Œåˆå§‹åŒ–è¿‡ç¨‹
- æ ¸å¿ƒç»„ä»¶çš„å·¥ä½œæµç¨‹å’Œæ•°æ®æµå‘
- KV Cacheå’ŒPrefix Cachingçš„ä¼˜åŒ–æœºåˆ¶

---

## ğŸ—‚ï¸ é¡¹ç›®å®Œæ•´ç›®å½•ç»“æ„

é¦–å…ˆè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹é¡¹ç›®çš„å®Œæ•´ç›®å½•æ ‘ï¼š

```
nano-vllm-main/
â”œâ”€â”€ nanovllm/                    # ä¸»åŒ…ç›®å½•
â”‚   â”œâ”€â”€ __init__.py             # åŒ…åˆå§‹åŒ–æ–‡ä»¶ï¼Œå®šä¹‰å¯¹å¤–æ¥å£
â”‚   â”œâ”€â”€ config.py               # é…ç½®ç±»å®šä¹‰
â”‚   â”œâ”€â”€ llm.py                  # ä¸»LLMç±»ï¼Œç”¨æˆ·å…¥å£
â”‚   â”œâ”€â”€ sampling_params.py      # é‡‡æ ·å‚æ•°ç±»
â”‚   â”œâ”€â”€ engine/                 # æ¨ç†å¼•æ“æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ block_manager.py    # KV Cacheå—ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ llm_engine.py       # LLMå¼•æ“æ ¸å¿ƒ
â”‚   â”‚   â”œâ”€â”€ model_runner.py     # æ¨¡å‹è¿è¡Œå™¨
â”‚   â”‚   â”œâ”€â”€ scheduler.py        # è¯·æ±‚è°ƒåº¦å™¨
â”‚   â”‚   â””â”€â”€ sequence.py         # åºåˆ—ç®¡ç†
â”‚   â”œâ”€â”€ models/                 # æ¨¡å‹å®ç°
â”‚   â”‚   â””â”€â”€ qwen3.py           # Qwen3æ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ layers/                 # åŸºç¡€å±‚å®ç°
â”‚   â”‚   â”œâ”€â”€ activation.py       # æ¿€æ´»å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ attention.py       # æ³¨æ„åŠ›æœºåˆ¶
â”‚   â”‚   â”œâ”€â”€ embed_head.py      # åµŒå…¥å’Œè¾“å‡ºå¤´
â”‚   â”‚   â”œâ”€â”€ layernorm.py       # å±‚å½’ä¸€åŒ–
â”‚   â”‚   â”œâ”€â”€ linear.py          # çº¿æ€§å±‚
â”‚   â”‚   â”œâ”€â”€ rotary_embedding.py # æ—‹è½¬ä½ç½®ç¼–ç 
â”‚   â”‚   â””â”€â”€ sampler.py         # é‡‡æ ·å™¨
â”‚   â””â”€â”€ utils/                  # å·¥å…·æ¨¡å—
â”‚       â”œâ”€â”€ context.py          # ä¸Šä¸‹æ–‡ç®¡ç†
â”‚       â””â”€â”€ loader.py           # æ¨¡å‹åŠ è½½å™¨
â”œâ”€â”€ UnderstandArch/             # æœ¬æ–‡æ¡£ç›®å½•
â”œâ”€â”€ assets/                     # èµ„æºæ–‡ä»¶
â”‚   â””â”€â”€ logo.png               # é¡¹ç›®Logo
â”œâ”€â”€ bench.py                   # æ€§èƒ½æµ‹è¯•è„šæœ¬
â”œâ”€â”€ example.py                 # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ LICENSE                    # å¼€æºè®¸å¯è¯
â”œâ”€â”€ pyproject.toml             # é¡¹ç›®é…ç½®æ–‡ä»¶
â””â”€â”€ README.md                  # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

## ğŸ”„ nano-vLLMæ•´ä½“æ¶æ„æµç¨‹å›¾

### ç³»ç»Ÿæ•´ä½“å·¥ä½œæµç¨‹

```mermaid
graph TD
    A[ç”¨æˆ·è¾“å…¥Prompt] --> B[LLM.generateè°ƒç”¨]
    B --> C[LLMEngineåˆå§‹åŒ–]
    C --> D[åˆ›å»ºå¤šä¸ªè¯·æ±‚Sequence]
    D --> E[Schedulerè°ƒåº¦]
    E --> F{æœ‰ç­‰å¾…çš„Sequence?}
    F -->|æ˜¯| G[Prefillé˜¶æ®µ<br/>å¤„ç†æ–°è¯·æ±‚]
    F -->|å¦| H[Decodeé˜¶æ®µ<br/>å¤„ç†è¿è¡Œä¸­è¯·æ±‚]
    G --> I[BlockManageråˆ†é…KV Cache]
    H --> I
    I --> J[ModelRunneræ‰§è¡Œå‰å‘è®¡ç®—]
    J --> K[æ¨¡å‹æ¨ç†Qwen3ForCausalLM]
    K --> L[é‡‡æ ·ç”Ÿæˆæ–°token]
    L --> M[æ›´æ–°SequenceçŠ¶æ€]
    M --> N{æ‰€æœ‰Sequenceå®Œæˆ?}
    N -->|å¦| E
    N -->|æ˜¯| O[è¿”å›ç”Ÿæˆç»“æœ]
    O --> P[ç”¨æˆ·è·å¾—è¾“å‡ºæ–‡æœ¬]
```

### æ ¸å¿ƒç»„ä»¶äº¤äº’æµç¨‹

```mermaid
sequenceDiagram
    participant U as User
    participant L as LLM/LLMEngine
    participant S as Scheduler
    participant BM as BlockManager
    participant MR as ModelRunner
    participant M as Qwen3Model

    U->>L: generate(prompts, params)
    L->>S: add_request(sequence)

    loop Main Generation Loop
        L->>S: schedule()
        S->>S: Prefillé˜¶æ®µ: æŒ‘é€‰æ–°è¯·æ±‚
        S->>BM: allocate(sequence)
        BM->>BM: æ£€æŸ¥Prefix Caching
        BM-->>S: åˆ†é…å—IDåˆ—è¡¨

        S->>S: Decodeé˜¶æ®µ: å¤„ç†è¿è¡Œä¸­è¯·æ±‚
        S-->>L: è¿”å›å¾…å¤„ç†åºåˆ—åˆ—è¡¨

        L->>MR: run(sequences, is_prefill)
        MR->>M: model(input_ids, kv_cache)
        M-->>MR: è¿”å›logits
        MR->>MR: sampler(logits, temperature)
        MR-->>L: è¿”å›æ–°token IDs

        L->>S: postprocess(sequences, tokens)
        S->>BM: update sequence blocks
        S->>S: æ›´æ–°åºåˆ—çŠ¶æ€

        L->>L: æ£€æŸ¥æ˜¯å¦å®Œæˆ
    end

    L-->>U: è¿”å›ç”Ÿæˆç»“æœ
```

### æ•°æ®æµå‘å›¾

```mermaid
flowchart LR
    A[åŸå§‹æ–‡æœ¬] --> B[Tokenizer]
    B --> C[input_ids]
    C --> D[Schedulerè°ƒåº¦]
    D --> E[æ‰¹å¤„ç†é˜Ÿåˆ—]
    E --> F[ModelRunner]
    F --> G[Qwen3æ¨¡å‹å‰å‘è®¡ç®—]
    G --> H[æ³¨æ„åŠ›è®¡ç®—]
    H --> I[KV Cacheè¯»å†™]
    I --> J[è¾“å‡ºlogits]
    J --> K[é‡‡æ ·å™¨]
    K --> L[new_token_ids]
    L --> M[æ›´æ–°åºåˆ—]
    M --> N{ç”Ÿæˆé•¿åº¦è¾¾æ ‡?}
    N -->|å¦| E
    N -->|æ˜¯| O[æ”¶é›†å®Œæ•´è¾“å‡º]
    O --> P[Detokenizer]
    P --> Q[æœ€ç»ˆæ–‡æœ¬è¾“å‡º]
```

## ğŸ”§ æ ¸å¿ƒé…ç½®æ–‡ä»¶åˆ†æ

### pyproject.toml - é¡¹ç›®å…ƒæ•°æ®å’Œä¾èµ–

```toml
[build-system]
requires = ["setuptools>=61"]
build-backend = "setuptools.build_meta"

[project]
name = "nano-vllm"
version = "0.2.0"
authors = [{ name = "Xingkai Yu" }]
license = "MIT"
license-files = ["LICENSE"]
readme = "README.md"
description = "a lightweight vLLM implementation built from scratch"
requires-python = ">=3.10,<3.13"
dependencies = [
    "torch>=2.4.0",          # PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
    "triton>=3.0.0",         # GPUæ ¸å‡½æ•°ä¼˜åŒ–åº“
    "transformers>=4.51.0",  # HuggingFaceæ¨¡å‹åº“
    "flash-attn",            # Flash Attentionä¼˜åŒ–
    "xxhash",                # å¿«é€Ÿå“ˆå¸Œç®—æ³•
]
```

**é€è¡Œåˆ†æ**ï¼š

1. **`torch>=2.4.0`**:
   - PyTorchæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€æ¡†æ¶
   - éœ€è¦é«˜ç‰ˆæœ¬æ”¯æŒæœ€æ–°çš„GPUä¼˜åŒ–ç‰¹æ€§

2. **`triton>=3.0.0`**:
   - OpenAIå¼€å‘çš„GPUç¼–ç¨‹è¯­è¨€
   - ç”¨äºç¼–å†™é«˜æ•ˆçš„GPUæ ¸å‡½æ•°
   - nano-vLLMç”¨å®ƒæ¥ä¼˜åŒ–æŸäº›è®¡ç®—æ“ä½œ

3. **`flash-attn`**:
   - Flash Attentionå®ç°
   - å¤§å¹…æå‡æ³¨æ„åŠ›è®¡ç®—æ•ˆç‡
   - å‡å°‘å†…å­˜ä½¿ç”¨

4. **`xxhash`**:
   - æå¿«çš„éåŠ å¯†å“ˆå¸Œç®—æ³•
   - ç”¨äºPrefix Cachingä¸­çš„å“ˆå¸Œè®¡ç®—

---

## ğŸ“¦ ä¸»åŒ…å…¥å£åˆ†æ

### nanovllm/__init__.py - åŒ…çš„å…¬å…±æ¥å£

```python
from nanovllm.llm import LLM
from nanovllm.sampling_params import SamplingParams
```

**ä»£ç åˆ†æ**ï¼š
- è¿™åªæœ‰ä¸¤è¡Œä»£ç ï¼Œä½†éå¸¸å…³é”®
- å®šä¹‰äº†åŒ…çš„å…¬å…±æ¥å£ï¼Œç”¨æˆ·åªéœ€è¦å¯¼å…¥è¿™ä¸¤ä¸ªç±»
- `LLM`: ä¸»è¦çš„æ¨ç†å¼•æ“ç±»
- `SamplingParams`: é‡‡æ ·å‚æ•°é…ç½®ç±»

**è®¾è®¡æ€æƒ³**ï¼šéšè—å†…éƒ¨å¤æ‚æ€§ï¼Œç”¨æˆ·åªéœ€è¦çŸ¥é“è¿™ä¸¤ä¸ªç±»å°±èƒ½ä½¿ç”¨æ•´ä¸ªç³»ç»Ÿã€‚

### nanovllm/llm.py - ç”¨æˆ·ä¸»å…¥å£

```python
from nanovllm.engine.llm_engine import LLMEngine


class LLM(LLMEngine):
    pass
```

**ä»£ç åˆ†æ**ï¼š
- çœ‹èµ·æ¥å¾ˆç®€å•ï¼Œåªæ˜¯ç»§æ‰¿äº†`LLMEngine`
- å®é™…ä¸Šè¿™æ˜¯ä¸€ä¸ª"å¤–è§‚æ¨¡å¼"è®¾è®¡
- ç”¨æˆ·ç›´æ¥ä½¿ç”¨`LLM`ç±»ï¼Œä½†å®é™…åŠŸèƒ½éƒ½åœ¨`LLMEngine`ä¸­å®ç°
- è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼š
  - ç®€åŒ–ç”¨æˆ·æ¥å£
  - å°†å®ç°ç»†èŠ‚å°è£…åœ¨å†…éƒ¨
  - ä¾¿äºå°†æ¥æ‰©å±•æˆ–é‡æ„

---

## âš™ï¸ é…ç½®ç³»ç»Ÿæ·±åº¦è§£æ

### nanovllm/config.py - é…ç½®ç®¡ç†

```python
import os
from dataclasses import dataclass
from transformers import AutoConfig


@dataclass
class Config:
    model: str                                      # æ¨¡å‹è·¯å¾„
    max_num_batched_tokens: int = 16384            # æœ€å¤§æ‰¹å¤„ç†tokenæ•°
    max_num_seqs: int = 512                         # æœ€å¤§å¹¶å‘åºåˆ—æ•°
    max_model_len: int = 4096                       # æ¨¡å‹æœ€å¤§é•¿åº¦
    gpu_memory_utilization: float = 0.9            # GPUå†…å­˜ä½¿ç”¨ç‡
    tensor_parallel_size: int = 1                   # å¼ é‡å¹¶è¡Œå¤§å°
    enforce_eager: bool = False                     # å¼ºåˆ¶eageræ¨¡å¼
    hf_config: AutoConfig | None = None            # HuggingFaceé…ç½®
    eos: int = -1                                   # ç»“æŸtoken ID
    kvcache_block_size: int = 256                   # KV Cacheå—å¤§å°
    num_kvcache_blocks: int = -1                   # KV Cacheå—æ•°é‡

    def __post_init__(self):
        assert os.path.isdir(self.model)            # ç¡®ä¿æ¨¡å‹è·¯å¾„å­˜åœ¨
        assert self.kvcache_block_size % 256 == 0   # å—å¤§å°å¿…é¡»æ˜¯256çš„å€æ•°
        assert 1 <= self.tensor_parallel_size <= 8  # å¹¶è¡Œå¤§å°é™åˆ¶
        self.hf_config = AutoConfig.from_pretrained(self.model)  # åŠ è½½HFé…ç½®
        self.max_model_len = min(self.max_model_len, self.hf_config.max_position_embeddings)  # è°ƒæ•´æœ€å¤§é•¿åº¦
        assert self.max_num_batched_tokens >= self.max_model_len  # æ‰¹å¤„ç†å¤§å°é™åˆ¶
```

**å…³é”®å‚æ•°è¯¦è§£**ï¼š

1. **`max_num_batched_tokens = 16384`**
   - ä¸€æ¬¡æœ€å¤šå¤„ç†å¤šå°‘ä¸ªtoken
   - å½±å“å†…å­˜ä½¿ç”¨å’Œå¤„ç†é€Ÿåº¦
   - è¶Šå¤§è¶Šå¥½ï¼Œä½†å—GPUå†…å­˜é™åˆ¶

2. **`max_num_seqs = 512`**
   - åŒæ—¶å¤„ç†çš„æœ€å¤§è¯·æ±‚æ•°
   - å½±å“å¹¶å‘èƒ½åŠ›
   - éœ€è¦åœ¨å†…å­˜å’Œå»¶è¿Ÿé—´å¹³è¡¡

3. **`kvcache_block_size = 256`**
   - KV Cacheçš„å—å¤§å°
   - 256æ˜¯ä¸€ä¸ªç»éªŒæœ€ä¼˜å€¼
   - å½±å“å†…å­˜åˆ†é…æ•ˆç‡

4. **`tensor_parallel_size`**
   - å¼ é‡å¹¶è¡Œçš„GPUæ•°é‡
   - 1è¡¨ç¤ºå•GPU
   - 8è¡¨ç¤ºæœ€å¤š8ä¸ªGPUå¹¶è¡Œ

**é…ç½®éªŒè¯é€»è¾‘**ï¼š
- `__post_init__`æ–¹æ³•åœ¨å¯¹è±¡åˆ›å»ºåè‡ªåŠ¨è°ƒç”¨
- è¿›è¡Œå„ç§å‚æ•°åˆæ³•æ€§æ£€æŸ¥
- è‡ªåŠ¨åŠ è½½HuggingFaceé…ç½®
- è°ƒæ•´ä¸åˆç†çš„å‚æ•°å€¼

---

## ğŸ² é‡‡æ ·å‚æ•°ç³»ç»Ÿ

### nanovllm/sampling_params.py - é‡‡æ ·é…ç½®

```python
from dataclasses import dataclass


@dataclass
class SamplingParams:
    temperature: float = 1.0                         # æ¸©åº¦å‚æ•°
    max_tokens: int = 64                            # æœ€å¤§ç”Ÿæˆtokenæ•°
    ignore_eos: bool = False                        # æ˜¯å¦å¿½ç•¥ç»“æŸtoken

    def __post_init__(self):
        assert self.temperature > 1e-10, "greedy sampling is not permitted"
```

**å‚æ•°è¯¦è§£**ï¼š

1. **`temperature`**
   - æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§
   - 1.0: æ ‡å‡†éšæœºæ€§
   - 0.1: è¶‹å‘ç¡®å®šæ€§ï¼ˆç±»ä¼¼è´ªå¿ƒé‡‡æ ·ï¼‰
   - 2.0: é«˜éšæœºæ€§ï¼ˆåˆ›æ„æ€§å¼ºï¼‰
   - ä¸å…è®¸æ¥è¿‘0ï¼Œå› ä¸ºnano-vLLMä¸æ”¯æŒçº¯è´ªå¿ƒé‡‡æ ·

2. **`max_tokens`**
   - é™åˆ¶ç”Ÿæˆé•¿åº¦
   - é˜²æ­¢æ— é™ç”Ÿæˆ
   - éœ€è¦æƒè¡¡å“åº”é•¿åº¦å’Œå»¶è¿Ÿ

3. **`ignore_eos`**
   - æ˜¯å¦å¿½ç•¥ç»“æŸtoken
   - True: å¼ºåˆ¶ç”ŸæˆæŒ‡å®šé•¿åº¦
   - False: é‡åˆ°ç»“æŸtokenå°±åœæ­¢

---

## ğŸš€ å¼•æ“æ¨¡å—é€è¡Œåˆ†æ

### nanovllm/engine/sequence.py - åºåˆ—ç®¡ç†

```python
from copy import copy
from enum import Enum, auto
from itertools import count

from nanovllm.sampling_params import SamplingParams


class SequenceStatus(Enum):
    WAITING = auto()    # ç­‰å¾…å¤„ç†
    RUNNING = auto()    # æ­£åœ¨å¤„ç†
    FINISHED = auto()   # å·²å®Œæˆ


class Sequence:
    block_size = 256                    # å—å¤§å°å¸¸é‡
    counter = count()                  # å…¨å±€è®¡æ•°å™¨ï¼Œç”¨äºç”Ÿæˆå”¯ä¸€ID

    def __init__(self, token_ids: list[int], sampling_params = SamplingParams()):
        self.seq_id = next(Sequence.counter)           # ç”Ÿæˆå”¯ä¸€åºåˆ—ID
        self.status = SequenceStatus.WAITING           # åˆå§‹çŠ¶æ€ä¸ºç­‰å¾…
        self.token_ids = copy(token_ids)               # å¤åˆ¶è¾“å…¥tokenåˆ—è¡¨
        self.last_token = token_ids[-1]                # æœ€åä¸€ä¸ªtoken
        self.num_tokens = len(token_ids)               # æ€»tokenæ•°
        self.num_prompt_tokens = len(token_ids)        # æç¤ºè¯tokenæ•°
        self.num_cached_tokens = 0                     # å·²ç¼“å­˜tokenæ•°
        self.block_table = []                          # KV Cacheå—è¡¨
        self.temperature = sampling_params.temperature  # é‡‡æ ·æ¸©åº¦
        self.max_tokens = sampling_params.max_tokens    # æœ€å¤§ç”Ÿæˆé•¿åº¦
        self.ignore_eos = sampling_params.ignore_eos    # æ˜¯å¦å¿½ç•¥EOS

    def __len__(self):
        return self.num_tokens

    def __getitem__(self, key):
        return self.token_ids[key]

    @property
    def is_finished(self):
        return self.status == SequenceStatus.FINISHED

    @property
    def num_completion_tokens(self):
        return self.num_tokens - self.num_prompt_tokens

    @property
    def completion_token_ids(self):
        return self.token_ids[self.num_prompt_tokens:]
```

**å…³é”®è®¾è®¡ç‚¹åˆ†æ**ï¼š

1. **çŠ¶æ€ç®¡ç†**ï¼š
   - ä½¿ç”¨æšä¸¾å®šä¹‰ä¸‰ç§çŠ¶æ€
   - è·Ÿè¸ªåºåˆ—çš„ç”Ÿå‘½å‘¨æœŸ

2. **å”¯ä¸€IDç”Ÿæˆ**ï¼š
   - ä½¿ç”¨`itertools.count()`ç”Ÿæˆå…¨å±€å”¯ä¸€ID
   - é¿å…IDå†²çª

3. **Tokenç®¡ç†**ï¼š
   - åŒºåˆ†æç¤ºè¯tokenå’Œç”Ÿæˆtoken
   - è·Ÿè¸ªç¼“å­˜çŠ¶æ€

4. **å†…å­˜ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨`copy()`é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
   - å—è¡¨ç®¡ç†KV Cacheå¼•ç”¨

### nanovllm/engine/block_manager.py - KV Cacheç®¡ç†å™¨

è¿™æ˜¯nano-vLLMä¸­æœ€é‡è¦çš„ä¼˜åŒ–æ¨¡å—ä¹‹ä¸€ï¼è®©æˆ‘è¯¦ç»†è§£æå®ƒçš„è®¾è®¡å’ŒPrefix CachingåŸç†ã€‚

#### ğŸ¤” ä¸ºä»€ä¹ˆéœ€è¦KV Cacheï¼Ÿ

åœ¨è‡ªå›å½’ç”Ÿæˆä¸­ï¼Œæ¨¡å‹æ¯æ¬¡åªç”Ÿæˆä¸€ä¸ªtokenï¼Œä½†æ˜¯éœ€è¦çœ‹ä¹‹å‰æ‰€æœ‰çš„tokenæ¥è®¡ç®—æ³¨æ„åŠ›ã€‚

**ä¼ ç»Ÿåšæ³•çš„ç—›ç‚¹**ï¼š
```
ç”¨æˆ·è¾“å…¥: "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"

ç¬¬1æ­¥: "ä½ å¥½" â†’ è®¡ç®—æ‰€æœ‰tokençš„æ³¨æ„åŠ› â†’ ç”Ÿæˆ"ï¼Œ"
ç¬¬2æ­¥: "ä½ å¥½ï¼Œ" â†’ é‡æ–°è®¡ç®—æ‰€æœ‰tokençš„æ³¨æ„åŠ› â†’ ç”Ÿæˆ"è¯·"
ç¬¬3æ­¥: "ä½ å¥½ï¼Œè¯·" â†’ å†æ¬¡é‡æ–°è®¡ç®—æ‰€æœ‰tokençš„æ³¨æ„åŠ› â†’ ç”Ÿæˆ"ä»‹"
...
```

**é—®é¢˜**ï¼šæ¯æ¬¡éƒ½è¦é‡æ–°è®¡ç®—ä¹‹å‰æ‰€æœ‰tokençš„Keyå’ŒValueï¼Œæµªè´¹äº†å¤§é‡è®¡ç®—ï¼

**KV Cacheçš„è§£å†³æ–¹æ¡ˆ**ï¼š
```
ç”¨æˆ·è¾“å…¥: "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"

ç¬¬1æ­¥: "ä½ å¥½" â†’ è®¡ç®—æ‰€æœ‰tokençš„K,V â†’ ç¼“å­˜ â†’ ç”Ÿæˆ"ï¼Œ"
       ç¼“å­˜å†…å®¹: ["ä½ å¥½"çš„K, "ä½ å¥½"çš„V]

ç¬¬2æ­¥: "ä½ å¥½ï¼Œ" â†’ åªè®¡ç®—æ–°token "," çš„K,V â†’ ç¼“å­˜ â†’ ç”Ÿæˆ"è¯·"
       ç¼“å­˜å†…å®¹: ["ä½ å¥½"çš„K,V, ","çš„K,V]

ç¬¬3æ­¥: "ä½ å¥½ï¼Œè¯·" â†’ åªè®¡ç®—æ–°token "è¯·" çš„K,V â†’ ç¼“å­˜ â†’ ç”Ÿæˆ"ä»‹"
       ç¼“å­˜å†…å®¹: ["ä½ å¥½"çš„K,V, ","çš„K,V, "è¯·"çš„K,V]
...
```

**æ€§èƒ½æå‡**ï¼š
- è®¡ç®—å¤æ‚åº¦ä» O(nÂ²) é™ä½åˆ° O(1)
- æ¯æ¬¡åªè®¡ç®—ä¸€ä¸ªæ–°tokenï¼Œè€Œä¸æ˜¯é‡æ–°è®¡ç®—æ‰€æœ‰å†å²token

#### ğŸ—ï¸ BlockManagerçš„è®¾è®¡åŸç†

**åŸºæœ¬æ•°æ®ç»“æ„**ï¼š
```python
# å‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹é…ç½®
block_size = 4          # æ¯ä¸ªå—å­˜å‚¨4ä¸ªtoken
num_blocks = 10         # æ€»å…±10ä¸ªå—

# è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥ç¼“å­˜ 10 * 4 = 40ä¸ªtokençš„KVå€¼
```

#### å®Œæ•´çš„BlockManagerå®ç°åˆ†æ

```python
from collections import deque
import xxhash
import numpy as np

from nanovllm.engine.sequence import Sequence


class Block:
    def __init__(self, block_id):
        self.block_id = block_id           # å—çš„å”¯ä¸€æ ‡è¯†
        self.ref_count = 0                 # å¼•ç”¨è®¡æ•°ï¼ˆæœ‰å¤šå°‘ä¸ªåºåˆ—åœ¨ä½¿ç”¨è¿™ä¸ªå—ï¼‰
        self.hash = -1                     # å†…å®¹çš„å“ˆå¸Œå€¼ï¼ˆç”¨äºPrefix Cachingï¼‰
        self.token_ids = []                # è¿™ä¸ªå—å­˜å‚¨çš„tokenåˆ—è¡¨

    def update(self, hash: int, token_ids: list[int]):
        """æ›´æ–°å—çš„å†…å®¹"""
        self.hash = hash
        self.token_ids = token_ids

    def reset(self):
        """é‡ç½®å—åˆ°åˆå§‹çŠ¶æ€"""
        self.ref_count = 1
        self.hash = -1
        self.token_ids = []


class BlockManager:
    def __init__(self, num_blocks: int, block_size: int):
        self.block_size = block_size
        # åˆ›å»ºæ‰€æœ‰çš„å—
        self.blocks: list[Block] = [Block(i) for i in range(num_blocks)]
        # å“ˆå¸Œåˆ°å—IDçš„æ˜ å°„ï¼ˆç”¨äºPrefix Cachingï¼‰
        self.hash_to_block_id: dict[int, int] = dict()
        # ç©ºé—²å—çš„åŒç«¯é˜Ÿåˆ—
        self.free_block_ids: deque[int] = deque(range(num_blocks))
        # å·²ä½¿ç”¨å—çš„é›†åˆ
        self.used_block_ids: set[int] = set()

    @classmethod
    def compute_hash(cls, token_ids: list[int], prefix: int = -1):
        """è®¡ç®—tokenåºåˆ—çš„å“ˆå¸Œå€¼"""
        h = xxhash.xxh64()
        if prefix != -1:
            h.update(prefix.to_bytes(8, "little"))
        # å°†tokenåˆ—è¡¨è½¬æ¢ä¸ºå­—èŠ‚æ•°ç»„è®¡ç®—å“ˆå¸Œ
        h.update(np.array(token_ids).tobytes())
        return h.intdigest()
```

#### ğŸš€ Prefix Cachingçš„å®Œæ•´å·¥ä½œæµç¨‹

**Prefix Caching**ï¼šå¤šä¸ªè¯·æ±‚å¦‚æœå…·æœ‰ç›¸åŒçš„å‰ç¼€ï¼Œå¯ä»¥å…±äº«è¿™äº›å‰ç¼€çš„KV Cacheè®¡ç®—ç»“æœã€‚

**å®é™…åº”ç”¨åœºæ™¯**ï¼š
```
ç”¨æˆ·A: "è¯·ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€"
ç”¨æˆ·B: "è¯·ä»‹ç»ä¸€ä¸‹Javaç¼–ç¨‹è¯­è¨€"
ç”¨æˆ·C: "è¯·ä»‹ç»ä¸€ä¸‹C++ç¼–ç¨‹è¯­è¨€"
```

è¿™ä¸‰ä¸ªè¯·æ±‚éƒ½æœ‰ç›¸åŒçš„å‰ç¼€ï¼š`"è¯·ä»‹ç»"`

### KV Cacheå·¥ä½œæµç¨‹å›¾

#### ä¼ ç»Ÿæ–¹å¼ vs KV Cacheä¼˜åŒ–

```mermaid
graph TB
    subgraph "ä¼ ç»Ÿæ–¹å¼ (æ— ä¼˜åŒ–)"
        T1[Token 1] --> T1A[è®¡ç®—æ‰€æœ‰å†å²tokençš„KV]
        T2[Token 2] --> T2A[é‡æ–°è®¡ç®—Token 1+2çš„KV]
        T3[Token 3] --> T3A[é‡æ–°è®¡ç®—Token 1+2+3çš„KV]
        T4[Token 4] --> T4A[é‡æ–°è®¡ç®—Token 1+2+3+4çš„KV]
    end

    subgraph "KV Cacheä¼˜åŒ–"
        C1[Token 1] --> C1A[è®¡ç®—Token 1çš„KVå¹¶ç¼“å­˜]
        C2[Token 2] --> C2A[åªè®¡ç®—Token 2çš„KV]
        C3[Token 3] --> C3A[åªè®¡ç®—Token 3çš„KV]
        C4[Token 4] --> C4A[åªè®¡ç®—Token 4çš„KV]

        C1A --> Cache[KV Cacheå­˜å‚¨]
        Cache --> C2A
        C2A --> Cache
        Cache --> C3A
        C3A --> Cache
        Cache --> C4A
    end
```

#### Prefix Cachingè¯¦ç»†æµç¨‹

```mermaid
flowchart TD
    A[æ–°è¯·æ±‚åˆ°è¾¾] --> B[æ£€æŸ¥åºåˆ—å‰ç¼€]
    B --> C{å‰ç¼€æ˜¯å¦å·²ç¼“å­˜?}

    C -->|æ˜¯| D[å¤ç”¨ç¼“å­˜çš„KVå—]
    C -->|å¦| E[åˆ†é…æ–°å—è®¡ç®—KV]

    D --> F[å¢åŠ å¼•ç”¨è®¡æ•°]
    E --> G[æ‰§è¡Œå‰å‘è®¡ç®—]
    G --> H[å­˜å‚¨æ–°çš„KVå—]
    H --> I[æ›´æ–°å“ˆå¸Œæ˜ å°„]

    F --> J[å‡†å¤‡æ¨¡å‹è¾“å…¥]
    I --> J
    J --> K[æ¨¡å‹æ¨ç†]
    K --> L[ç”Ÿæˆæ–°token]
    L --> M[æ›´æ–°åºåˆ—çŠ¶æ€]

    M --> N{åºåˆ—å®Œæˆ?}
    N -->|å¦| B
    N -->|æ˜¯| O[å‡å°‘å¼•ç”¨è®¡æ•°]
    O --> P[å¼•ç”¨è®¡æ•°=0?]
    P -->|æ˜¯| Q[é‡Šæ”¾å—åˆ°ç©ºé—²é˜Ÿåˆ—]
    P -->|å¦| R[ä¿æŒç¼“å­˜]
    Q --> S[ç»“æŸ]
    R --> S
```

#### å—åˆ†é…å’Œç®¡ç†æµç¨‹

```mermaid
stateDiagram-v2
    [*] --> ç©ºé—²çŠ¶æ€: åˆå§‹åŒ–
    ç©ºé—²çŠ¶æ€ --> åˆ†é…ä¸­: è¯·æ±‚åˆ†é…
    åˆ†é…ä¸­ --> å·²åˆ†é…: åˆ†é…æˆåŠŸ
    åˆ†é…ä¸­ --> ç©ºé—²çŠ¶æ€: åˆ†é…å¤±è´¥

    å·²åˆ†é… --> ä½¿ç”¨ä¸­: å¼€å§‹ä½¿ç”¨
    ä½¿ç”¨ä¸­ --> å¼•ç”¨è®¡æ•°>1: å¤šä¸ªåºåˆ—å…±äº«
    å¼•ç”¨è®¡æ•°>1 --> ä½¿ç”¨ä¸­: ç»§ç»­ä½¿ç”¨

    ä½¿ç”¨ä¸­ --> å¼•ç”¨è®¡æ•°=1: åªæœ‰ä¸€ä¸ªåºåˆ—ä½¿ç”¨
    å¼•ç”¨è®¡æ•°=1 --> é‡Šæ”¾ä¸­: åºåˆ—å®Œæˆ
    é‡Šæ”¾ä¸­ --> ç©ºé—²çŠ¶æ€: å¼•ç”¨è®¡æ•°å½’é›¶
    é‡Šæ”¾ä¸­ --> å·²åˆ†é…: ä»æœ‰å¼•ç”¨

    ç©ºé—²çŠ¶æ€ --> [*]: ç³»ç»Ÿå…³é—­
```

#### å†…å­˜åˆ†é…è¯¦ç»†æµç¨‹

```mermaid
flowchart LR
    A[åºåˆ—è¯·æ±‚] --> B[è®¡ç®—æ‰€éœ€å—æ•°]
    B --> C[æ£€æŸ¥ç©ºé—²å—]
    C --> D{ç©ºé—²å—è¶³å¤Ÿ?}

    D -->|æ˜¯| E[é€å—åˆ†é…]
    D -->|å¦| F[è§¦å‘æŠ¢å æœºåˆ¶]
    F --> G[é€‰æ‹©é•¿åºåˆ—æŠ¢å ]
    G --> H[é‡Šæ”¾è¢«æŠ¢å çš„å—]
    H --> E

    E --> I[æ£€æŸ¥æ¯ä¸ªå—çš„ç¼“å­˜]
    I --> J{å—å·²ç¼“å­˜?}

    J -->|æ˜¯| K[å¢åŠ å¼•ç”¨è®¡æ•°]
    J -->|å¦| L[åˆ†é…æ–°å—]
    L --> M[è®¡ç®—å—å“ˆå¸Œ]
    M --> N[å­˜å‚¨KVæ•°æ®]

    K --> O[æ›´æ–°åºåˆ—å—è¡¨]
    N --> O
    O --> P[åˆ†é…å®Œæˆ]
```

#### è¯¦ç»†çš„å·¥ä½œç¤ºä¾‹

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„ä¾‹å­æ¥ç†è§£Prefix Cachingï¼š

```python
# é…ç½®
block_size = 8
total_blocks = 20

# è¯·æ±‚é˜Ÿåˆ—
requests = [
    "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",                    # è¯·æ±‚1
    "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",                    # è¯·æ±‚2 (ç›¸åŒå‰ç¼€)
    "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",                          # è¯·æ±‚3 (ä¸åŒå‰ç¼€)
    "è¯·è§£é‡Šä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ",                    # è¯·æ±‚4 (ç›¸åŒå‰ç¼€)
]

# TokenåŒ–ç»“æœ (å‡è®¾)
request_tokens = {
    "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ": [1024, 2048, 512, 1024, 3072, 4096],
    "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ": [1024, 2048, 512, 1024, 3584, 4096],
    "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ": [1024, 2048, 256, 1024, 1536],
    "è¯·è§£é‡Šä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ": [1024, 2048, 512, 1024, 4608, 5120]
}
```

**ç¬¬1æ­¥ï¼šå¤„ç†ç¬¬ä¸€ä¸ªè¯·æ±‚**

```python
# å¤„ç†è¯·æ±‚1: "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
tokens1 = [1024, 2048, 512, 1024, 3072, 4096]

# è®¡ç®—æ‰€æœ‰å¯èƒ½å‰ç¼€çš„å“ˆå¸Œ
prefixes_and_hashes = []
for i in range(1, len(tokens1) + 1):
    prefix = tokens1[:i]
    hash_val = BlockManager.compute_hash(prefix)
    prefixes_and_hashes.append((prefix, hash_val))

print("è¯·æ±‚1çš„å‰ç¼€å“ˆå¸Œ:")
for prefix, hash_val in prefixes_and_hashes:
    print(f"  {prefix} â†’ {hash_val}")

# è¾“å‡º:
# è¯·æ±‚1çš„å‰ç¼€å“ˆå¸Œ:
#   [1024] â†’ 12345
#   [1024, 2048] â†’ 23456
#   [1024, 2048, 512] â†’ 34567
#   [1024, 2048, 512, 1024] â†’ 45678
#   [1024, 2048, 512, 1024, 3072] â†’ 56789
#   [1024, 2048, 512, 1024, 3072, 4096] â†’ 67890

# åˆ†é…å—å¹¶ç¼“å­˜
blocks_used = []
current_block = []
current_block_id = 0

for i, (prefix, hash_val) in enumerate(prefixes_and_hashes):
    current_block.append(prefix[-1])  # æ·»åŠ å½“å‰token

    # å—æ»¡äº†æˆ–åˆ°æœ€åä¸€ä¸ªtoken
    if len(current_block) == block_size or i == len(prefixes_and_hashes) - 1:
        block = Block(current_block_id)
        block.update(hash_val, current_block)
        blocks_used.append(block)
        hash_to_block[hash_val] = current_block_id
        current_block = []
        current_block_id += 1

print(f"è¯·æ±‚1ä½¿ç”¨äº† {len(blocks_used)} ä¸ªå—")
```

**ç¬¬2æ­¥ï¼šå¤„ç†ç¬¬äºŒä¸ªè¯·æ±‚ï¼ˆæœ‰ç›¸åŒå‰ç¼€ï¼‰**

```python
# å¤„ç†è¯·æ±‚2: "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"
tokens2 = [1024, 2048, 512, 1024, 3584, 4096]

print(f"\nå¤„ç†è¯·æ±‚2: {tokens2}")

# é€ä¸ªtokenæ£€æŸ¥ç¼“å­˜
cached_blocks = []
new_tokens_to_compute = []

for i, token in enumerate(tokens2):
    current_prefix = tokens2[:i+1]
    current_hash = BlockManager.compute_hash(current_prefix)

    print(f"æ£€æŸ¥å‰ç¼€ {current_prefix} (å“ˆå¸Œ: {current_hash})")

    if current_hash in hash_to_block:
        cached_block_id = hash_to_block[current_hash]
        cached_blocks.append(cached_block_id)
        print(f"  âœ… ç¼“å­˜å‘½ä¸­ï¼ä½¿ç”¨å— {cached_block_id}")
    else:
        new_tokens_to_compute.append((token, current_hash))
        print(f"  âŒ ç¼“å­˜æœªå‘½ä¸­ï¼Œéœ€è¦è®¡ç®—")

# ç»“æœåˆ†æ
print(f"\nè¯·æ±‚2åˆ†æ:")
print(f"  ç¼“å­˜å‘½ä¸­çš„å—æ•°: {len(cached_blocks)}")
print(f"  éœ€è¦æ–°è®¡ç®—çš„tokenæ•°: {len(new_tokens_to_compute)}")
print(f"  è®¡ç®—èŠ‚çœ: {len(cached_blocks)}/{len(tokens2)} = {len(cached_blocks)/len(tokens2)*100:.1f}%")
```

**æ€§èƒ½æå‡çš„é‡åŒ–åˆ†æ**ï¼š

```python
def analyze_prefix_caching_benefit():
    """åˆ†æPrefix Cachingçš„æ€§èƒ½æå‡"""

    # æ¨¡æ‹Ÿ100ä¸ªè¯·æ±‚ï¼Œå…¶ä¸­70%æœ‰ç›¸åŒå‰ç¼€
    common_prefix = [1024, 2048, 512]  # "è¯·è§£é‡Š"
    requests = []

    for i in range(100):
        if i < 70:  # 70%æœ‰å…±åŒå‰ç¼€
            request = common_prefix + [i + 1000, i + 2000, i + 3000]
        else:      # 30%éšæœºå‰ç¼€
            request = [i + 5000, i + 6000, i + 7000]
        requests.append(request)

    total_tokens = sum(len(req) for req in requests)
    cached_tokens = 0

    # æ¨¡æ‹Ÿç¼“å­˜è¿‡ç¨‹
    cache = {}

    for request in requests:
        for i in range(1, len(request) + 1):
            prefix = tuple(request[:i])
            if prefix in cache:
                cached_tokens += 1
            else:
                cache[prefix] = True

    cache_hit_rate = cached_tokens / total_tokens * 100
    computation_saved = cached_tokens

    print(f"Prefix Cachingæ€§èƒ½åˆ†æ:")
    print(f"  æ€»è¯·æ±‚æ•°: 100")
    print(f"  æ€»tokenæ•°: {total_tokens}")
    print(f"  ç¼“å­˜å‘½ä¸­tokenæ•°: {cached_tokens}")
    print(f"  ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1f}%")
    print(f"  è®¡ç®—èŠ‚çœ: {computation_saved} token")
    print(f"  æ€§èƒ½æå‡: {cache_hit_rate:.1f}%")

# è¿è¡Œåˆ†æ
analyze_prefix_caching_benefit()
```

**å…¸å‹è¾“å‡ºç»“æœ**ï¼š
```
Prefix Cachingæ€§èƒ½åˆ†æ:
  æ€»è¯·æ±‚æ•°: 100
  æ€»tokenæ•°: 300
  ç¼“å­˜å‘½ä¸­tokenæ•°: 140
  ç¼“å­˜å‘½ä¸­ç‡: 46.7%
  è®¡ç®—èŠ‚çœ: 140 token
  æ€§èƒ½æå‡: 46.7%
```

#### ğŸ”§ å®é™…çš„å—åˆ†é…ç®—æ³•

```python
def allocate_block_for_sequence(self, sequence: Sequence):
    """ä¸ºåºåˆ—åˆ†é…å—"""

    # è®¡ç®—åºåˆ—éœ€è¦çš„å—æ•°
    num_tokens = len(sequence.token_ids)
    num_blocks_needed = (num_tokens + self.block_size - 1) // self.block_size

    print(f"åºåˆ—{sequence.seq_id}: {num_tokens}ä¸ªtokenï¼Œéœ€è¦{num_blocks_needed}ä¸ªå—")

    allocated_blocks = []

    for block_idx in range(num_blocks_needed):
        start_pos = block_idx * self.block_size
        end_pos = min(start_pos + self.block_size, num_tokens)

        # è·å–å½“å‰å—çš„token
        block_tokens = sequence.token_ids[start_pos:end_pos]

        # è®¡ç®—è¿™ä¸ªå—çš„å“ˆå¸Œï¼ˆåŸºäºtokenå†…å®¹å’Œå‰é¢çš„å—ï¼‰
        block_hash = self.compute_hash(block_tokens, block_idx)

        print(f"  å—{block_idx}: token {block_tokens}, å“ˆå¸Œ {block_hash}")

        # æ£€æŸ¥æ˜¯å¦å·²ç»ç¼“å­˜äº†è¿™ä¸ªå—
        if block_hash in self.hash_to_block_id:
            cached_block_id = self.hash_to_block_id[block_hash]
            print(f"    âœ… å—å·²ç¼“å­˜åœ¨{cached_block_id}")
            self.blocks[cached_block_id].ref_count += 1
            allocated_blocks.append(cached_block_id)
        else:
            # åˆ†é…æ–°å—
            if not self.free_block_ids:
                raise RuntimeError("æ²¡æœ‰è¶³å¤Ÿçš„ç©ºé—²å—ï¼")

            new_block_id = self.free_block_ids.popleft()
            print(f"    âŒ åˆ†é…æ–°å—{new_block_id}")

            block = self.blocks[new_block_id]
            block.update(block_hash, block_tokens)
            block.ref_count = 1

            self.hash_to_block_id[block_hash] = new_block_id
            self.used_block_ids.add(new_block_id)
            allocated_blocks.append(new_block_id)

    sequence.block_table = allocated_blocks
    print(f"åºåˆ—{sequence.seq_id}æ€»å…±åˆ†é…äº†{len(allocated_blocks)}ä¸ªå—: {allocated_blocks}")
    return allocated_blocks
```

#### ğŸ’¡ å—å¤§å°é€‰æ‹©çš„å½±å“

```python
def analyze_block_size_impact():
    """åˆ†æä¸åŒå—å¤§å°çš„å½±å“"""

    sequence_lengths = [10, 20, 50, 100, 200]  # å…¸å‹åºåˆ—é•¿åº¦
    block_sizes = [4, 8, 16, 32, 64]           # å€™é€‰å—å¤§å°

    print("å—å¤§å°å½±å“åˆ†æ:")
    print("åºåˆ—é•¿åº¦ | å—å¤§å° | éœ€è¦å—æ•° | å†…å­˜æµªè´¹ | ç¢ç‰‡åŒ–ç¨‹åº¦")
    print("-" * 55)

    for seq_len in sequence_lengths:
        for block_size in block_sizes:
            blocks_needed = (seq_len + block_size - 1) // block_size
            total_capacity = blocks_needed * block_size
            waste = total_capacity - seq_len
            waste_percent = waste / total_capacity * 100
            fragmentation = 1 - (seq_len / total_capacity)

            print(f"{seq_len:8d} | {block_size:6d} | {blocks_needed:8d} | {waste:6d} ({waste_percent:5.1f}%) | {fragmentation:6.1%}")

analyze_block_size_impact()
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
å—å¤§å°å½±å“åˆ†æ:
åºåˆ—é•¿åº¦ | å—å¤§å° | éœ€è¦å—æ•° | å†…å­˜æµªè´¹ | ç¢ç‰‡åŒ–ç¨‹åº¦
    100 |     8 |      13 |     4 (4.0%) |  4.0%
    100 |    16 |       7 |    12 (14.3%) | 14.3%
    100 |    32 |       4 |    28 (21.9%) | 21.9%
```

#### ğŸ¯ BlockManagerçš„æ ¸å¿ƒåŠŸèƒ½æ€»ç»“

1. **å—åˆ†é…**ï¼šç®¡ç†ç©ºé—²å—å’Œå·²ä½¿ç”¨å—çš„åˆ†é…å’Œå›æ”¶
2. **å“ˆå¸Œç¼“å­˜**ï¼šé€šè¿‡å“ˆå¸Œå®ç°Prefix Cachingï¼Œå¤§å¹…å‡å°‘é‡å¤è®¡ç®—
3. **å¼•ç”¨è®¡æ•°**ï¼šè·Ÿè¸ªæ¯ä¸ªå—è¢«å¤šå°‘ä¸ªåºåˆ—å¼•ç”¨ï¼Œå®‰å…¨åœ°å›æ”¶å†…å­˜
4. **å†…å­˜ä¼˜åŒ–**ï¼šé€šè¿‡å—åŒ–ç®¡ç†é¿å…å†…å­˜ç¢ç‰‡ï¼Œæé«˜å†…å­˜ä½¿ç”¨æ•ˆç‡

**å®é™…åº”ç”¨æ•ˆæœ**ï¼š
- **30-50%çš„è®¡ç®—èŠ‚çœ**ï¼ˆå–å†³äºè¯·æ±‚çš„ç›¸ä¼¼æ€§ï¼‰
- **2-3å€çš„å¹¶å‘èƒ½åŠ›æå‡**
- **æ˜¾è‘—çš„å»¶è¿Ÿé™ä½**

è¿™äº›ä¼˜åŒ–ä½¿å¾—nano-vLLMèƒ½å¤Ÿåœ¨æœ‰é™çš„èµ„æºä¸‹å¤„ç†æ›´å¤šçš„å¹¶å‘è¯·æ±‚ï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒã€‚

### nanovllm/engine/scheduler.py - è¯·æ±‚è°ƒåº¦å™¨

```python
from collections import deque

from nanovllm.config import Config
from nanovllm.engine.sequence import Sequence, SequenceStatus
from nanovllm.engine.block_manager import BlockManager


class Scheduler:
    def __init__(self, config: Config):
        self.max_num_seqs = config.max_num_seqs
        self.max_num_batched_tokens = config.max_num_batched_tokens
        self.eos = config.eos
        self.block_manager = BlockManager(config.num_kvcache_blocks, config.kvcache_block_size)
        self.waiting: deque[Sequence] = deque()      # ç­‰å¾…é˜Ÿåˆ—
        self.running: deque[Sequence] = deque()      # è¿è¡Œé˜Ÿåˆ—

    def is_finished(self):
        return not self.waiting and not self.running

    def add(self, seq: Sequence):
        self.waiting.append(seq)

    def schedule(self) -> tuple[list[Sequence], bool]:
        # Prefillé˜¶æ®µï¼šå¤„ç†æ–°è¯·æ±‚
        scheduled_seqs = []
        num_seqs = 0
        num_batched_tokens = 0

        while self.waiting and num_seqs < self.max_num_seqs:
            seq = self.waiting[0]
            if num_batched_tokens + len(seq) > self.max_num_batched_tokens or not self.block_manager.can_allocate(seq):
                break

            num_seqs += 1
            self.block_manager.allocate(seq)
            num_batched_tokens += len(seq) - seq.num_cached_tokens
            seq.status = SequenceStatus.RUNNING
            self.waiting.popleft()
            self.running.append(seq)
            scheduled_seqs.append(seq)

        if scheduled_seqs:
            return scheduled_seqs, True

        # Decodeé˜¶æ®µï¼šå¤„ç†è¿è¡Œä¸­çš„è¯·æ±‚
        while self.running and num_seqs < self.max_num_seqs:
            seq = self.running.popleft()

            while not self.block_manager.can_append(seq):
                if self.running:
                    self.preempt(self.running.pop())
                else:
                    self.preempt(seq)
                    break
            else:
                num_seqs += 1
                self.running.append(seq)
                scheduled_seqs.append(seq)

        return scheduled_seqs, False
```

**è°ƒåº¦ç­–ç•¥åˆ†æ**ï¼š

1. **ä¸¤é˜¶æ®µè°ƒåº¦**ï¼š
   - **Prefillé˜¶æ®µ**ï¼šå¤„ç†æ–°è¯·æ±‚çš„åˆå§‹è®¡ç®—
   - **Decodeé˜¶æ®µ**ï¼šå¤„ç†å·²è¿è¡Œè¯·æ±‚çš„å¢é‡è®¡ç®—

2. **èµ„æºé™åˆ¶**ï¼š
   - `max_num_seqs`ï¼šé™åˆ¶æœ€å¤§å¹¶å‘æ•°
   - `max_num_batched_tokens`ï¼šé™åˆ¶æ‰¹å¤„ç†å¤§å°

3. **æŠ¢å æœºåˆ¶**ï¼š
   - å½“å†…å­˜ä¸è¶³æ—¶ï¼ŒæŠ¢å é•¿åºåˆ—çš„èµ„æº
   - ä¿è¯çŸ­åºåˆ—èƒ½å¤Ÿç»§ç»­å¤„ç†

### è°ƒåº¦å™¨å·¥ä½œæµç¨‹å›¾

#### ä¸¤é˜¶æ®µè°ƒåº¦ç­–ç•¥

```mermaid
flowchart TD
    A[å¼€å§‹è°ƒåº¦] --> B{æœ‰ç­‰å¾…åºåˆ—?}
    B -->|å¦| C[è¿›å…¥Decodeé˜¶æ®µ]
    B -->|æ˜¯| D[è¿›å…¥Prefillé˜¶æ®µ]

    D --> E[æŒ‘é€‰ç­‰å¾…é˜Ÿåˆ—å¤´éƒ¨åºåˆ—]
    E --> F{èµ„æºè¶³å¤Ÿ?}
    F -->|å¦| C
    F -->|æ˜¯| G[åˆ†é…KV Cache]
    G --> H[ç§»åŠ¨åˆ°è¿è¡Œé˜Ÿåˆ—]
    H --> I{è¾¾åˆ°æ‰¹å¤„ç†é™åˆ¶?}
    I -->|å¦| E
    I -->|æ˜¯| J[è¿”å›Prefillæ‰¹æ¬¡]

    C --> K[å¤„ç†è¿è¡Œé˜Ÿåˆ—]
    K --> L[æŒ‘é€‰è¿è¡Œåºåˆ—]
    L --> M{éœ€è¦é¢å¤–å—?}
    M -->|å¦| N[åŠ å…¥æ‰¹æ¬¡]
    M -->|æ˜¯| O[è§¦å‘æŠ¢å æœºåˆ¶]
    O --> P[æŠ¢å é•¿åºåˆ—]
    P --> Q[é‡Šæ”¾å—]
    Q --> N

    N --> R{è¾¾åˆ°æ‰¹å¤„ç†é™åˆ¶?}
    R -->|å¦| L
    R -->|æ˜¯| S[è¿”å›Decodeæ‰¹æ¬¡]

    J --> T[æ‰§è¡Œæ¨¡å‹æ¨ç†]
    S --> T
```

#### åºåˆ—çŠ¶æ€è½¬æ¢å›¾

```mermaid
stateDiagram-v2
    [*] --> WAITING: æ–°è¯·æ±‚åˆ°è¾¾
    WAITING --> RUNNING: è°ƒåº¦æˆåŠŸ
    RUNNING --> FINISHED: ç”Ÿæˆé•¿åº¦è¾¾æ ‡
    RUNNING --> WAITING: è¢«æŠ¢å 
    FINISHED --> [*]: å¤„ç†å®Œæˆ

    note right of RUNNING
        Prefill: å¤„ç†å®Œæ•´åºåˆ—
        Decode: é€tokenå¢é‡å¤„ç†
    end note
```

#### èµ„æºç®¡ç†ç­–ç•¥

```mermaid
graph TB
    subgraph "èµ„æºé™åˆ¶æ£€æŸ¥"
        A[max_num_seqsé™åˆ¶] --> B{å½“å‰åºåˆ—æ•° < é™åˆ¶?}
        C[max_num_batched_tokensé™åˆ¶] --> D{æ€»tokenæ•° < é™åˆ¶?}
        E[å†…å­˜å—å¯ç”¨æ€§] --> F{æœ‰ç©ºé—²å—?}
    end

    subgraph "æŠ¢å æœºåˆ¶"
        G[æ£€æµ‹å†…å­˜ä¸è¶³] --> H[é€‰æ‹©æœ€é•¿åºåˆ—]
        H --> I[æŠ¢å å…¶èµ„æº]
        I --> J[é‡Šæ”¾KV Cacheå—]
        J --> K[åºåˆ—å›åˆ°ç­‰å¾…é˜Ÿåˆ—]
    end

    subgraph "æ‰¹å¤„ç†å†³ç­–"
        L[æ”¶é›†å€™é€‰åºåˆ—] --> M[è®¡ç®—èµ„æºéœ€æ±‚]
        M --> N[èµ„æºè¯„ä¼°]
        N --> O{æ˜¯å¦å¯ä»¥æ‰¹å¤„ç†?}
        O -->|æ˜¯| P[å½¢æˆæ‰¹æ¬¡]
        O -->|å¦| Q[ç­‰å¾…ä¸‹æ¬¡è°ƒåº¦]
    end
```

#### è°ƒåº¦æ€§èƒ½ä¼˜åŒ–å›¾

```mermaid
flowchart LR
    A[è¯·æ±‚åˆ°è¾¾] --> B[å¿«é€Ÿæ£€æŸ¥]
    B --> C{ç®€å•æ¡ä»¶æ»¡è¶³?}

    C -->|æ˜¯| D[ç›´æ¥è°ƒåº¦]
    C -->|å¦| E[è¯¦ç»†è¯„ä¼°]

    D --> F[ç«‹å³å¤„ç†]
    E --> G[è®¡ç®—èµ„æºéœ€æ±‚]
    G --> H[æ£€æŸ¥ç¼“å­˜å‘½ä¸­]
    H --> I{ç¼“å­˜å‘½ä¸­ç‡é«˜?}

    I -->|æ˜¯| J[ä¼˜å…ˆè°ƒåº¦]
    I -->|å¦| K[æ™®é€šè°ƒåº¦]

    F --> L[æ‰§è¡Œæ¨ç†]
    J --> L
    K --> L

    L --> M[æ›´æ–°ç»Ÿè®¡]
    M --> N[è°ƒæ•´ç­–ç•¥]
```

#### æ‰¹å¤„ç†ä¼˜åŒ–ç­–ç•¥

```mermaid
mindmap
  root((æ‰¹å¤„ç†ä¼˜åŒ–))
    Prefillé˜¶æ®µ
      å¤§æ‰¹é‡å¤„ç†
      ç›¸ä¼¼å‰ç¼€åˆå¹¶
      å¹¶è¡Œè®¡ç®—
    Decodeé˜¶æ®µ
      å°æ‰¹é‡å¢é‡
      åŠ¨æ€æ‰¹æ¬¡å¤§å°
      å¿«é€Ÿå“åº”
    æ··åˆç­–ç•¥
      ä¼˜å…ˆçº§è°ƒåº¦
      é•¿çŸ­åºåˆ—åˆ†ç¦»
      èµ„æºåŠ¨æ€åˆ†é…
```

---

## ğŸ§  æ¨¡å‹è¿è¡Œå™¨æ·±åº¦è§£æ

### nanovllm/engine/model_runner.py - æ¨¡å‹æ‰§è¡Œæ ¸å¿ƒ

è®©æˆ‘ä»¬æ·±å…¥åˆ†æè¿™ä¸ªæœ€é‡è¦çš„æ–‡ä»¶ï¼š

```python
import pickle
import torch
import torch.distributed as dist
from multiprocessing.synchronize import Event
from multiprocessing.shared_memory import SharedMemory

from nanovllm.config import Config
from nanovllm.engine.sequence import Sequence
from nanovllm.models.qwen3 import Qwen3ForCausalLM
from nanovllm.layers.sampler import Sampler
from nanovllm.utils.context import set_context, get_context, reset_context
from nanovllm.utils.loader import load_model


class ModelRunner:
    def __init__(self, config: Config, rank: int, event: Event | list[Event]):
        # é…ç½®å’Œè®¾å¤‡è®¾ç½®
        self.config = config
        hf_config = config.hf_config
        self.block_size = config.kvcache_block_size
        self.enforce_eager = config.enforce_eager
        self.world_size = config.tensor_parallel_size
        self.rank = rank
        self.event = event

        # åˆå§‹åŒ–åˆ†å¸ƒå¼é€šä¿¡
        dist.init_process_group("nccl", "tcp://localhost:2333", world_size=self.world_size, rank=rank)
        torch.cuda.set_device(rank)

        # è®¾ç½®é»˜è®¤æ•°æ®ç±»å‹
        default_dtype = torch.get_default_dtype()
        torch.set_default_dtype(hf_config.torch_dtype)
        torch.set_default_device("cuda")

        # åˆ›å»ºæ¨¡å‹å’Œé‡‡æ ·å™¨
        self.model = Qwen3ForCausalLM(hf_config)
        load_model(self.model, config.model)
        self.sampler = Sampler()

        # æ¨¡å‹ä¼˜åŒ–
        self.warmup_model()
        self.allocate_kv_cache()
        if not self.enforce_eager:
            self.capture_cudagraph()

        # æ¢å¤é»˜è®¤è®¾ç½®
        torch.set_default_device("cpu")
        torch.set_default_dtype(default_dtype)

        # å¤šè¿›ç¨‹å¤„ç†
        if self.world_size > 1:
            if rank == 0:
                self.shm = SharedMemory(name="nanovllm", create=True, size=2**20)
                dist.barrier()
            else:
                dist.barrier()
                self.shm = SharedMemory(name="nanovllm")
                self.loop()
```

**åˆå§‹åŒ–è¿‡ç¨‹è¯¦è§£**ï¼š

1. **åˆ†å¸ƒå¼è®¾ç½®**ï¼š
   ```python
   dist.init_process_group("nccl", "tcp://localhost:2333", world_size=self.world_size, rank=rank)
   ```
   - NCCLæ˜¯NVIDIAçš„é«˜æ•ˆé€šä¿¡åº“
   - ç”¨äºå¤šGPUé—´çš„å¼ é‡é€šä¿¡
   - å›ºå®šç«¯å£2333ç”¨äºè¿›ç¨‹é—´é€šä¿¡

2. **è®¾å¤‡ç®¡ç†**ï¼š
   ```python
   torch.cuda.set_device(rank)
   ```
   - æ¯ä¸ªè¿›ç¨‹ç»‘å®šåˆ°ç‰¹å®šçš„GPU
   - rank 0ç»‘å®šåˆ°GPU 0ï¼Œrank 1ç»‘å®šåˆ°GPU 1

3. **æ¨¡å‹åŠ è½½**ï¼š
   ```python
   self.model = Qwen3ForCausalLM(hf_config)
   load_model(self.model, config.model)
   ```
   - åˆ›å»ºæ¨¡å‹å®ä¾‹
   - ä»æ–‡ä»¶åŠ è½½é¢„è®­ç»ƒæƒé‡

4. **CUDA Graphä¼˜åŒ–**ï¼š
   ```python
   if not self.enforce_eager:
       self.capture_cudagraph()
   ```
   - æ•è·è®¡ç®—å›¾ï¼Œé¿å…é‡å¤çš„å¼€é”€
   - å¤§å¹…æå‡æ¨ç†é€Ÿåº¦

### ModelRunnerå·¥ä½œæµç¨‹å›¾

#### åˆå§‹åŒ–æµç¨‹

```mermaid
flowchart TD
    A[ModelRunneråˆ›å»º] --> B[è®¾ç½®é…ç½®å‚æ•°]
    B --> C[åˆå§‹åŒ–NCCLé€šä¿¡]
    C --> D[è®¾ç½®GPUè®¾å¤‡]
    D --> E[è®¾ç½®æ•°æ®ç±»å‹]
    E --> F[åˆ›å»ºQwen3æ¨¡å‹]
    F --> G[åŠ è½½é¢„è®­ç»ƒæƒé‡]
    G --> H[åˆ›å»ºé‡‡æ ·å™¨]
    H --> I[æ¨¡å‹é¢„çƒ­]
    I --> J[åˆ†é…KV Cacheå†…å­˜]
    J --> K{å¯ç”¨CUDA Graph?}
    K -->|æ˜¯| L[æ•è·CUDAè®¡ç®—å›¾]
    K -->|å¦| M[ä½¿ç”¨Eageræ¨¡å¼]
    L --> N[åˆå§‹åŒ–å®Œæˆ]
    M --> N
```

#### æ¨ç†æ‰§è¡Œæµç¨‹

```mermaid
sequenceDiagram
    participant LE as LLMEngine
    participant MR as ModelRunner
    participant M as Qwen3Model
    participant KV as KVCache
    participant S as Sampler

    LE->>MR: run(sequences, is_prefill)

    alt Prefillé˜¶æ®µ
        MR->>MR: å‡†å¤‡æ‰€æœ‰è¾“å…¥token
        MR->>KV: allocate_blocks()
        KV-->>MR: è¿”å›slot_mapping
    else Decodeé˜¶æ®µ
        MR->>MR: å‡†å¤‡æœ€å1ä¸ªtoken
        MR->>KV: get_existing_blocks()
        KV-->>MR: è¿”å›å·²æœ‰ç¼“å­˜ä½ç½®
    end

    MR->>MR: å‡†å¤‡position_ids
    MR->>MR: å‡†å¤‡attention_mask

    alt ä½¿ç”¨CUDA Graph
        MR->>MR: æ‰§è¡Œæ•è·çš„è®¡ç®—å›¾
    else ä½¿ç”¨Eageræ¨¡å¼
        MR->>M: forward(input_ids, position_ids, slot_mapping)
        M->>M: è®¡ç®—transformerå±‚
        M->>KV: è¯»å†™KV Cache
        KV-->>M: è¿”å›cached_kv
        M-->>MR: è¿”å›logits
    end

    MR->>S: sampler(logits, temperatures)
    S-->>MR: è¿”å›next_token_ids
    MR->>KV: update_cache(new_tokens)
    MR-->>LE: è¿”å›new_token_ids
```

#### å¼ é‡å¹¶è¡Œæ‰§è¡Œæµç¨‹

```mermaid
graph TB
    subgraph "GPU 0 (Rank 0)"
        A1[è¾“å…¥åˆ†ç‰‡] --> B1[çº¿æ€§å±‚1]
        B1 --> C1[All-Reduceé€šä¿¡]
        C1 --> D1[æ³¨æ„åŠ›å±‚]
        D1 --> E1[çº¿æ€§å±‚2]
        E1 --> F1[è¾“å‡ºåˆ†ç‰‡]
    end

    subgraph "GPU 1 (Rank 1)"
        A2[è¾“å…¥åˆ†ç‰‡] --> B2[çº¿æ€§å±‚1]
        B2 --> C2[All-Reduceé€šä¿¡]
        C2 --> D2[æ³¨æ„åŠ›å±‚]
        D2 --> E2[çº¿æ€§å±‚2]
        E2 --> F2[è¾“å‡ºåˆ†ç‰‡]
    end

    C1 <--> C2
    F1 --> G[ç»“æœåˆå¹¶]
    F2 --> G
```

#### CUDA Graphä¼˜åŒ–æµç¨‹

```mermaid
stateDiagram-v2
    [*] --> æ•è·é˜¶æ®µ: åˆ›å»ºCUDA Graph
    æ•è·é˜¶æ®µ --> é¢„çƒ­è¿è¡Œ: ç¬¬ä¸€æ¬¡æ‰§è¡Œ
    é¢„çƒ­è¿è¡Œ --> å›¾å½•å°±ç»ª: æ•è·å®Œæˆ
    å›¾å½•å°±ç»ª --> å›¾é‡æ”¾: åç»­æ¨ç†
    å›¾é‡æ”¾ --> å›¾é‡æ”¾: é‡å¤ä½¿ç”¨
    å›¾é‡æ”¾ --> [*]: æ¨¡å‹é”€æ¯

    note right of å›¾é‡æ”¾
        ä¼˜åŠ¿ï¼š
        - é¿å…å†…æ ¸å¯åŠ¨å¼€é”€
        - å†…å­˜åˆ†é…é¢„ä¼˜åŒ–
        - GPUæ‰§è¡Œæµä¼˜åŒ–
    end note
```

#### å†…å­˜ç®¡ç†ç­–ç•¥

```mermaid
flowchart TB
    A[å†…å­˜è¯·æ±‚] --> B{æ˜¯å¦éœ€è¦æ–°å†…å­˜?}
    B -->|å¦| C[å¤ç”¨ç°æœ‰å†…å­˜]
    B -->|æ˜¯| D[æ£€æŸ¥å†…å­˜æ± ]

    D --> E{å†…å­˜æ± æœ‰ç©ºé—´?}
    E -->|æ˜¯| F[ä»å†…å­˜æ± åˆ†é…]
    E -->|å¦| G[æ¸…ç†ä¸´æ—¶å†…å­˜]

    G --> H{æ¸…ç†åè¶³å¤Ÿ?}
    H -->|æ˜¯| F
    H -->|å¦| I[è§¦å‘å†…å­˜å›æ”¶]

    I --> J[åƒåœ¾å›æ”¶]
    J --> K{ç°åœ¨è¶³å¤Ÿ?}
    K -->|æ˜¯| F
    K -->|å¦| L[åˆ†é…å¤±è´¥]

    C --> M[æ‰§è¡Œè®¡ç®—]
    F --> M
    L --> N[æŠ¥é”™é€€å‡º]
    M --> O[ä½¿ç”¨å®Œæˆ]
    O --> P[å½’è¿˜åˆ°å†…å­˜æ± ]
```

#### é‡‡æ ·è¿‡ç¨‹æµç¨‹

```mermaid
flowchart LR
    A[æ¨¡å‹è¾“å‡ºlogits] --> B[åº”ç”¨æ¸©åº¦ç¼©æ”¾]
    B --> C[è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ]
    C --> D[é‡‡æ ·ç­–ç•¥é€‰æ‹©]

    D --> E{é‡‡æ ·æ¨¡å¼}
    E -->|Top-k| F[é€‰æ‹©top-kæ¦‚ç‡]
    E -->|Top-p| G[ç´¯è®¡æ¦‚ç‡ç­›é€‰]
    E -->|æ¸©åº¦é‡‡æ ·| H[ç›´æ¥æ¦‚ç‡é‡‡æ ·]

    F --> I[å½’ä¸€åŒ–æ¦‚ç‡]
    G --> I
    H --> I
    I --> J[é‡‡æ ·ä¸‹ä¸€ä¸ªtoken]
    J --> K[æ£€æŸ¥ç»“æŸæ¡ä»¶]
    K --> L{ç»§ç»­ç”Ÿæˆ?}
    L -->|æ˜¯| A
    L -->|å¦| M[è¿”å›å®Œæ•´åºåˆ—]
```

---

## ğŸ”§ å·¥å…·æ¨¡å—åˆ†æ

### nanovllm/utils/context.py - ä¸Šä¸‹æ–‡ç®¡ç†

```python
import threading
from typing import Any, Dict

_context = threading.local()


def set_context(**kwargs):
    """è®¾ç½®ä¸Šä¸‹æ–‡å˜é‡"""
    for key, value in kwargs.items():
        setattr(_context, key, value)


def get_context(key: str, default: Any = None) -> Any:
    """è·å–ä¸Šä¸‹æ–‡å˜é‡"""
    return getattr(_context, key, default)


def reset_context():
    """é‡ç½®ä¸Šä¸‹æ–‡"""
    _context.__dict__.clear()


class Context:
    """ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""

    def __init__(self, **kwargs):
        self.kwargs = kwargs
        self.old_context = {}

    def __enter__(self):
        # ä¿å­˜æ—§çš„ä¸Šä¸‹æ–‡
        for key, value in self.kwargs.items():
            self.old_context[key] = get_context(key)
            set_context(**{key: value})
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        # æ¢å¤æ—§çš„ä¸Šä¸‹æ–‡
        reset_context()
        for key, value in self.old_context.items():
            if value is not None:
                set_context(**{key: value})
```

**ä¸Šä¸‹æ–‡ç³»ç»Ÿçš„ä½œç”¨**ï¼š
- çº¿ç¨‹æœ¬åœ°å­˜å‚¨ï¼Œé¿å…å¤šçº¿ç¨‹å†²çª
- ç®¡ç†å…¨å±€é…ç½®å’ŒçŠ¶æ€
- æ”¯æŒä¸´æ—¶ä¿®æ”¹é…ç½®

### nanovllm/utils/loader.py - æ¨¡å‹åŠ è½½å™¨

```python
import torch
from typing import Dict, Any


def load_model(model: torch.nn.Module, model_path: str) -> None:
    """åŠ è½½æ¨¡å‹æƒé‡"""
    state_dict = torch.load(f"{model_path}/pytorch_model.bin", map_location="cpu")
    model.load_state_dict(state_dict, strict=True)
    print(f"Loaded model from {model_path}")
```

**åŠ è½½è¿‡ç¨‹**ï¼š
- ä»ç£ç›˜åŠ è½½æƒé‡æ–‡ä»¶
- ä½¿ç”¨CPUåŠ è½½é¿å…å†…å­˜çˆ†ç‚¸
- ä¸¥æ ¼åŒ¹é…æƒé‡åç§°

---

## ğŸš€ é¡¹ç›®å¯åŠ¨æµç¨‹åˆ†æ

### å®Œæ•´çš„åˆå§‹åŒ–åºåˆ—

è®©æˆ‘ä»¬é€šè¿‡è·Ÿè¸ªä¸€ä¸ªç®€å•çš„ä½¿ç”¨ç¤ºä¾‹æ¥ç†è§£å¯åŠ¨æµç¨‹ï¼š

```python
from nanovllm import LLM, SamplingParams

# 1. åˆ›å»ºLLMå®ä¾‹
llm = LLM("/path/to/model", tensor_parallel_size=1)
```

**èƒŒåå‘ç”Ÿçš„äº‹æƒ…**ï¼š

1. **é…ç½®è§£æ**ï¼š
   ```python
   # LLM.__init__ è°ƒç”¨ LLMEngine.__init__
   config = Config(model, **kwargs)  # è§£æå¹¶éªŒè¯é…ç½®
   ```

2. **å¤šè¿›ç¨‹åˆå§‹åŒ–**ï¼š
   ```python
   ctx = mp.get_context("spawn")
   for i in range(1, config.tensor_parallel_size):  # å¯¹äºæ¯ä¸ªé¢å¤–GPU
       event = ctx.Event()
       process = ctx.Process(target=ModelRunner, args=(config, i, event))
       process.start()
   ```

3. **ä¸»è¿›ç¨‹ModelRunneråˆå§‹åŒ–**ï¼š
   ```python
   self.model_runner = ModelRunner(config, 0, self.events)
   ```

4. **æ¨¡å‹åŠ è½½å’Œä¼˜åŒ–**ï¼š
   ```python
   # åœ¨ModelRunner.__init__ä¸­
   self.model = Qwen3ForCausalLM(hf_config)
   load_model(self.model, config.model)
   self.warmup_model()
   self.allocate_kv_cache()
   self.capture_cudagraph()  # å¦‚æœå¯ç”¨
   ```

5. **è°ƒåº¦å™¨åˆå§‹åŒ–**ï¼š
   ```python
   self.scheduler = Scheduler(config)
   ```

### ç”Ÿæˆè¿‡ç¨‹çš„è°ƒç”¨é“¾

```python
# 2. ç”Ÿæˆæ–‡æœ¬
outputs = llm.generate(prompts, sampling_params)
```

**è°ƒç”¨é“¾åˆ†æ**ï¼š

1. **LLMEngine.generate()**ï¼š
   ```python
   # æ·»åŠ æ‰€æœ‰è¯·æ±‚åˆ°è°ƒåº¦å™¨
   for prompt, sp in zip(prompts, sampling_params):
       self.add_request(prompt, sp)

   # ä¸»æ¨ç†å¾ªç¯
   while not self.is_finished():
       output, num_tokens = self.step()  # æ‰§è¡Œä¸€æ­¥æ¨ç†
       # å¤„ç†è¾“å‡º...
   ```

2. **LLMEngine.step()**ï¼š
   ```python
   # 1. è°ƒåº¦ï¼šå†³å®šå“ªäº›åºåˆ—è¦å¤„ç†
   seqs, is_prefill = self.scheduler.schedule()

   # 2. æ¨ç†ï¼šè®©æ¨¡å‹è®¡ç®—
   token_ids = self.model_runner.call("run", seqs, is_prefill)

   # 3. åå¤„ç†ï¼šæ›´æ–°åºåˆ—çŠ¶æ€
   self.scheduler.postprocess(seqs, token_ids)
   ```

3. **ModelRunner.run()**ï¼š
   ```python
   # 1. å‡†å¤‡è¾“å…¥æ•°æ®
   input_ids, position_ids, slot_mapping = self.prepare_inputs(seqs)

   # 2. æ¨¡å‹å‰å‘è®¡ç®—
   logits = self.model(input_ids, position_ids, slot_mapping)

   # 3. é‡‡æ ·ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
   next_token_ids = self.sampler(logits, temperatures)

   # 4. æ›´æ–°åºåˆ—
   self.update_sequences(seqs, next_token_ids)
   ```

### å®Œæ•´çš„å¯åŠ¨å’Œè¿è¡Œæµç¨‹å›¾

#### ç³»ç»Ÿåˆå§‹åŒ–æ—¶åºå›¾

```mermaid
sequenceDiagram
    participant U as User
    participant L as LLM/LLMEngine
    participant C as Config
    participant S as Scheduler
    participant BM as BlockManager
    participant MR as ModelRunner
    participant P as ProcessPool

    U->>L: __init__(model_path, **kwargs)
    L->>C: Config(model, **kwargs)
    C-->>L: validated_config

    alt tensor_parallel_size > 1
        L->>P: åˆ›å»ºå¤šä¸ªè¿›ç¨‹
        P->>MR: ModelRunner(config, rank, event)
        MR->>MR: åˆå§‹åŒ–å¤šè¿›ç¨‹ç¯å¢ƒ
    end

    L->>MR: ModelRunner(config, rank=0, events)
    MR->>MR: åˆå§‹åŒ–NCCLé€šä¿¡
    MR->>MR: è®¾ç½®GPUè®¾å¤‡
    MR->>MR: åŠ è½½æ¨¡å‹æƒé‡
    MR->>MR: åˆ†é…KV Cacheå†…å­˜
    MR->>MR: æ•è·CUDA Graph
    MR-->>L: åˆå§‹åŒ–å®Œæˆ

    L->>S: Scheduler(config)
    S->>BM: BlockManager(num_blocks, block_size)
    BM-->>S: åˆå§‹åŒ–å®Œæˆ
    S-->>L: è°ƒåº¦å™¨å°±ç»ª
    L-->>U: ç³»ç»Ÿå‡†å¤‡å°±ç»ª
```

#### å®Œæ•´æ¨ç†å¾ªç¯æµç¨‹

```mermaid
flowchart TD
    A[ç”¨æˆ·è°ƒç”¨generate] --> B[å¤„ç†æ‰€æœ‰prompts]
    B --> C[ä¸ºæ¯ä¸ªpromptåˆ›å»ºSequence]
    C --> D[æ·»åŠ åˆ°Schedulerç­‰å¾…é˜Ÿåˆ—]
    D --> E[ä¸»æ¨ç†å¾ªç¯å¼€å§‹]

    E --> F[è°ƒç”¨stepæ–¹æ³•]
    F --> G[Scheduler.scheduleè°ƒåº¦]
    G --> H{æœ‰å¯å¤„ç†çš„åºåˆ—?}

    H -->|å¦| I[æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è¯·æ±‚å®Œæˆ]
    H -->|æ˜¯| J[è°ƒç”¨ModelRunner.run]

    J --> K[å‡†å¤‡è¾“å…¥æ•°æ®]
    K --> L[æ¨¡å‹å‰å‘æ¨ç†]
    L --> M[é‡‡æ ·ç”Ÿæˆæ–°token]
    M --> N[æ›´æ–°åºåˆ—çŠ¶æ€]

    N --> O[æ”¶é›†å®Œæˆçš„åºåˆ—]
    O --> P[è¿”å›å®Œæˆåºåˆ—]
    P --> Q{è¿˜æœ‰æœªå®Œæˆçš„è¯·æ±‚?}

    Q -->|æ˜¯| F
    Q -->|å¦| R[ç»“æŸæ¨ç†å¾ªç¯]

    I --> S{æ‰€æœ‰åºåˆ—å®Œæˆ?}
    S -->|å¦| F
    S -->|æ˜¯| R

    R --> T[è§£ç æœ€ç»ˆè¾“å‡º]
    T --> U[è¿”å›ç»™ç”¨æˆ·]
```

#### å¤šç”¨æˆ·å¹¶å‘å¤„ç†æµç¨‹

```mermaid
graph TB
    subgraph "ç”¨æˆ·å±‚"
        U1[ç”¨æˆ·1: è¯·ä»‹ç»AI]
        U2[ç”¨æˆ·2: ä»€ä¹ˆæ˜¯ML]
        U3[ç”¨æˆ·3: Pythonæ•™ç¨‹]
        U4[ç”¨æˆ·4: ç®—æ³•è§£æ]
    end

    subgraph "è°ƒåº¦å±‚"
        WQ[ç­‰å¾…é˜Ÿåˆ—]
        RQ[è¿è¡Œé˜Ÿåˆ—]
        S[Schedulerè°ƒåº¦å™¨]
    end

    subgraph "è®¡ç®—å±‚"
        BM[BlockManager<br/>KV Cacheç®¡ç†]
        MR[ModelRunner<br/>æ¨¡å‹æ‰§è¡Œ]
        M[Qwen3æ¨¡å‹]
    end

    U1 --> WQ
    U2 --> WQ
    U3 --> WQ
    U4 --> WQ

    WQ --> S
    S --> RQ
    RQ --> BM
    BM --> MR
    MR --> M

    M -->|æ–°token| RQ
    RQ -->|æ›´æ–°çŠ¶æ€| S
    S -->|è°ƒåº¦ç»“æœ| WQ

    M -->|å®Œæˆè¾“å‡º| U1
    M -->|å®Œæˆè¾“å‡º| U2
    M -->|å®Œæˆè¾“å‡º| U3
    M -->|å®Œæˆè¾“å‡º| U4
```

#### é”™è¯¯å¤„ç†å’Œæ¢å¤æµç¨‹

```mermaid
stateDiagram-v2
    [*] --> æ­£å¸¸è¿è¡Œ: ç³»ç»Ÿå¯åŠ¨
    æ­£å¸¸è¿è¡Œ --> å†…å­˜ä¸è¶³: OOMé”™è¯¯
    æ­£å¸¸è¿è¡Œ --> æ¨¡å‹é”™è¯¯: æ¨ç†å¤±è´¥
    æ­£å¸¸è¿è¡Œ --> ç½‘ç»œé”™è¯¯: é€šä¿¡å¤±è´¥

    å†…å­˜ä¸è¶³ --> æ¸…ç†ç¼“å­˜: é‡Šæ”¾KV Cache
    æ¨¡å‹é”™è¯¯ --> é‡è¯•æ¨ç†: é‡æ–°æ‰§è¡Œ
    ç½‘ç»œé”™è¯¯ --> é‡å»ºè¿æ¥: é‡æ–°é€šä¿¡

    æ¸…ç†ç¼“å­˜ --> é™ä½æ‰¹æ¬¡: å‡å°‘å¹¶å‘æ•°
    é‡è¯•æ¨ç† --> æ­£å¸¸è¿è¡Œ: æˆåŠŸ
    é‡å»ºè¿æ¥ --> æ­£å¸¸è¿è¡Œ: è¿æ¥æ¢å¤

    é™ä½æ‰¹æ¬¡ --> æ­£å¸¸è¿è¡Œ: ç»§ç»­è¿è¡Œ
    é™ä½æ‰¹æ¬¡ --> ä»ç„¶ä¸è¶³: è¿›ä¸€æ­¥é™ä½
    ä»ç„¶ä¸è¶³ --> ç­‰å¾…èµ„æº: æš‚åœå¤„ç†
    ç­‰å¾…èµ„æº --> æ­£å¸¸è¿è¡Œ: èµ„æºå¯ç”¨
```

---

## ğŸ“Š æ¨¡å—ä¾èµ–å…³ç³»å›¾

```
LLM (ç”¨æˆ·æ¥å£)
    â†“ ç»§æ‰¿
LLMEngine (å¼•æ“æ ¸å¿ƒ)
    â†“ ä½¿ç”¨
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scheduler (è°ƒåº¦å™¨)                                      â”‚
â”‚ â†“ ä½¿ç”¨ â†“ ä½¿ç”¨ â†“ ä½¿ç”¨                                   â”‚
â”‚ BlockManager â† Sequence â† ModelRunner                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“ ä½¿ç”¨
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Qwen3ForCausalLM (æ¨¡å‹)                                â”‚
â”‚ â†“ ä½¿ç”¨ â†“ ä½¿ç”¨                                          â”‚
â”‚ Attention, Linear, etc. (åŸºç¡€å±‚)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“ ä½¿ç”¨
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Context, Loader (å·¥å…·)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” ä»£ç å¯¼èˆªæŠ€å·§

### å¦‚ä½•å¿«é€Ÿæ‰¾åˆ°ç‰¹å®šåŠŸèƒ½

1. **ç”¨æˆ·å…¥å£**ï¼šä»`nanovllm/llm.py`å¼€å§‹
2. **æ ¸å¿ƒé€»è¾‘**ï¼š`nanovllm/engine/llm_engine.py`
3. **æ¨¡å‹å®šä¹‰**ï¼š`nanovllm/models/qwen3.py`
4. **æ€§èƒ½ä¼˜åŒ–**ï¼š`nanovllm/layers/`ç›®å½•
5. **é…ç½®ç®¡ç†**ï¼š`nanovllm/config.py`

### è°ƒè¯•å’Œåˆ†ææŠ€å·§

1. **æ·»åŠ æ—¥å¿—**ï¼š
   ```python
   print(f"[DEBUG] å½“å‰å¤„ç†åºåˆ—æ•°: {len(seqs)}")
   print(f"[DEBUG] å†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.2f}GB")
   ```

2. **æ€§èƒ½åˆ†æ**ï¼š
   ```python
   import time
   start = time.time()
   # ... ä»£ç  ...
   print(f"è€—æ—¶: {time.time() - start:.3f}s")
   ```

3. **å†…å­˜æ£€æŸ¥**ï¼š
   ```python
   print(f"GPUå†…å­˜: {torch.cuda.max_memory_allocated() / 1e9:.2f}GB")
   ```

---

## ğŸ’¡ æœ¬ç« æ€»ç»“

### å…³é”®è¦ç‚¹å›é¡¾

1. **é¡¹ç›®ç»“æ„æ¸…æ™°**ï¼š
   - åˆ†å±‚è®¾è®¡ï¼ŒèŒè´£æ˜ç¡®
   - æ¯ä¸ªæ¨¡å—éƒ½æœ‰ç‰¹å®šçš„åŠŸèƒ½

2. **é…ç½®ç³»ç»Ÿå®Œå–„**ï¼š
   - ç»Ÿä¸€çš„é…ç½®ç®¡ç†
   - åˆç†çš„é»˜è®¤å€¼å’ŒéªŒè¯

3. **å¯åŠ¨æµç¨‹å¤æ‚ä½†æœ‰åº**ï¼š
   - å¤šè¿›ç¨‹åˆå§‹åŒ–
   - æ¨¡å‹åŠ è½½å’Œä¼˜åŒ–
   - è°ƒåº¦å™¨å‡†å¤‡

4. **è°ƒç”¨é“¾æ¸…æ™°**ï¼š
   - ç”¨æˆ·æ¥å£ â†’ å¼•æ“ â†’ è°ƒåº¦å™¨ â†’ æ¨¡å‹è¿è¡Œå™¨ â†’ æ¨¡å‹
   - æ¯ä¸€å±‚éƒ½æœ‰æ˜ç¡®çš„èŒè´£

### é‡è¦ä»£ç ä½ç½®

- **ä¸»å…¥å£**ï¼š`nanovllm/llm.py:4` - LLMç±»å®šä¹‰
- **å¼•æ“æ ¸å¿ƒ**ï¼š`nanovllm/engine/llm_engine.py:15` - LLMEngineç±»
- **è°ƒåº¦é€»è¾‘**ï¼š`nanovllm/engine/scheduler.py:24` - scheduleæ–¹æ³•
- **æ¨¡å‹è¿è¡Œ**ï¼š`nanovllm/engine/model_runner.py:17` - ModelRunnerç±»
- **åºåˆ—ç®¡ç†**ï¼š`nanovllm/engine/sequence.py:14` - Sequenceç±»

### ä¸‹ä¸€æ­¥é¢„å‘Š

ä¸‹ä¸€ç« æˆ‘ä»¬å°†æ·±å…¥åˆ†æã€Šæ ¸å¿ƒå¼•æ“æ¨¡å—ã€‹ï¼ŒåŒ…æ‹¬ï¼š
- LLMEngineçš„è¯¦ç»†å®ç°
- Schedulerçš„è°ƒåº¦ç®—æ³•
- ModelRunnerçš„æ‰§è¡Œæµç¨‹
- å„ç§ä¼˜åŒ–æŠ€æœ¯çš„å…·ä½“å®ç°

ç°åœ¨ä½ å¯¹nano-vLLMçš„æ•´ä½“ç»“æ„åº”è¯¥æœ‰äº†æ¸…æ™°çš„ç†è§£ï¼Œå‡†å¤‡å¥½è¿›å…¥æ›´æ·±å…¥çš„æŠ€æœ¯ç»†èŠ‚äº†å—ï¼ŸğŸš€