# ç¬¬ä¸‰ç« ï¼šæ ¸å¿ƒå¼•æ“æ¨¡å—

## ğŸ“š æœ¬ç« ç›®æ ‡

é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œä½ å°†æ·±å…¥ç†è§£ï¼š
- LLMEngineçš„å®Œæ•´æ¶æ„å’Œå®ç°ç»†èŠ‚
- Scheduleræ™ºèƒ½è°ƒåº¦çš„ç®—æ³•å’Œç­–ç•¥
- ModelRunnerçš„é«˜æ•ˆæ‰§è¡Œæœºåˆ¶
- Sequenceç”Ÿå‘½å‘¨æœŸçš„å®Œæ•´ç®¡ç†
- æ ¸å¿ƒç»„ä»¶é—´çš„åè°ƒå’Œæ•°æ®æµè½¬
- æ€§èƒ½ä¼˜åŒ–çš„å…·ä½“å®ç°æŠ€æœ¯

---

## ğŸ¯ æ ¸å¿ƒå¼•æ“æ¨¡å—æ¦‚è§ˆ

nano-vLLMçš„æ ¸å¿ƒå¼•æ“ç”±å››ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼Œæ¯ä¸ªç»„ä»¶éƒ½æœ‰æ˜ç¡®çš„èŒè´£å’Œç²¾å¯†çš„åä½œæœºåˆ¶ï¼š

```mermaid
graph TB
    subgraph "æ ¸å¿ƒå¼•æ“æ¨¡å—æ¶æ„"
        LE[LLMEngine<br/>æ€»æ§åˆ¶å™¨]
        S[Scheduler<br/>æ™ºèƒ½è°ƒåº¦å™¨]
        MR[ModelRunner<br/>æ¨¡å‹æ‰§è¡Œå™¨]
        BM[BlockManager<br/>å†…å­˜ç®¡ç†å™¨]
        Seq[Sequence<br/>åºåˆ—ç®¡ç†]
    end

    subgraph "æ•°æ®æµå‘"
        A[ç”¨æˆ·è¯·æ±‚] --> LE
        LE --> S
        S --> BM
        S --> MR
        MR --> S
        S --> LE
        LE --> B[ç”¨æˆ·å“åº”]
    end

    subgraph "æ§åˆ¶æµå‘"
        C[é…ç½®å‚æ•°] --> LE
        C --> S
        C --> MR
        C --> BM
    end
```

### æ¨¡å—èŒè´£åˆ†å·¥

| æ¨¡å— | æ ¸å¿ƒèŒè´£ | å…³é”®åŠŸèƒ½ | æ€§èƒ½ä¼˜åŒ– |
|------|---------|---------|---------|
| **LLMEngine** | æ€»æ§åˆ¶å™¨ | ç”¨æˆ·æ¥å£ã€æµç¨‹åè°ƒã€é”™è¯¯å¤„ç† | å¼‚æ­¥å¤„ç†ã€èµ„æºç®¡ç† |
| **Scheduler** | æ™ºèƒ½è°ƒåº¦å™¨ | è¯·æ±‚è°ƒåº¦ã€æ‰¹å¤„ç†ä¼˜åŒ–ã€èµ„æºåˆ†é… | Prefill/Decodeåˆ†ç¦»ã€æŠ¢å æœºåˆ¶ |
| **ModelRunner** | æ¨¡å‹æ‰§è¡Œå™¨ | æ¨¡å‹æ¨ç†ã€GPUè®¡ç®—ã€å¼ é‡å¹¶è¡Œ | CUDA Graphã€å†…å­˜ä¼˜åŒ– |
| **BlockManager** | å†…å­˜ç®¡ç†å™¨ | KV Cacheç®¡ç†ã€Prefix Caching | å—åˆ†é…ã€å¼•ç”¨è®¡æ•°ã€ç¼“å­˜ä¼˜åŒ– |
| **Sequence** | åºåˆ—ç®¡ç†å™¨ | è¯·æ±‚ç”Ÿå‘½å‘¨æœŸã€çŠ¶æ€ç®¡ç† | é«˜æ•ˆçŠ¶æ€è½¬æ¢ã€å†…å­˜ä¼˜åŒ– |

---

## ğŸš€ LLMEngineï¼šæ€»æ§åˆ¶å™¨çš„æ·±åº¦è§£æ

### LLMEngineçš„è®¾è®¡ç†å¿µ

LLMEngineæ˜¯æ•´ä¸ªnano-vLLMç³»ç»Ÿçš„"å¤§è„‘"ï¼Œå®ƒä¸ä»…æ˜¯ä¸€ä¸ªç®€å•çš„æ§åˆ¶å™¨ï¼Œæ›´æ˜¯ä¸€ä¸ªå¤æ‚çš„åè°ƒè€…ã€‚è®©æˆ‘ä»¬æ·±å…¥åˆ†æå®ƒçš„è®¾è®¡æ€è·¯ï¼š

```mermaid
flowchart TD
    A[ç”¨æˆ·è°ƒç”¨generate] --> B[LLMEngineåˆå§‹åŒ–]
    B --> C[å¤šè¿›ç¨‹ç¯å¢ƒè®¾ç½®]
    C --> D[æ¨¡å‹åŠ è½½å’Œä¼˜åŒ–]
    D --> E[è°ƒåº¦å™¨åˆ›å»º]
    E --> F[ä¸»æ¨ç†å¾ªç¯]

    F --> G[è¯·æ±‚è°ƒåº¦]
    G --> H[æ¨¡å‹æ‰§è¡Œ]
    H --> I[ç»“æœå¤„ç†]
    I --> J{å¾ªç¯æ¡ä»¶æ£€æŸ¥}
    J -->|æœªå®Œæˆ| G
    J -->|å·²å®Œæˆ| K[ç»“æœæ”¶é›†]
    K --> L[è¿”å›ç»™ç”¨æˆ·]
```

### LLMEngineçš„å®Œæ•´å®ç°åˆ†æ

```python
from nanovllm.engine.llm_engine import LLMEngine
```

è®©æˆ‘ä»¬é€è¡Œåˆ†æLLMEngineçš„å®ç°ï¼š

#### 1. åˆå§‹åŒ–è¿‡ç¨‹çš„è¯¦ç»†åˆ†æ

```python
class LLMEngine:

    def __init__(self, model, **kwargs):
        # ç¬¬ä¸€æ­¥ï¼šé…ç½®è§£æå’ŒéªŒè¯
        config_fields = {field.name for field in fields(Config)}
        config_kwargs = {k: v for k, v in kwargs.items() if k in config_fields}
        config = Config(model, **config_kwargs)

        # ç¬¬äºŒæ­¥ï¼šè¿›ç¨‹ç®¡ç†å‡†å¤‡
        self.ps = []           # è¿›ç¨‹åˆ—è¡¨ï¼ˆç”¨äºå¤šGPUï¼‰
        self.events = []       # è¿›ç¨‹åŒæ­¥äº‹ä»¶

        # ç¬¬ä¸‰æ­¥ï¼šå¤šè¿›ç¨‹ç¯å¢ƒåˆå§‹åŒ–
        ctx = mp.get_context("spawn")  # ä½¿ç”¨spawnä¸Šä¸‹æ–‡ï¼Œé¿å…forké—®é¢˜
        for i in range(1, config.tensor_parallel_size):  # ä»1å¼€å§‹ï¼Œ0æ˜¯ä¸»è¿›ç¨‹
            event = ctx.Event()  # åˆ›å»ºåŒæ­¥äº‹ä»¶
            process = ctx.Process(target=ModelRunner, args=(config, i, event))
            process.start()
            self.ps.append(process)
            self.events.append(event)

        # ç¬¬å››æ­¥ï¼šä¸»è¿›ç¨‹ModelRunneråˆå§‹åŒ–
        self.model_runner = ModelRunner(config, 0, self.events)  # rank=0æ˜¯ä¸»è¿›ç¨‹

        # ç¬¬äº”æ­¥ï¼šè¾…åŠ©ç»„ä»¶åˆå§‹åŒ–
        self.tokenizer = AutoTokenizer.from_pretrained(config.model, use_fast=True)
        config.eos = self.tokenizer.eos_token_id  # è®¾ç½®ç»“æŸtoken
        self.scheduler = Scheduler(config)  # åˆ›å»ºè°ƒåº¦å™¨

        # ç¬¬å…­æ­¥ï¼šæ¸…ç†æ³¨å†Œ
        atexit.register(self.exit)
```

**åˆå§‹åŒ–è¿‡ç¨‹çš„è¯¦ç»†è§£è¯»**ï¼š

1. **é…ç½®è§£æé˜¶æ®µ**ï¼š
   - ä½¿ç”¨åå°„æœºåˆ¶æå–Configç±»çš„æ‰€æœ‰å­—æ®µ
   - è¿‡æ»¤ç”¨æˆ·æä¾›çš„å‚æ•°ï¼Œåªä¿ç•™Configç±»å®šä¹‰çš„å‚æ•°
   - è¿™æä¾›äº†ä¸€ä¸ªæ¸…æ™°çš„é…ç½®æ¥å£ï¼Œé¿å…å‚æ•°æ··ä¹±

2. **å¤šè¿›ç¨‹æ¶æ„è®¾è®¡**ï¼š
   - **ä¸ºä»€ä¹ˆä½¿ç”¨spawnè€Œä¸æ˜¯forkï¼Ÿ**
     ```python
     # forkçš„é—®é¢˜ï¼šå¯èƒ½ç»§æ‰¿çˆ¶è¿›ç¨‹çš„GPUä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´å†²çª
     # spawnçš„ä¼˜åŠ¿ï¼šåˆ›å»ºå…¨æ–°çš„è¿›ç¨‹ç¯å¢ƒï¼Œé¿å…çŠ¶æ€æ±¡æŸ“
     ctx = mp.get_context("spawn")
     ```

3. **è¿›ç¨‹åŒæ­¥æœºåˆ¶**ï¼š
   ```python
   # Eventç”¨äºè¿›ç¨‹é—´åŒæ­¥
   event = ctx.Event()
   # ä¸»è¿›ç¨‹ç­‰å¾…æ‰€æœ‰å­è¿›ç¨‹å°±ç»ª
   for event in self.events:
       event.wait()
   ```

4. **ä¸»ä»è¿›ç¨‹æ¨¡å¼**ï¼š
   - Rank 0ï¼šä¸»è¿›ç¨‹ï¼Œè´Ÿè´£ä»»åŠ¡è°ƒåº¦å’Œç»“æœæ”¶é›†
   - Rank 1-Nï¼šå·¥ä½œè¿›ç¨‹ï¼Œè´Ÿè´£æ¨¡å‹è®¡ç®—

#### 2. è¯·æ±‚æ·»åŠ æœºåˆ¶çš„æ·±å…¥åˆ†æ

```python
def add_request(self, prompt: str | list[int], sampling_params: SamplingParams):
    """æ·»åŠ è¯·æ±‚åˆ°å¤„ç†é˜Ÿåˆ—"""
    if isinstance(prompt, str):
        # æ–‡æœ¬åˆ°tokençš„è½¬æ¢
        prompt = self.tokenizer.encode(prompt)

    # åˆ›å»ºSequenceå¯¹è±¡
    seq = Sequence(prompt, sampling_params)

    # æ·»åŠ åˆ°è°ƒåº¦å™¨çš„ç­‰å¾…é˜Ÿåˆ—
    self.scheduler.add(seq)
```

**å…³é”®è®¾è®¡å†³ç­–åˆ†æ**ï¼š

1. **çµæ´»çš„è¾“å…¥æ¥å£**ï¼š
   - æ”¯æŒå­—ç¬¦ä¸²è¾“å…¥ï¼šè‡ªåŠ¨è¿›è¡Œtokenization
   - æ”¯æŒtokenæ•°ç»„è¾“å…¥ï¼šè·³è¿‡tokenizationï¼Œé€‚åˆé¢„å¤„ç†åœºæ™¯

2. **Sequenceå¯¹è±¡çš„åˆ›å»ºæ—¶æœº**ï¼š
   ```python
   # ä¸ºä»€ä¹ˆåœ¨add_requestæ—¶åˆ›å»ºSequenceï¼Ÿ
   # 1. å°è£…çŠ¶æ€ï¼šå°†è¯·æ±‚çš„æ‰€æœ‰ä¿¡æ¯å°è£…åœ¨ä¸€ä¸ªå¯¹è±¡ä¸­
   # 2. ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼šSequenceå¯¹è±¡è·Ÿè¸ªè¯·æ±‚çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸ
   # 3. çŠ¶æ€ä¸€è‡´æ€§ï¼šé¿å…æ•°æ®åœ¨å¤šä¸ªç»„ä»¶é—´ä¼ é€’æ—¶çš„ä¸ä¸€è‡´
   ```

3. **ç«‹å³åŠ å…¥ç­‰å¾…é˜Ÿåˆ—**ï¼š
   - ä¸æ˜¯åœ¨generateæ—¶æ‰¹é‡å¤„ç†ï¼Œè€Œæ˜¯ç«‹å³åŠ å…¥é˜Ÿåˆ—
   - è¿™å…è®¸æ›´çµæ´»çš„è°ƒåº¦ç­–ç•¥å’Œæ›´å¥½çš„èµ„æºåˆ©ç”¨

#### 3. æ ¸å¿ƒæ¨ç†å¾ªç¯çš„è¯¦ç»†åˆ†æ

```python
def step(self):
    """æ‰§è¡Œä¸€æ­¥æ¨ç†ï¼Œè¿™æ˜¯æ•´ä¸ªç³»ç»Ÿçš„æ ¸å¿ƒå¾ªç¯"""

    # ç¬¬ä¸€é˜¶æ®µï¼šè°ƒåº¦å†³ç­–
    seqs, is_prefill = self.scheduler.schedule()

    # ç¬¬äºŒé˜¶æ®µï¼šæ¨¡å‹æ‰§è¡Œ
    token_ids = self.model_runner.call("run", seqs, is_prefill)

    # ç¬¬ä¸‰é˜¶æ®µï¼šåå¤„ç†
    self.scheduler.postprocess(seqs, token_ids)

    # ç¬¬å››é˜¶æ®µï¼šç»“æœæ”¶é›†
    outputs = [(seq.seq_id, seq.completion_token_ids) for seq in seqs if seq.is_finished]
    num_tokens = sum(len(seq) for seq in seqs) if is_prefill else -len(seqs)

    return outputs, num_tokens
```

**stepæ–¹æ³•çš„æ·±åº¦è§£æ**ï¼š

1. **è°ƒåº¦é˜¶æ®µï¼ˆscheduleï¼‰**ï¼š
   ```mermaid
   flowchart TD
       A[è°ƒç”¨schedule] --> B{æ£€æŸ¥ç­‰å¾…é˜Ÿåˆ—}
       B -->|æœ‰ç­‰å¾…åºåˆ—| C[Prefillé˜¶æ®µå¤„ç†]
       B -->|æ— ç­‰å¾…åºåˆ—| D[Decodeé˜¶æ®µå¤„ç†]
       C --> E[åˆ†é…KV Cache]
       D --> F[å¤„ç†è¿è¡Œä¸­åºåˆ—]
       E --> G[è¿”å›åºåˆ—åˆ—è¡¨å’Œé˜¶æ®µæ ‡è®°]
       F --> G
   ```

2. **æ¨¡å‹æ‰§è¡Œé˜¶æ®µï¼ˆmodel_runner.callï¼‰**ï¼š
   - ä½¿ç”¨`call`è€Œä¸æ˜¯ç›´æ¥è°ƒç”¨ï¼Œæ”¯æŒå¤šè¿›ç¨‹é€šä¿¡
   - `is_prefill`å‚æ•°å½±å“å¤„ç†ç­–ç•¥
   - è¿”å›æ–°ç”Ÿæˆçš„token IDs

3. **åå¤„ç†é˜¶æ®µï¼ˆpostprocessï¼‰**ï¼š
   - æ›´æ–°SequenceçŠ¶æ€
   - ç®¡ç†KV Cacheçš„å¼•ç”¨è®¡æ•°
   - å¤„ç†å®Œæˆçš„åºåˆ—

#### 4. å®Œæ•´çš„generateæ–¹æ³•æ·±åº¦åˆ†æ

```python
def generate(
    self,
    prompts: list[str] | list[list[int]],
    sampling_params: SamplingParams | list[SamplingParams],
    use_tqdm: bool = True,
) -> list[str]:
    """ä¸»è¦çš„ç”Ÿæˆæ¥å£"""

    # è¾“å…¥æ ‡å‡†åŒ–å¤„ç†
    if not isinstance(sampling_params, list):
        sampling_params = [sampling_params] * len(prompts)

    # è¿›åº¦æ¡åˆå§‹åŒ–
    if use_tqdm:
        pbar = tqdm(total=len(prompts), desc="Generating", dynamic_ncols=True)

    # è¯·æ±‚æ·»åŠ é˜¶æ®µ
    for prompt, sp in zip(prompts, sampling_params):
        self.add_request(prompt, sp)

    # ç»“æœæ”¶é›†å’Œæ€§èƒ½ç»Ÿè®¡
    outputs = {}
    prefill_throughput = decode_throughput = 0.

    # ä¸»æ¨ç†å¾ªç¯
    while not self.is_finished():
        t = perf_counter()  # æ€§èƒ½è®¡æ—¶å¼€å§‹

        # æ‰§è¡Œä¸€æ­¥æ¨ç†
        output, num_tokens = self.step()

        # æ€§èƒ½ç»Ÿè®¡æ›´æ–°
        if use_tqdm:
            if num_tokens > 0:  # Prefillé˜¶æ®µ
                prefill_throughput = num_tokens / (perf_counter() - t)
            else:  # Decodeé˜¶æ®µ (num_tokensæ˜¯è´Ÿæ•°)
                decode_throughput = -num_tokens / (perf_counter() - t)

            # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
            pbar.set_postfix({
                "Prefill": f"{int(prefill_throughput)}tok/s",
                "Decode": f"{int(decode_throughput)}tok/s",
            })

        # æ”¶é›†å®Œæˆçš„åºåˆ—
        for seq_id, token_ids in output:
            outputs[seq_id] = token_ids
            if use_tqdm:
                pbar.update(1)

    # ç»“æœæ•´ç†å’Œè¿”å›
    outputs = [outputs[seq_id] for seq_id in sorted(outputs.keys())]
    outputs = [{"text": self.tokenizer.decode(token_ids), "token_ids": token_ids} for token_ids in outputs]

    if use_tqdm:
        pbar.close()

    return outputs
```

**generateæ–¹æ³•çš„è®¾è®¡äº®ç‚¹**ï¼š

1. **è¾“å…¥æ ‡å‡†åŒ–**ï¼š
   ```python
   # æ”¯æŒå•ä¸ªé‡‡æ ·å‚æ•°åº”ç”¨åˆ°æ‰€æœ‰prompt
   if not isinstance(sampling_params, list):
       sampling_params = [sampling_params] * len(prompts)
   ```

2. **æ€§èƒ½ç›‘æ§é›†æˆ**ï¼š
   ```python
   # å®æ—¶æ€§èƒ½ç»Ÿè®¡
   if num_tokens > 0:  # Prefill
       prefill_throughput = num_tokens / time_elapsed
   else:  # Decode
       decode_throughput = -num_tokens / time_elapsed
   ```

3. **è¿›åº¦æ¡é›†æˆ**ï¼š
   - æ˜¾ç¤ºå¤„ç†è¿›åº¦
   - å®æ—¶æ˜¾ç¤ºååé‡
   - æ”¯æŒåŠ¨æ€è°ƒæ•´æ˜¾ç¤º

#### 5. é”™è¯¯å¤„ç†å’Œèµ„æºç®¡ç†

```python
def exit(self):
    """æ¸…ç†èµ„æºå’Œé€€å‡º"""
    try:
        # é€šçŸ¥æ¨¡å‹è¿è¡Œå™¨é€€å‡º
        self.model_runner.call("exit")
    except Exception as e:
        print(f"Warning: Error during model runner exit: {e}")

    try:
        # æ¸…ç†æ¨¡å‹è¿è¡Œå™¨
        del self.model_runner
    except Exception as e:
        print(f"Warning: Error during model runner cleanup: {e}")

    try:
        # ç­‰å¾…æ‰€æœ‰å­è¿›ç¨‹é€€å‡º
        for p in self.ps:
            p.join(timeout=10)  # è®¾ç½®è¶…æ—¶é¿å…æ— é™ç­‰å¾…
            if p.is_alive():
                p.terminate()  # å¼ºåˆ¶ç»ˆæ­¢
                p.join(timeout=5)
    except Exception as e:
        print(f"Warning: Error during process cleanup: {e}")
```

**èµ„æºç®¡ç†çš„æœ€ä½³å®è·µ**ï¼š

1. **å¼‚å¸¸å®‰å…¨**ï¼šæ¯ä¸ªæ¸…ç†æ­¥éª¤éƒ½æœ‰å¼‚å¸¸å¤„ç†
2. **è¶…æ—¶æœºåˆ¶**ï¼šé¿å…æ— é™ç­‰å¾…è¿›ç¨‹é€€å‡º
3. **å¼ºåˆ¶ç»ˆæ­¢**ï¼šåœ¨å¿…è¦æ—¶å¼ºåˆ¶æ¸…ç†èµ„æº

### LLMEngineçš„æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

#### 1. å¼‚æ­¥å¤„ç†æ¶æ„

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant LE as LLMEngine
    participant S as Scheduler
    participant MR as ModelRunner

    U->>LE: generate(prompts)
    LE->>LE: å¼‚æ­¥æ·»åŠ è¯·æ±‚

    par å¹¶è¡Œå¤„ç†
        LE->>S: è°ƒåº¦è¯·æ±‚
    and
        LE->>MR: æ‰§è¡Œæ¨ç†
    end

    MR-->>LE: è¿”å›tokens
    LE->>LE: å¼‚æ­¥æ”¶é›†ç»“æœ

    alt æ‰€æœ‰è¯·æ±‚å®Œæˆ
        LE-->>U: è¿”å›æœ€ç»ˆç»“æœ
    else ä»æœ‰è¯·æ±‚
        LE->>LE: ç»§ç»­å¾ªç¯
    end
```

#### 2. å†…å­˜é¢„åˆ†é…ç­–ç•¥

```python
class MemoryOptimizedLLMEngine(LLMEngine):
    def __init__(self, model, **kwargs):
        super().__init__(model, **kwargs)

        # é¢„åˆ†é…å¸¸ç”¨æ•°æ®ç»“æ„
        self.preallocated_buffers = {
            'input_ids': torch.zeros((self.max_batch_size, self.max_seq_len), dtype=torch.long),
            'position_ids': torch.zeros((self.max_batch_size, self.max_seq_len), dtype=torch.long),
            'attention_mask': torch.zeros((self.max_batch_size, self.max_seq_len), dtype=torch.bool)
        }

    def get_buffers(self, actual_batch_size, actual_seq_len):
        """è·å–é¢„åˆ†é…çš„bufferï¼Œæ”¯æŒåŠ¨æ€å¤§å°"""
        return {
            key: buf[:actual_batch_size, :actual_seq_len].clone()
            for key, buf in self.preallocated_buffers.items()
        }
```

#### 3. æ‰¹å¤„ç†ä¼˜åŒ–ç­–ç•¥

```python
def optimize_batch_processing(self, sequences):
    """æ‰¹å¤„ç†ä¼˜åŒ–ç­–ç•¥"""

    # 1. é•¿åº¦åˆ†ç»„ï¼šç›¸ä¼¼é•¿åº¦çš„åºåˆ—ä¸€èµ·å¤„ç†
    length_groups = {}
    for seq in sequences:
        length = len(seq)
        if length not in length_groups:
            length_groups[length] = []
        length_groups[length].append(seq)

    # 2. å‰ç¼€åˆ†ç»„ï¼šç›¸åŒå‰ç¼€çš„åºåˆ—ä¸€èµ·å¤„ç†
    prefix_groups = {}
    for length, seqs in length_groups.items():
        for seq in seqs:
            prefix = self.get_prefix_hash(seq)
            if prefix not in prefix_groups:
                prefix_groups[prefix] = []
            prefix_groups[prefix].append(seq)

    # 3. ä¼˜å…ˆçº§æ’åºï¼šçŸ­åºåˆ—ä¼˜å…ˆå¤„ç†
    optimized_batches = []
    for prefix, seqs in prefix_groups.items():
        seqs.sort(key=len)  # çŸ­åºåˆ—ä¼˜å…ˆ
        optimized_batches.append(seqs)

    return optimized_batches
```

---

## âš™ï¸ Schedulerï¼šæ™ºèƒ½è°ƒåº¦å™¨çš„æ·±åº¦è§£æ

### Schedulerçš„æ ¸å¿ƒè®¾è®¡æ€æƒ³

Scheduleræ˜¯nano-vLLMçš„"å¤§è„‘"ï¼Œå®ƒè´Ÿè´£å†³å®šï¼š
1. **å“ªäº›è¯·æ±‚åº”è¯¥è¢«å¤„ç†**ï¼ˆè°ƒåº¦å†³ç­–ï¼‰
2. **å¦‚ä½•ç»„ç»‡æ‰¹å¤„ç†**ï¼ˆæ‰¹å¤„ç†ä¼˜åŒ–ï¼‰
3. **å¦‚ä½•ç®¡ç†å†…å­˜èµ„æº**ï¼ˆèµ„æºåˆ†é…ï¼‰
4. **å¦‚ä½•å¤„ç†èµ„æºä¸è¶³**ï¼ˆæŠ¢å æœºåˆ¶ï¼‰

### Schedulerçš„è¯¦ç»†å®ç°åˆ†æ

#### 1. åˆå§‹åŒ–å’Œé…ç½®

```python
class Scheduler:
    def __init__(self, config: Config):
        # èµ„æºé™åˆ¶é…ç½®
        self.max_num_seqs = config.max_num_seqs              # æœ€å¤§å¹¶å‘åºåˆ—æ•°
        self.max_num_batched_tokens = config.max_num_batched_tokens  # æœ€å¤§æ‰¹å¤„ç†tokenæ•°
        self.eos = config.eos                                # ç»“æŸtoken ID

        # æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–
        self.block_manager = BlockManager(
            config.num_kvcache_blocks,
            config.kvcache_block_size
        )

        # é˜Ÿåˆ—ç®¡ç†
        self.waiting: deque[Sequence] = deque()      # ç­‰å¾…é˜Ÿåˆ—ï¼ˆFIFOï¼‰
        self.running: deque[Sequence] = deque()      # è¿è¡Œé˜Ÿåˆ—ï¼ˆå¯é‡æ’ï¼‰

        # è°ƒåº¦ç»Ÿè®¡
        self.stats = {
            'total_requests': 0,
            'prefill_requests': 0,
            'decode_requests': 0,
            'preemptions': 0
        }
```

**åˆå§‹åŒ–å‚æ•°çš„è¯¦ç»†è¯´æ˜**ï¼š

1. **max_num_seqsï¼ˆæœ€å¤§å¹¶å‘åºåˆ—æ•°ï¼‰**ï¼š
   - é™åˆ¶åŒæ—¶å¤„ç†çš„è¯·æ±‚æ•°é‡
   - å½±å“å†…å­˜ä½¿ç”¨å’Œå“åº”å»¶è¿Ÿ
   - å…¸å‹å€¼ï¼š512ï¼Œå¯æ ¹æ®GPUå†…å­˜è°ƒæ•´

2. **max_num_batched_tokensï¼ˆæœ€å¤§æ‰¹å¤„ç†tokenæ•°ï¼‰**ï¼š
   - é™åˆ¶å•æ¬¡æ¨ç†çš„æ€»tokenæ•°
   - å½±å“GPUåˆ©ç”¨ç‡å’Œè®¡ç®—æ•ˆç‡
   - å…¸å‹å€¼ï¼š16384

3. **é˜Ÿåˆ—è®¾è®¡å†³ç­–**ï¼š
   ```python
   # ä¸ºä»€ä¹ˆwaitingä½¿ç”¨dequeè€Œrunningä¹Ÿç”¨dequeï¼Ÿ
   # waitingï¼šFIFOé˜Ÿåˆ—ï¼Œä¿è¯å…¬å¹³æ€§
   # runningï¼šdequeä¾¿äºä¸¤ç«¯çš„æ’å…¥å’Œåˆ é™¤ï¼Œæ”¯æŒæŠ¢å 
   ```

#### 2. ä¸¤é˜¶æ®µè°ƒåº¦ç­–ç•¥çš„æ ¸å¿ƒå®ç°

```python
def schedule(self) -> tuple[list[Sequence], bool]:
    """
    ä¸¤é˜¶æ®µè°ƒåº¦ç­–ç•¥ï¼š
    1. Prefillé˜¶æ®µï¼šå¤„ç†æ–°è¯·æ±‚çš„å®Œæ•´è®¡ç®—
    2. Decodeé˜¶æ®µï¼šå¤„ç†è¿è¡Œä¸­è¯·æ±‚çš„å¢é‡è®¡ç®—

    è¿”å›ï¼š(è°ƒåº¦çš„åºåˆ—åˆ—è¡¨, æ˜¯å¦æ˜¯Prefillé˜¶æ®µ)
    """

    # === Prefillé˜¶æ®µï¼šå¤„ç†æ–°è¯·æ±‚ ===
    scheduled_seqs = []
    num_seqs = 0
    num_batched_tokens = 0

    # ä»ç­‰å¾…é˜Ÿåˆ—æŒ‘é€‰åºåˆ—
    while self.waiting and num_seqs < self.max_num_seqs:
        seq = self.waiting[0]  # æŸ¥çœ‹é˜Ÿé¦–ä½†ä¸ç§»é™¤

        # æ£€æŸ¥èµ„æºé™åˆ¶
        would_exceed_token_limit = num_batched_tokens + len(seq) > self.max_num_batched_tokens
        no_available_memory = not self.block_manager.can_allocate(seq)

        if would_exceed_token_limit or no_available_memory:
            break  # èµ„æºä¸è¶³ï¼Œåœæ­¢Prefill

        # èµ„æºå……è¶³ï¼Œå¼€å§‹å¤„ç†è¿™ä¸ªåºåˆ—
        num_seqs += 1

        # åˆ†é…KV Cacheå†…å­˜
        self.block_manager.allocate(seq)
        num_batched_tokens += len(seq) - seq.num_cached_tokens  # åªè®¡ç®—æœªç¼“å­˜çš„token

        # æ›´æ–°åºåˆ—çŠ¶æ€
        seq.status = SequenceStatus.RUNNING
        self.waiting.popleft()  # ä»ç­‰å¾…é˜Ÿåˆ—ç§»é™¤
        self.running.append(seq)  # åŠ å…¥è¿è¡Œé˜Ÿåˆ—
        scheduled_seqs.append(seq)

        # æ›´æ–°ç»Ÿè®¡
        self.stats['prefill_requests'] += 1

    # å¦‚æœPrefillé˜¶æ®µæœ‰å¤„ç†ï¼Œç›´æ¥è¿”å›
    if scheduled_seqs:
        self.stats['total_requests'] += len(scheduled_seqs)
        return scheduled_seqs, True

    # === Decodeé˜¶æ®µï¼šå¤„ç†è¿è¡Œä¸­çš„è¯·æ±‚ ===
    self.running = deque(sorted(self.running, key=lambda seq: len(seq)))  # æŒ‰é•¿åº¦æ’åºï¼ŒçŸ­åºåˆ—ä¼˜å…ˆ

    while self.running and num_seqs < self.max_num_seqs:
        seq = self.running.popleft()

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é¢å¤–å†…å­˜ï¼ˆç”¨äºæ–°ç”Ÿæˆçš„tokenï¼‰
        while not self.block_manager.can_append(seq):
            if self.running:
                # æŠ¢å æœºåˆ¶ï¼šä¼˜å…ˆä¿æŠ¤çŸ­åºåˆ—
                preempted_seq = self.running.pop()  # æŠ¢å æœ€é•¿çš„åºåˆ—
                self.preempt(preempted_seq)
                self.stats['preemptions'] += 1
            else:
                # æ²¡æœ‰å¯æŠ¢å çš„åºåˆ—ï¼Œåªèƒ½æŠ¢å å½“å‰åºåˆ—
                self.preempt(seq)
                break  # è·³å‡ºå†…å±‚å¾ªç¯ï¼Œé‡æ–°è°ƒåº¦
        else:
            # å†…å­˜æ£€æŸ¥é€šè¿‡ï¼Œå¯ä»¥å¤„ç†
            num_seqs += 1
            self.running.append(seq)
            scheduled_seqs.append(seq)
            self.stats['decode_requests'] += 1

    if scheduled_seqs:
        self.stats['total_requests'] += len(scheduled_seqs)
        return scheduled_seqs, False

    # æ²¡æœ‰å¯è°ƒåº¦çš„åºåˆ—
    return [], False
```

**ä¸¤é˜¶æ®µè°ƒåº¦ç­–ç•¥çš„æ·±å…¥åˆ†æ**ï¼š

1. **Prefillé˜¶æ®µçš„ä¼˜åŒ–ç­–ç•¥**ï¼š
   ```mermaid
   flowchart TD
       A[Prefillé˜¶æ®µå¼€å§‹] --> B[æ£€æŸ¥ç­‰å¾…é˜Ÿåˆ—]
       B --> C{æœ‰ç­‰å¾…åºåˆ—?}
       C -->|å¦| D[è¿›å…¥Decodeé˜¶æ®µ]
       C -->|æ˜¯| E[æ£€æŸ¥é˜Ÿé¦–åºåˆ—]
       E --> F{èµ„æºå……è¶³?}
       F -->|å¦| G[æš‚åœPrefill]
       F -->|æ˜¯| H[åˆ†é…å†…å­˜]
       H --> I[ç§»åŠ¨åˆ°è¿è¡Œé˜Ÿåˆ—]
       I --> J{è¾¾åˆ°é™åˆ¶?}
       J -->|å¦| E
       J -->|æ˜¯| K[è¿”å›Prefillæ‰¹æ¬¡]
       G --> D
   ```

2. **Decodeé˜¶æ®µçš„ä¼˜å…ˆçº§ç­–ç•¥**ï¼š
   ```python
   # å…³é”®ä¼˜åŒ–ï¼šæŒ‰åºåˆ—é•¿åº¦æ’åº
   self.running = deque(sorted(self.running, key=lambda seq: len(seq)))
   # ä¼˜åŠ¿ï¼š
   # 1. çŸ­åºåˆ—ä¼˜å…ˆå®Œæˆï¼Œæé«˜å“åº”é€Ÿåº¦
   # 2. å‡å°‘é•¿åºåˆ—å¯¹çŸ­åºåˆ—çš„é˜»å¡
   # 3. æé«˜æ•´ä½“ååé‡
   ```

3. **æŠ¢å æœºåˆ¶çš„è§¦å‘æ¡ä»¶**ï¼š
   ```python
   # æŠ¢å å†³ç­–é€»è¾‘
   def should_preempt(self, running_seq, new_seq):
       # 1. å¦‚æœè¿è¡Œåºåˆ—æ˜æ˜¾æ›´é•¿ï¼Œä¼˜å…ˆæŠ¢å 
       if len(running_seq) > len(new_seq) * 2:
           return True

       # 2. å¦‚æœè¿è¡Œåºåˆ—å·²ç»å®Œæˆå¤§éƒ¨åˆ†ï¼Œä¸æŠ¢å 
       if running_seq.num_completion_tokens > running_seq.max_tokens * 0.8:
           return False

       # 3. é»˜è®¤ç­–ç•¥ï¼šä¼˜å…ˆä¿æŠ¤çŸ­åºåˆ—
       return len(running_seq) > len(new_seq)
   ```

#### 3. æŠ¢å æœºåˆ¶çš„è¯¦ç»†å®ç°

```python
def preempt(self, seq: Sequence):
    """
    æŠ¢å æœºåˆ¶ï¼šå½“å†…å­˜ä¸è¶³æ—¶ï¼Œä¸­æ–­é•¿åºåˆ—ä»¥é‡Šæ”¾èµ„æºç»™çŸ­åºåˆ—
    """

    # 1. æ›´æ–°åºåˆ—çŠ¶æ€
    seq.status = SequenceStatus.WAITING

    # 2. é‡Šæ”¾KV Cacheå—
    self.block_manager.deallocate(seq)

    # 3. ç§»å›ç­‰å¾…é˜Ÿåˆ—
    self.waiting.appendleft(seq)  # æ”¾åˆ°é˜Ÿé¦–ï¼Œä¼˜å…ˆé‡æ–°è°ƒåº¦

    # 4. è®°å½•æŠ¢å ç»Ÿè®¡
    self.stats['preemptions'] += 1

    # 5. å¯é€‰ï¼šè°ƒæ•´åºåˆ—çš„ä¼˜å…ˆçº§
    seq.preemption_count += 1
    if seq.preemption_count > 3:  # è¢«æŠ¢å å¤ªå¤šæ¬¡ï¼Œé™ä½ä¼˜å…ˆçº§
        seq.priority_penalty += 1
```

**æŠ¢å æœºåˆ¶çš„è®¾è®¡è€ƒé‡**ï¼š

1. **å…¬å¹³æ€§ä¿è¯**ï¼š
   ```python
   # è¢«æŠ¢å çš„åºåˆ—æ”¾åˆ°ç­‰å¾…é˜Ÿåˆ—å‰ç«¯ï¼Œè€Œä¸æ˜¯åç«¯
   self.waiting.appendleft(seq)
   # è¿™æ ·ç¡®ä¿è¢«æŠ¢å çš„åºåˆ—ä¼˜å…ˆé‡æ–°è·å¾—è°ƒåº¦æœºä¼š
   ```

2. **ä¼˜å…ˆçº§è°ƒæ•´**ï¼š
   ```python
   # é¿å…é¥¿æ­»ï¼šé˜²æ­¢æŸäº›åºåˆ—æ€»æ˜¯è¢«æŠ¢å 
   if seq.preemption_count > 3:
       seq.priority_penalty += 1
   ```

3. **æ€§èƒ½å½±å“åˆ†æ**ï¼š
   - **æ­£é¢å½±å“**ï¼šçŸ­åºåˆ—å¿«é€Ÿå“åº”ï¼Œæé«˜ç”¨æˆ·ä½“éªŒ
   - **è´Ÿé¢å½±å“**ï¼šé•¿åºåˆ—å¯èƒ½è¢«å¤šæ¬¡ä¸­æ–­ï¼Œé™ä½æ•´ä½“æ•ˆç‡
   - **å¹³è¡¡ç­–ç•¥**ï¼šåŠ¨æ€è°ƒæ•´æŠ¢å é˜ˆå€¼

#### 4. åºåˆ—çŠ¶æ€ç®¡ç†

```python
class SequenceManager:
    def __init__(self, scheduler: Scheduler):
        self.scheduler = scheduler
        self.state_transitions = {
            SequenceStatus.WAITING: self._handle_waiting_to_running,
            SequenceStatus.RUNNING: self._handle_running_to_finished,
            SequenceStatus.FINISHED: self._handle_finished_cleanup
        }

    def transition_state(self, seq: Sequence, new_status: SequenceStatus):
        """å¤„ç†åºåˆ—çŠ¶æ€è½¬æ¢"""
        old_status = seq.status

        if old_status == new_status:
            return  # çŠ¶æ€æœªå˜åŒ–

        # è®°å½•çŠ¶æ€è½¬æ¢
        self.log_state_transition(seq, old_status, new_status)

        # æ‰§è¡ŒçŠ¶æ€è½¬æ¢é€»è¾‘
        if new_status in self.state_transitions:
            self.state_transitions[new_status](seq)

        # æ›´æ–°çŠ¶æ€
        seq.status = new_status

        # è§¦å‘ç›¸å…³çš„é’©å­å‡½æ•°
        self.on_state_changed(seq, old_status, new_status)

    def _handle_waiting_to_running(self, seq: Sequence):
        """å¤„ç†ä»ç­‰å¾…åˆ°è¿è¡Œçš„çŠ¶æ€è½¬æ¢"""
        # åˆ†é…èµ„æº
        self.scheduler.block_manager.allocate(seq)

        # æ›´æ–°è°ƒåº¦å™¨é˜Ÿåˆ—
        self.scheduler.running.append(seq)

        # è®°å½•å¼€å§‹æ—¶é—´
        seq.start_time = time.time()

    def _handle_running_to_finished(self, seq: Sequence):
        """å¤„ç†ä»è¿è¡Œåˆ°å®Œæˆçš„çŠ¶æ€è½¬æ¢"""
        # è®¡ç®—å¤„ç†æ—¶é—´
        seq.processing_time = time.time() - seq.start_time

        # æ›´æ–°ç»Ÿè®¡
        self.scheduler.update_completion_stats(seq)

        # ä»è¿è¡Œé˜Ÿåˆ—ç§»é™¤
        try:
            self.scheduler.running.remove(seq)
        except ValueError:
            pass  # å¯èƒ½å·²ç»è¢«å…¶ä»–æ“ä½œç§»é™¤
```

#### 5. æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡

```python
class SchedulerMetrics:
    def __init__(self, scheduler: Scheduler):
        self.scheduler = scheduler
        self.metrics_history = deque(maxlen=1000)  # ä¿ç•™æœ€è¿‘1000ä¸ªæ•°æ®ç‚¹

    def collect_metrics(self):
        """æ”¶é›†è°ƒåº¦å™¨çš„æ€§èƒ½æŒ‡æ ‡"""

        current_time = time.time()

        metrics = {
            'timestamp': current_time,
            'waiting_queue_size': len(self.scheduler.waiting),
            'running_queue_size': len(self.scheduler.running),
            'total_memory_blocks': len(self.scheduler.block_manager.used_block_ids),
            'free_memory_blocks': len(self.scheduler.block_manager.free_block_ids),
            'cache_hit_rate': self.calculate_cache_hit_rate(),
            'throughput': self.calculate_current_throughput(),
            'avg_latency': self.calculate_avg_latency(),
            'preemption_rate': self.calculate_preemption_rate()
        }

        self.metrics_history.append(metrics)
        return metrics

    def calculate_cache_hit_rate(self):
        """è®¡ç®—KV Cacheå‘½ä¸­ç‡"""
        total_blocks = len(self.scheduler.block_manager.blocks)
        reused_blocks = sum(1 for block in self.scheduler.block_manager.blocks
                           if block.ref_count > 1)
        return reused_blocks / total_blocks if total_blocks > 0 else 0

    def get_performance_summary(self):
        """è·å–æ€§èƒ½æ‘˜è¦"""
        if not self.metrics_history:
            return {}

        recent_metrics = list(self.metrics_history)[-100:]  # æœ€è¿‘100ä¸ªæ•°æ®ç‚¹

        return {
            'avg_waiting_size': sum(m['waiting_queue_size'] for m in recent_metrics) / len(recent_metrics),
            'avg_throughput': sum(m['throughput'] for m in recent_metrics) / len(recent_metrics),
            'avg_cache_hit_rate': sum(m['cache_hit_rate'] for m in recent_metrics) / len(recent_metrics),
            'peak_memory_usage': max(m['total_memory_blocks'] for m in recent_metrics),
            'peak_queue_size': max(m['waiting_queue_size'] for m in recent_metrics)
        }
```

### Schedulerçš„é«˜çº§ä¼˜åŒ–ç­–ç•¥

#### 1. è‡ªé€‚åº”æ‰¹å¤„ç†å¤§å°

```python
class AdaptiveBatchingScheduler(Scheduler):
    def __init__(self, config: Config):
        super().__init__(config)
        self.target_latency = 100  # ç›®æ ‡å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
        self.performance_history = deque(maxlen=50)

    def get_adaptive_batch_size(self):
        """æ ¹æ®å†å²æ€§èƒ½åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°"""

        if len(self.performance_history) < 10:
            return self.max_num_batched_tokens

        # è®¡ç®—æœ€è¿‘å¹³å‡å»¶è¿Ÿ
        avg_latency = sum(self.performance_history) / len(self.performance_history)

        # åŠ¨æ€è°ƒæ•´ç­–ç•¥
        if avg_latency > self.target_latency * 1.2:
            # å»¶è¿Ÿå¤ªé«˜ï¼Œå‡å°æ‰¹å¤„ç†å¤§å°
            new_size = int(self.max_num_batched_tokens * 0.8)
        elif avg_latency < self.target_latency * 0.8:
            # å»¶è¿Ÿå¾ˆä½ï¼Œå¯ä»¥å¢å¤§æ‰¹å¤„ç†å¤§å°
            new_size = int(self.max_num_batched_tokens * 1.2)
        else:
            # å»¶è¿Ÿåˆé€‚ï¼Œä¿æŒå½“å‰å¤§å°
            new_size = self.max_num_batched_tokens

        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        new_size = max(new_size, 1024)  # æœ€å°1024 tokens
        new_size = min(new_size, self.max_num_batched_tokens * 2)  # æœ€å¤§2å€é…ç½®å€¼

        return new_size
```

#### 2. æ™ºèƒ½é¢„æµ‹è°ƒåº¦

```python
class PredictiveScheduler(Scheduler):
    def __init__(self, config: Config):
        super().__init__(config)
        self.request_predictor = RequestPatternPredictor()
        self.resource_predictor = ResourceUsagePredictor()

    def schedule_with_prediction(self):
        """åŸºäºé¢„æµ‹çš„æ™ºèƒ½è°ƒåº¦"""

        # 1. é¢„æµ‹å³å°†åˆ°æ¥çš„è¯·æ±‚æ¨¡å¼
        predicted_requests = self.request_predictor.predict_next_requests()

        # 2. é¢„æµ‹èµ„æºä½¿ç”¨æƒ…å†µ
        predicted_usage = self.resource_predictor.predict_resource_usage(predicted_requests)

        # 3. é¢„åˆ†é…èµ„æº
        if predicted_usage.memory_pressure > 0.8:
            # é¢„æœŸå†…å­˜å‹åŠ›å¤§ï¼Œæå‰æ¸…ç†
            self.proactive_cleanup()

        # 4. æ‰§è¡Œæ ‡å‡†è°ƒåº¦
        return self.schedule()

    def proactive_cleanup(self):
        """ä¸»åŠ¨æ¸…ç†ç­–ç•¥"""
        # è¯†åˆ«ä½ä¼˜å…ˆçº§çš„é•¿æ—¶é—´è¿è¡Œåºåˆ—
        cleanup_candidates = []
        for seq in self.running:
            if (seq.priority_penalty > 2 and
                seq.num_completion_tokens > seq.max_tokens * 0.3):
                cleanup_candidates.append(seq)

        # æ¸…ç†éƒ¨åˆ†åºåˆ—ï¼Œä¸ºé¢„æœŸçš„é«˜ä¼˜å…ˆçº§è¯·æ±‚è…¾å‡ºç©ºé—´
        for seq in cleanup_candidates[:len(cleanup_candidates)//2]:
            self.preempt(seq)
```

---

## ğŸ§  ModelRunnerï¼šæ¨¡å‹æ‰§è¡Œå™¨çš„æ·±åº¦è§£æ

### ModelRunnerçš„è®¾è®¡æ¶æ„

ModelRunneræ˜¯nano-vLLMçš„"æ‰§è¡Œå¼•æ“"ï¼Œè´Ÿè´£ï¼š
1. **æ¨¡å‹å‰å‘è®¡ç®—**ï¼šæ‰§è¡ŒTransformeræ¨¡å‹çš„æ¨ç†
2. **GPUèµ„æºç®¡ç†**ï¼šé«˜æ•ˆåˆ©ç”¨GPUè®¡ç®—èµ„æº
3. **å¼ é‡å¹¶è¡Œ**ï¼šæ”¯æŒå¤šGPUå¹¶è¡Œè®¡ç®—
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šCUDA Graphã€å†…å­˜ä¼˜åŒ–ç­‰

### ModelRunnerçš„å®Œæ•´å®ç°åˆ†æ

#### 1. åˆå§‹åŒ–è¿‡ç¨‹çš„æ·±åº¦è§£æ

```python
class ModelRunner:

    def __init__(self, config: Config, rank: int, event: Event | list[Event]):
        # === é…ç½®å’Œç¯å¢ƒè®¾ç½® ===
        self.config = config
        hf_config = config.hf_config
        self.block_size = config.kvcache_block_size
        self.enforce_eager = config.enforce_eager
        self.world_size = config.tensor_parallel_size
        self.rank = rank
        self.event = event

        # === åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ– ===
        # ä¸ºä»€ä¹ˆä½¿ç”¨NCCLè€Œä¸æ˜¯Glooï¼Ÿ
        # NCCLæ˜¯NVIDIAä¸“ä¸ºGPUé—´é€šä¿¡ä¼˜åŒ–çš„åº“
        # å¯¹äºå¤§æ¨¡å‹æ¨ç†ï¼ŒNCCLçš„æ€§èƒ½è¿œè¶…Gloo
        dist.init_process_group(
            "nccl",
            "tcp://localhost:2333",  # å›ºå®šç«¯å£ï¼Œä¾¿äºå¤šè¿›ç¨‹é€šä¿¡
            world_size=self.world_size,
            rank=rank
        )

        # === GPUè®¾å¤‡ç®¡ç† ===
        torch.cuda.set_device(rank)
        torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜

        # === æ•°æ®ç±»å‹å’Œè®¾å¤‡è®¾ç½® ===
        default_dtype = torch.get_default_dtype()
        torch.set_default_dtype(hf_config.torch_dtype)
        torch.set_default_device("cuda")

        # === æ¨¡å‹åˆ›å»ºå’ŒåŠ è½½ ===
        self.model = Qwen3ForCausalLM(hf_config)
        load_model(self.model, config.model)

        # === æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ– ===
        self.sampler = Sampler()

        # === æ€§èƒ½ä¼˜åŒ– ===
        self.warmup_model()
        self.allocate_kv_cache()
        if not self.enforce_eager:
            self.capture_cudagraph()

        # === æ¢å¤é»˜è®¤è®¾ç½® ===
        torch.set_default_device("cpu")
        torch.set_default_dtype(default_dtype)

        # === å¤šè¿›ç¨‹åè°ƒ ===
        if self.world_size > 1:
            if rank == 0:
                # ä¸»è¿›ç¨‹ï¼šåˆ›å»ºå…±äº«å†…å­˜
                self.shm = SharedMemory(name="nanovllm", create=True, size=2**20)
                dist.barrier()  # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å°±ç»ª
            else:
                # å·¥ä½œè¿›ç¨‹ï¼šè¿æ¥å…±äº«å†…å­˜
                dist.barrier()
                self.shm = SharedMemory(name="nanovllm")
                self.loop()  # è¿›å…¥å·¥ä½œå¾ªç¯
```

**åˆå§‹åŒ–è¿‡ç¨‹çš„æ·±åº¦åˆ†æ**ï¼š

1. **NCCLé€šä¿¡åˆå§‹åŒ–**ï¼š
   ```mermaid
   sequenceDiagram
       participant P0 as ä¸»è¿›ç¨‹ (rank 0)
       participant P1 as å·¥ä½œè¿›ç¨‹ (rank 1)
       participant NCCL as NCCL Backend

       P0->>NCCL: init_process_group("nccl", rank=0)
       P1->>NCCL: init_process_group("nccl", rank=1)
       NCCL->>NCCL: å»ºç«‹é€šä¿¡æ‹“æ‰‘
       P0->>P1: é€šè¿‡NCCLé€šä¿¡
   ```

2. **è®¾å¤‡ç»‘å®šç­–ç•¥**ï¼š
   ```python
   # ä¸ºä»€ä¹ˆæ¯ä¸ªè¿›ç¨‹éœ€è¦ç»‘å®šåˆ°ç‰¹å®šGPUï¼Ÿ
   torch.cuda.set_device(rank)
   # åŸå› ï¼š
   # 1. é¿å…å¤šè¿›ç¨‹ç«äº‰åŒä¸€ä¸ªGPU
   # 2. ç¡®ä¿å¼ é‡å¹¶è¡Œè®¡ç®—çš„æ­£ç¡®æ€§
   # 3. ä¼˜åŒ–å†…å­˜åˆ†é…å’Œè®¿é—®
   ```

3. **å†…å­˜ç®¡ç†ç­–ç•¥**ï¼š
   ```python
   # åˆ†é˜¶æ®µå†…å­˜ç®¡ç†
   torch.set_default_device("cuda")  # æ¨¡å‹åŠ è½½é˜¶æ®µï¼šä½¿ç”¨GPU
   # ... æ¨¡å‹åŠ è½½ ...
   torch.set_default_device("cpu")   # æ§åˆ¶é˜¶æ®µï¼šä½¿ç”¨CPU
   # è¿™å‡å°‘äº†CPUå’ŒGPUé—´çš„å†…å­˜ä¼ è¾“
   ```

#### 2. æ¨¡å‹é¢„çƒ­æœºåˆ¶

```python
def warmup_model(self):
    """æ¨¡å‹é¢„çƒ­ï¼šæ¶ˆé™¤é¦–æ¬¡æ¨ç†çš„å¼€é”€"""

    # åˆ›å»ºé¢„çƒ­æ•°æ®
    warmup_batch_size = min(8, self.config.max_num_seqs)
    warmup_seq_len = min(128, self.config.max_model_len)

    # é¢„çƒ­è¾“å…¥
    warmup_input_ids = torch.randint(
        0, self.model.config.vocab_size,
        (warmup_batch_size, warmup_seq_len),
        device="cuda",
        dtype=torch.long
    )

    warmup_position_ids = torch.arange(
        0, warmup_seq_len,
        device="cuda",
        dtype=torch.long
    ).unsqueeze(0).expand(warmup_batch_size, -1)

    warmup_slot_mapping = torch.zeros(
        warmup_batch_size, warmup_seq_len,
        device="cuda",
        dtype=torch.long
    )

    print(f"æ¨¡å‹é¢„çƒ­ä¸­... æ‰¹æ¬¡å¤§å°: {warmup_batch_size}, åºåˆ—é•¿åº¦: {warmup_seq_len}")

    # å¤šæ¬¡é¢„çƒ­ç¡®ä¿æ‰€æœ‰ç»„ä»¶éƒ½è¢«åˆå§‹åŒ–
    for i in range(3):
        with torch.no_grad():
            logits = self.model(
                warmup_input_ids,
                warmup_position_ids,
                warmup_slot_mapping
            )

        # ç¡®ä¿GPUåŒæ­¥
        torch.cuda.synchronize()

        # é‡‡æ ·é¢„çƒ­
        temperatures = torch.ones(warmup_batch_size, device="cuda")
        _ = self.sampler(logits, temperatures)

        print(f"é¢„çƒ­å®Œæˆç¬¬ {i+1} è½®")

    # æ¸…ç†é¢„çƒ­å†…å­˜
    del warmup_input_ids, warmup_position_ids, warmup_slot_mapping, logits
    torch.cuda.empty_cache()
```

**é¢„çƒ­æœºåˆ¶çš„è®¾è®¡è€ƒé‡**ï¼š

1. **é¢„çƒ­æ•°æ®è®¾è®¡**ï¼š
   ```python
   # ä¸ºä»€ä¹ˆä½¿ç”¨éšæœºæ•°æ®è€Œä¸æ˜¯çœŸå®æ•°æ®ï¼Ÿ
   # 1. é¿å…æ•°æ®å‡†å¤‡çš„å¼€é”€
   # 2. ç¡®ä¿æ‰€æœ‰token IDéƒ½è¢«æµ‹è¯•
   # 3. æ¨¡æ‹ŸçœŸå®ä½¿ç”¨çš„æ•°æ®åˆ†å¸ƒ
   ```

2. **å¤šæ¬¡é¢„çƒ­çš„åŸå› **ï¼š
   ```python
   for i in range(3):  # ä¸‰æ¬¡é¢„çƒ­
   # ç¬¬1æ¬¡ï¼šåˆå§‹åŒ–GPUå†…æ ¸å’Œå†…å­˜åˆ†é…
   # ç¬¬2æ¬¡ï¼šå»ºç«‹CUDA Graphï¼ˆå¦‚æœå¯ç”¨ï¼‰
   # ç¬¬3æ¬¡ï¼šç¡®ä¿æ‰€æœ‰ç¼“å­˜éƒ½è¢«å¡«å……
   ```

3. **å†…å­˜æ¸…ç†ç­–ç•¥**ï¼š
   ```python
   torch.cuda.empty_cache()  # é¢„çƒ­åæ¸…ç†ä¸´æ—¶å†…å­˜
   # ç¡®ä¿é¢„çƒ­ä¸ä¼šå½±å“å®é™…æ¨ç†çš„å†…å­˜ä½¿ç”¨
   ```

#### 3. KV Cacheåˆ†é…æœºåˆ¶

```python
def allocate_kv_cache(self):
    """ä¸ºKV Cacheåˆ†é…å†…å­˜"""

    # è®¡ç®—éœ€è¦çš„å†…å­˜å¤§å°
    num_layers = self.model.config.num_hidden_layers
    num_heads = self.model.config.num_attention_heads
    head_dim = self.model.config.hidden_size // num_heads
    num_blocks = self.config.num_kvcache_blocks

    # è®¡ç®—æ¯ä¸ªå—çš„å¤§å°
    block_size = self.block_size
    per_block_bytes = (
        num_layers *  # æ¯å±‚éƒ½æœ‰KV
        2 *  # Keyå’ŒValue
        num_heads *  # æ³¨æ„åŠ›å¤´æ•°
        block_size *  # åºåˆ—é•¿åº¦
        head_dim *  # å¤´ç»´åº¦
        torch.finfo(torch.float16).bits // 8  # float16çš„å­—èŠ‚æ•°
    )

    total_memory_gb = (num_blocks * per_block_bytes) / (1024**3)
    print(f"åˆ†é…KV Cacheå†…å­˜: {num_blocks}å—, æ€»è®¡çº¦{total_memory_gb:.2f}GB")

    # ä¸ºæ¯å±‚åˆ†é…KV Cache
    self.kv_cache = []

    for layer_idx in range(num_layers):
        # Key cache: (num_blocks, num_heads, block_size, head_dim)
        k_cache = torch.zeros(
            (num_blocks, num_heads, block_size, head_dim),
            dtype=torch.float16,
            device="cuda"
        )

        # Value cache: (num_blocks, num_heads, block_size, head_dim)
        v_cache = torch.zeros(
            (num_blocks, num_heads, block_size, head_dim),
            dtype=torch.float16,
            device="cuda"
        )

        self.kv_cache.append((k_cache, v_cache))

    # åˆ›å»ºå—ä½ç½®çš„æ˜ å°„
    self.slot_mapping_buffer = torch.zeros(
        (self.config.max_num_seqs, self.config.max_model_len),
        dtype=torch.long,
        device="cuda"
    )

    print(f"KV Cacheåˆ†é…å®Œæˆ: {num_layers}å±‚, æ¯å±‚{num_blocks}å—")
```

**KV Cacheåˆ†é…çš„è¯¦ç»†åˆ†æ**ï¼š

1. **å†…å­˜è®¡ç®—å…¬å¼**ï¼š
   ```python
   total_memory = num_layers Ã— 2 Ã— num_heads Ã— num_blocks Ã— block_size Ã— head_dim Ã— bytes_per_element
   ```
   - `num_layers`ï¼šTransformerå±‚æ•°
   - `2`ï¼šKeyå’ŒValueä¸¤ä¸ªçŸ©é˜µ
   - `num_heads`ï¼šæ³¨æ„åŠ›å¤´æ•°
   - `num_blocks`ï¼šKV Cacheå—æ•°
   - `block_size`ï¼šæ¯å—tokenæ•°
   - `head_dim`ï¼šæ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
   - `bytes_per_element`ï¼šfloat16 = 2å­—èŠ‚

2. **å†…å­˜å¸ƒå±€ä¼˜åŒ–**ï¼š
   ```mermaid
   flowchart TD
       A[KV Cacheå†…å­˜å¸ƒå±€] --> B[æŒ‰å±‚ç»„ç»‡]
       B --> C[Layer 0: K_cache, V_cache]
       B --> D[Layer 1: K_cache, V_cache]
       B --> E[Layer N: K_cache, V_cache]

       C --> F[å†…å­˜è¿ç»­è®¿é—®]
       D --> F
       E --> F

       F --> G[ä¼˜åŒ–GPUå†…å­˜å¸¦å®½åˆ©ç”¨ç‡]
   ```

3. **å—æ˜ å°„æœºåˆ¶**ï¼š
   ```python
   # slot_mappingçš„ä½œç”¨
   # å°†åºåˆ—ä½ç½®æ˜ å°„åˆ°KV Cacheå—çš„ä½ç½®
   # ä¾‹å¦‚ï¼šåºåˆ—[0,1,2,3,4,5] -> å—[0,0,0,0,1,1]
   # è¿™æ ·å¯ä»¥å¿«é€Ÿå®šä½KVæ•°æ®
   ```

#### 4. CUDA Graphä¼˜åŒ–æœºåˆ¶

```python
def capture_cudagraph(self):
    """æ•è·CUDA Graphä»¥ä¼˜åŒ–æ¨ç†æ€§èƒ½"""

    print("å¼€å§‹æ•è·CUDA Graph...")

    # å‡†å¤‡Graphæ•è·çš„é™æ€è¾“å…¥
    static_batch_size = 8
    static_seq_len = 256

    static_input_ids = torch.randint(
        0, self.model.config.vocab_size,
        (static_batch_size, static_seq_len),
        device="cuda",
        dtype=torch.long
    )

    static_position_ids = torch.arange(
        0, static_seq_len,
        device="cuda",
        dtype=torch.long
    ).unsqueeze(0).expand(static_batch_size, -1)

    static_slot_mapping = torch.zeros(
        static_batch_size, static_seq_len,
        device="cuda",
        dtype=torch.long
    )

    static_temperatures = torch.ones(static_batch_size, device="cuda")

    # ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆwarmupï¼‰
    print("CUDA Graph warmup...")
    with torch.no_grad():
        logits = self.model(static_input_ids, static_position_ids, static_slot_mapping)
        _ = self.sampler(logits, static_temperatures)
    torch.cuda.synchronize()

    # æ•è·Graph
    print("æ•è·CUDA Graph...")
    self.cuda_graphs = []
    self.graph_inputs = []
    self.graph_outputs = []

    # ä¸ºä¸åŒå¤§å°çš„æ‰¹æ¬¡æ•è·å¤šä¸ªGraph
    batch_sizes = [1, 2, 4, 8, 16, 32]

    for batch_size in batch_sizes:
        if batch_size > static_batch_size:
            continue

        # è°ƒæ•´è¾“å…¥å¤§å°
        current_input_ids = static_input_ids[:batch_size]
        current_position_ids = static_position_ids[:batch_size]
        current_slot_mapping = static_slot_mapping[:batch_size]
        current_temperatures = static_temperatures[:batch_size]

        # CUDA Graphæ•è·
        g = torch.cuda.CUDAGraph()

        with torch.cuda.graph(g):
            with torch.no_grad():
                logits = self.model(current_input_ids, current_position_ids, current_slot_mapping)
                outputs = self.sampler(logits, current_temperatures)

        self.cuda_graphs.append(g)
        self.graph_inputs.append((current_input_ids, current_position_ids, current_slot_mapping, current_temperatures))
        self.graph_outputs.append(outputs)

        print(f"CUDA Graphæ•è·å®Œæˆ: æ‰¹æ¬¡å¤§å°={batch_size}")

    print(f"æ€»å…±æ•è·äº†{len(self.cuda_graphs)}ä¸ªCUDA Graph")

    # æ¸…ç†ä¸´æ—¶å†…å­˜
    del static_input_ids, static_position_ids, static_slot_mapping, static_temperatures
    torch.cuda.empty_cache()
```

**CUDA Graphä¼˜åŒ–çš„æ·±åº¦åˆ†æ**ï¼š

1. **ä¸ºä»€ä¹ˆéœ€è¦CUDA Graphï¼Ÿ**
   ```mermaid
   flowchart LR
       A[ä¼ ç»ŸPyTorchæ‰§è¡Œ] --> B[æ¯æ¬¡éƒ½é‡æ–°ç¼–è¯‘]
       B --> C[GPUå†…æ ¸å¯åŠ¨å¼€é”€]
       C --> D[å†…å­˜åˆ†é…å¼€é”€]
       D --> E[CPU-GPUåŒæ­¥å¼€é”€]

       F[CUDA Graphæ‰§è¡Œ] --> G[é¢„ç¼–è¯‘çš„è®¡ç®—å›¾]
       G --> H[é›¶å†…æ ¸å¯åŠ¨å¼€é”€]
       H --> I[é¢„åˆ†é…å†…å­˜]
       I --> J[å‡å°‘CPU-GPUåŒæ­¥]

       E --> K[æ€§èƒ½ç“¶é¢ˆ]
       J --> L[æ€§èƒ½æå‡]
   ```

2. **Graphæ•è·çš„å…³é”®è¦ç´ **ï¼š
   ```python
   # é™æ€è¾“å…¥ï¼šGraphæ•è·æ—¶è¾“å…¥å¿…é¡»å›ºå®šå¤§å°
   static_input_ids = torch.zeros(...)

   # å¤šä¸ªGraphï¼šæ”¯æŒä¸åŒæ‰¹æ¬¡å¤§å°
   batch_sizes = [1, 2, 4, 8, 16, 32]
   # è¿™æ ·å¯ä»¥æ¥è¿‘çœŸå®ä½¿ç”¨åœºæ™¯
   ```

3. **Graphé€‰æ‹©ç­–ç•¥**ï¼š
   ```python
   def select_cuda_graph(self, batch_size):
       """æ ¹æ®å®é™…æ‰¹æ¬¡å¤§å°é€‰æ‹©æœ€åˆé€‚çš„Graph"""

       # æ‰¾åˆ°æœ€æ¥è¿‘ä½†ä¸å°äºå®é™…æ‰¹æ¬¡å¤§å°çš„Graph
       suitable_size = None
       for size in [1, 2, 4, 8, 16, 32]:
           if size >= batch_size:
               suitable_size = size
               break

       if suitable_size is None:
           # ä½¿ç”¨æœ€å¤§çš„Graph
           suitable_size = 32

       # è¿”å›å¯¹åº”çš„Graphç´¢å¼•
       graph_idx = [1, 2, 4, 8, 16, 32].index(suitable_size)
       return graph_idx
   ```

#### 5. å‰å‘æ¨ç†çš„æ ¸å¿ƒå®ç°

```python
def run(self, seqs: list[Sequence], is_prefill: bool) -> list[int]:
    """æ‰§è¡Œæ¨¡å‹æ¨ç†çš„æ ¸å¿ƒæ–¹æ³•"""

    # === è¾“å…¥å‡†å¤‡é˜¶æ®µ ===
    input_ids, position_ids, slot_mapping = self.prepare_inputs(seqs, is_prefill)

    # === æ‰§è¡Œæ¨ç† ===
    if not self.enforce_eager and not is_prefill:
        # Decodeé˜¶æ®µä½¿ç”¨CUDA Graphï¼ˆå¦‚æœå¯ç”¨ï¼‰
        next_token_ids = self.run_with_cuda_graph(input_ids, position_ids, slot_mapping, seqs)
    else:
        # Prefillé˜¶æ®µæˆ–å¼ºåˆ¶eageræ¨¡å¼ä½¿ç”¨æ ‡å‡†æ¨ç†
        with torch.no_grad():
            logits = self.model(input_ids, position_ids, slot_mapping)
            temperatures = torch.tensor([seq.temperature for seq in seqs], device="cuda")
            next_token_ids = self.sampler(logits, temperatures)

    # === åå¤„ç†é˜¶æ®µ ===
    self.update_sequences(seqs, next_token_ids)
    self.update_kv_cache(seqs, next_token_ids)

    return next_token_ids.tolist()

def prepare_inputs(self, seqs: list[Sequence], is_prefill: bool):
    """å‡†å¤‡æ¨¡å‹è¾“å…¥æ•°æ®"""

    batch_size = len(seqs)

    if is_prefill:
        # Prefillé˜¶æ®µï¼šå¤„ç†å®Œæ•´åºåˆ—
        max_len = max(len(seq) for seq in seqs)

        # è¾“å…¥IDå¡«å……
        input_ids = torch.zeros((batch_size, max_len), dtype=torch.long, device="cuda")
        position_ids = torch.zeros((batch_size, max_len), dtype=torch.long, device="cuda")
        slot_mapping = torch.full((batch_size, max_len), -1, dtype=torch.long, device="cuda")

        for i, seq in enumerate(seqs):
            seq_len = len(seq)
            input_ids[i, :seq_len] = torch.tensor(seq.token_ids, device="cuda")
            position_ids[i, :seq_len] = torch.arange(seq_len, device="cuda")

            # è®¾ç½®KV Cacheæ§½ä½æ˜ å°„
            for j, block_id in enumerate(seq.block_table):
                start_pos = j * self.block_size
                end_pos = min(start_pos + self.block_size, seq_len)
                block_offset = 0

                for k in range(start_pos, end_pos):
                    global_slot = block_id * self.block_size + block_offset
                    slot_mapping[i, k] = global_slot
                    block_offset += 1

    else:
        # Decodeé˜¶æ®µï¼šåªå¤„ç†æœ€åä¸€ä¸ªtoken
        input_ids = torch.zeros((batch_size, 1), dtype=torch.long, device="cuda")
        position_ids = torch.zeros((batch_size, 1), dtype=torch.long, device="cuda")
        slot_mapping = torch.zeros((batch_size, 1), dtype=torch.long, device="cuda")

        for i, seq in enumerate(seqs):
            input_ids[i, 0] = seq.token_ids[-1]
            position_ids[i, 0] = len(seq) - 1

            # è®¡ç®—æœ€åä¸€ä¸ªtokençš„KV Cacheä½ç½®
            if len(seq) > 0:
                last_block_idx = (len(seq) - 1) // self.block_size
                last_block_offset = (len(seq) - 1) % self.block_size
                global_slot = seq.block_table[last_block_idx] * self.block_size + last_block_offset
                slot_mapping[i, 0] = global_slot

    return input_ids, position_ids, slot_mapping

def update_sequences(self, seqs: list[Sequence], next_token_ids: list[int]):
    """æ›´æ–°åºåˆ—çŠ¶æ€"""

    for seq, next_token_id in zip(seqs, next_token_ids):
        # æ·»åŠ æ–°token
        seq.token_ids.append(next_token_id)
        seq.num_tokens += 1
        seq.last_token = next_token_id

        # æ£€æŸ¥ç»“æŸæ¡ä»¶
        if (next_token_id == self.config.eos and not seq.ignore_eos) or \
           (seq.num_completion_tokens >= seq.max_tokens):
            seq.status = SequenceStatus.FINISHED

def update_kv_cache(self, seqs: list[Sequence], next_token_ids: list[int]):
    """æ›´æ–°KV Cache"""

    for seq, next_token_id in zip(seqs, next_token_ids):
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ–°çš„KVå—
        if len(seq) % self.block_size == 0:
            # éœ€è¦åˆ†é…æ–°å—
            new_block_id = self.allocate_new_kv_block()
            seq.block_table.append(new_block_id)

        # è®¡ç®—æ–°tokençš„å­˜å‚¨ä½ç½®
        seq_len = len(seq)
        block_idx = (seq_len - 1) // self.block_size
        block_offset = (seq_len - 1) % self.block_size

        # KV Cacheæ›´æ–°ä¼šåœ¨æ¨¡å‹çš„forwardä¸­è‡ªåŠ¨å®Œæˆ
        # è¿™é‡Œä¸»è¦æ˜¯ç¡®ä¿block_tableæ­£ç¡®
```

**æ¨ç†è¿‡ç¨‹çš„æ€§èƒ½ä¼˜åŒ–åˆ†æ**ï¼š

1. **Prefill vs Decodeçš„ä¸åŒç­–ç•¥**ï¼š
   ```python
   if is_prefill:
       # Prefillï¼šæ‰¹é‡å¤„ç†å®Œæ•´åºåˆ—
       # ä¼˜åŠ¿ï¼šå¹¶è¡Œåº¦é«˜ï¼Œé€‚åˆé•¿åºåˆ—çš„åˆå§‹è®¡ç®—
       max_len = max(len(seq) for seq in seqs)
   else:
       # Decodeï¼šåªå¤„ç†æœ€åtoken
       # ä¼˜åŠ¿ï¼šè®¡ç®—é‡å°ï¼Œå»¶è¿Ÿä½
       input_ids = torch.zeros((batch_size, 1), ...)
   ```

2. **å†…å­˜è®¿é—®ä¼˜åŒ–**ï¼š
   ```python
   # slot_mappingçš„ä½œç”¨ï¼šå°†é€»è¾‘ä½ç½®æ˜ å°„åˆ°ç‰©ç†ä½ç½®
   # è¿™æ ·å¯ä»¥ï¼š
   # 1. æ”¯æŒéè¿ç»­çš„KVå­˜å‚¨
   # 2. æ”¯æŒPrefix Caching
   # 3. ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
   ```

3. **æ‰¹å¤„ç†ä¼˜åŒ–**ï¼š
   ```python
   # åŒä¸€æ‰¹æ¬¡å†…çš„åºåˆ—é•¿åº¦å·®å¼‚å¤„ç†
   # ä½¿ç”¨paddingç¡®ä¿tensorå½¢çŠ¶ä¸€è‡´
   # é€šè¿‡attention_maské¿å…æ— æ•ˆè®¡ç®—
   ```

---

## ğŸ“‹ Sequenceï¼šåºåˆ—ç®¡ç†å™¨çš„æ·±åº¦è§£æ

### Sequenceçš„è®¾è®¡ç†å¿µ

Sequenceæ˜¯nano-vLLMä¸­æœ€é‡è¦çš„æ•°æ®ç»“æ„ä¹‹ä¸€ï¼Œå®ƒå°è£…äº†å•ä¸ªæ¨ç†è¯·æ±‚çš„å®Œæ•´çŠ¶æ€ã€‚ä¸€ä¸ªå¥½çš„Sequenceè®¾è®¡éœ€è¦ï¼š

1. **çŠ¶æ€å®Œæ•´æ€§**ï¼šè·Ÿè¸ªè¯·æ±‚çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸ
2. **å†…å­˜æ•ˆç‡**ï¼šæœ€å°åŒ–å†…å­˜å ç”¨
3. **è®¿é—®é«˜æ•ˆ**ï¼šå¿«é€Ÿè®¿é—®å…³é”®ä¿¡æ¯
4. **æ‰©å±•æ€§**ï¼šæ”¯æŒæ–°åŠŸèƒ½çš„æ·»åŠ 

### Sequenceçš„è¯¦ç»†å®ç°åˆ†æ

#### 1. Sequenceç±»çš„å®Œæ•´ç»“æ„

```python
class Sequence:
    # ç±»çº§åˆ«å¸¸é‡
    block_size = 256                    # KV Cacheå—å¤§å°
    counter = count()                  # å…¨å±€IDç”Ÿæˆå™¨

    def __init__(self, token_ids: list[int], sampling_params = SamplingParams()):
        # === åŸºæœ¬æ ‡è¯†ä¿¡æ¯ ===
        self.seq_id = next(Sequence.counter)    # å”¯ä¸€åºåˆ—ID
        self.status = SequenceStatus.WAITING    # åˆå§‹çŠ¶æ€

        # === Tokenç®¡ç† ===
        self.token_ids = copy(token_ids)        # å®Œæ•´çš„tokenåºåˆ—
        self.last_token = token_ids[-1]         # æœ€åä¸€ä¸ªtokenï¼ˆå¿«é€Ÿè®¿é—®ï¼‰
        self.num_tokens = len(token_ids)        # æ€»tokenæ•°
        self.num_prompt_tokens = len(token_ids)  # æç¤ºè¯tokenæ•°
        self.num_cached_tokens = 0               # å·²ç¼“å­˜çš„tokenæ•°

        # === KV Cacheç®¡ç† ===
        self.block_table = []                    # KV Cacheå—è¡¨

        # === é‡‡æ ·å‚æ•° ===
        self.temperature = sampling_params.temperature
        self.max_tokens = sampling_params.max_tokens
        self.ignore_eos = sampling_params.ignore_eos

        # === æ€§èƒ½ç»Ÿè®¡ ===
        self.created_time = time.time()         # åˆ›å»ºæ—¶é—´
        self.start_time = None                   # å¼€å§‹å¤„ç†æ—¶é—´
        self.completion_time = None             # å®Œæˆæ—¶é—´
        self.preemption_count = 0                # è¢«æŠ¢å æ¬¡æ•°
        self.priority_penalty = 0                # ä¼˜å…ˆçº§æƒ©ç½š

        # === ç¼“å­˜ä¼˜åŒ– ===
        self._completion_token_ids = None        # ç¼“å­˜çš„ç”Ÿæˆtoken_ids
        self._length = len(token_ids)            # ç¼“å­˜çš„é•¿åº¦

    # === å±æ€§è®¿é—®å™¨ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰ ===
    @property
    def is_finished(self):
        return self.status == SequenceStatus.FINISHED

    @property
    def num_completion_tokens(self):
        return self.num_tokens - self.num_prompt_tokens

    @property
    def completion_token_ids(self):
        """è·å–ç”Ÿæˆéƒ¨åˆ†çš„token_idsï¼ˆç¼“å­˜ä¼˜åŒ–ï¼‰"""
        if self._completion_token_ids is None:
            self._completion_token_ids = self.token_ids[self.num_prompt_tokens:]
        return self._completion_token_ids

    # === åºåˆ—æ“ä½œæ–¹æ³• ===
    def __len__(self):
        return self.num_tokens

    def __getitem__(self, key):
        return self.token_ids[key]

    def append_token(self, token_id: int):
        """æ·»åŠ æ–°tokenï¼ˆå¸¦ä¼˜åŒ–ï¼‰"""
        self.token_ids.append(token_id)
        self.num_tokens += 1
        self.last_token = token_id
        self._length += 1

        # æ¸…ç†ç¼“å­˜çš„ç”Ÿæˆtoken
        self._completion_token_ids = None

    def start_processing(self):
        """æ ‡è®°å¼€å§‹å¤„ç†"""
        self.status = SequenceStatus.RUNNING
        if self.start_time is None:
            self.start_time = time.time()

    def finish_processing(self):
        """æ ‡è®°å¤„ç†å®Œæˆ"""
        self.status = SequenceStatus.FINISHED
        self.completion_time = time.time()

    def get_processing_time(self):
        """è·å–å¤„ç†æ—¶é—´"""
        if self.start_time is None:
            return 0
        end_time = self.completion_time or time.time()
        return end_time - self.start_time

    def should_preempt(self, other_seq):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¢«å…¶ä»–åºåˆ—æŠ¢å """
        # 1. ä¼˜å…ˆçº§æƒ©ç½šæ¯”è¾ƒ
        if self.priority_penalty > other_seq.priority_penalty:
            return True
        elif self.priority_penalty < other_seq.priority_penalty:
            return False

        # 2. è¢«æŠ¢å æ¬¡æ•°æ¯”è¾ƒï¼ˆè¢«æŠ¢å å¤šçš„ä¼˜å…ˆçº§ä½ï¼‰
        if self.preemption_count > other_seq.preemption_count + 2:
            return True
        elif other_seq.preemption_count > self.preemption_count + 2:
            return False

        # 3. é•¿åº¦æ¯”è¾ƒï¼ˆé•¿åºåˆ—ä¼˜å…ˆè¢«æŠ¢å ï¼‰
        return len(self) > len(other_seq)

    def get_memory_usage(self):
        """ä¼°ç®—å†…å­˜ä½¿ç”¨é‡"""
        # Tokenå­˜å‚¨
        token_memory = len(self.token_ids) * 8  # å‡è®¾æ¯ä¸ªtoken 8å­—èŠ‚

        # KV Cacheï¼ˆç²—ç•¥ä¼°ç®—ï¼‰
        kv_memory = len(self.block_table) * self.block_size * 1024  # å‡è®¾æ¯ä¸ªKVé¡¹1KB

        # å…ƒæ•°æ®å­˜å‚¨
        metadata_memory = 1024  # å‡è®¾1KB

        return token_memory + kv_memory + metadata_memory

    def __str__(self):
        return (f"Sequence(id={self.seq_id}, status={self.status.name}, "
                f"tokens={self.num_tokens}, blocks={len(self.block_table)})")

    def __repr__(self):
        return self.__str__()
```

**Sequenceè®¾è®¡çš„æ·±åº¦åˆ†æ**ï¼š

1. **IDç”Ÿæˆç­–ç•¥**ï¼š
   ```python
   counter = count()  # ä½¿ç”¨itertools.count()ç”Ÿæˆå”¯ä¸€ID
   # ä¼˜åŠ¿ï¼š
   # 1. çº¿ç¨‹å®‰å…¨
   # 2. æ— é‡å¤é£é™©
   # 3. æ€§èƒ½é«˜æ•ˆ
   ```

2. **ç¼“å­˜ä¼˜åŒ–ç­–ç•¥**ï¼š
   ```python
   # ä½¿ç”¨@propertyç¼“å­˜è®¡ç®—ç»“æœ
   @property
   def completion_token_ids(self):
       if self._completion_token_ids is None:
           self._completion_token_ids = self.token_ids[self.num_prompt_tokens:]
       return self._completion_token_ids
   # é¿å…ï¼šæ¯æ¬¡è®¿é—®éƒ½é‡æ–°è®¡ç®—
   ```

3. **å†…å­˜ä¼˜åŒ–æŠ€å·§**ï¼š
   ```python
   # copy()è€Œä¸æ˜¯ç›´æ¥èµ‹å€¼
   self.token_ids = copy(token_ids)
   # é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
   ```

#### 2. SequenceçŠ¶æ€è½¬æ¢æœºåˆ¶

```python
class SequenceStateMachine:
    """åºåˆ—çŠ¶æ€æœºç®¡ç†"""

    def __init__(self):
        # çŠ¶æ€è½¬æ¢è§„åˆ™
        self.transitions = {
            SequenceStatus.WAITING: {
                SequenceStatus.RUNNING: self._waiting_to_running,
            },
            SequenceStatus.RUNNING: {
                SequenceStatus.WAITING: self._running_to_waiting,  # è¢«æŠ¢å 
                SequenceStatus.FINISHED: self._running_to_finished,
            },
            SequenceStatus.FINISHED: {
                # FINISHEDæ˜¯ç»ˆæ€ï¼Œä¸å…è®¸è½¬æ¢
            }
        }

        # çŠ¶æ€è½¬æ¢å›è°ƒ
        self.callbacks = {
            'enter_waiting': [],
            'enter_running': [],
            'enter_finished': [],
            'exit_waiting': [],
            'exit_running': [],
            'exit_finished': []
        }

    def transition(self, seq: Sequence, new_status: SequenceStatus, reason: str = ""):
        """æ‰§è¡ŒçŠ¶æ€è½¬æ¢"""

        old_status = seq.status

        if old_status == new_status:
            return  # çŠ¶æ€æœªå˜åŒ–

        # æ£€æŸ¥è½¬æ¢æ˜¯å¦åˆæ³•
        if new_status not in self.transitions.get(old_status, {}):
            raise ValueError(f"éæ³•çŠ¶æ€è½¬æ¢: {old_status} -> {new_status}")

        # æ‰§è¡Œé€€å‡ºå›è°ƒ
        self._execute_callbacks(f'exit_{old_status.name.lower()}', seq, old_status, new_status)

        # æ‰§è¡ŒçŠ¶æ€è½¬æ¢é€»è¾‘
        transition_func = self.transitions[old_status][new_status]
        transition_func(seq, reason)

        # æ›´æ–°çŠ¶æ€
        seq.status = new_status

        # æ‰§è¡Œè¿›å…¥å›è°ƒ
        self._execute_callbacks(f'enter_{new_status.name.lower()}', seq, old_status, new_status)

        # è®°å½•çŠ¶æ€è½¬æ¢
        self._log_transition(seq, old_status, new_status, reason)

    def _waiting_to_running(self, seq: Sequence, reason: str):
        """ç­‰å¾…åˆ°è¿è¡Œçš„è½¬æ¢"""
        seq.start_processing()
        seq.block_manager.allocate(seq)  # åˆ†é…KV Cache

    def _running_to_waiting(self, seq: Sequence, reason: str):
        """è¿è¡Œåˆ°ç­‰å¾…çš„è½¬æ¢ï¼ˆè¢«æŠ¢å ï¼‰"""
        seq.preemption_count += 1
        seq.block_manager.deallocate(seq)  # é‡Šæ”¾KV Cache

    def _running_to_finished(self, seq: Sequence, reason: str):
        """è¿è¡Œåˆ°å®Œæˆçš„è½¬æ¢"""
        seq.finish_processing()
        seq.block_manager.deallocate(seq)  # é‡Šæ”¾KV Cache

    def add_callback(self, event: str, callback):
        """æ·»åŠ çŠ¶æ€è½¬æ¢å›è°ƒ"""
        if event in self.callbacks:
            self.callbacks[event].append(callback)

    def _execute_callbacks(self, event: str, seq: Sequence, old_status, new_status):
        """æ‰§è¡Œå›è°ƒå‡½æ•°"""
        for callback in self.callbacks.get(event, []):
            try:
                callback(seq, old_status, new_status)
            except Exception as e:
                print(f"çŠ¶æ€è½¬æ¢å›è°ƒé”™è¯¯: {e}")

    def _log_transition(self, seq: Sequence, old_status, new_status, reason: str):
        """è®°å½•çŠ¶æ€è½¬æ¢"""
        print(f"[{time.time():.3f}] åºåˆ—{seq.seq_id}: {old_status.name} -> {new_status.name} ({reason})")
```

#### 3. Sequenceæ€§èƒ½ç›‘æ§

```python
class SequenceMonitor:
    """åºåˆ—æ€§èƒ½ç›‘æ§"""

    def __init__(self):
        self.metrics = {
            'total_sequences': 0,
            'completed_sequences': 0,
            'preempted_sequences': 0,
            'total_tokens': 0,
            'total_time': 0,
            'peak_memory': 0
        }

        self.sequence_histories = {}  # åºåˆ—å†å²è®°å½•
        self.performance_stats = {}   # æ€§èƒ½ç»Ÿè®¡

    def monitor_sequence(self, seq: Sequence):
        """ç›‘æ§å•ä¸ªåºåˆ—"""
        seq_id = seq.seq_id

        # è®°å½•åºåˆ—åˆ›å»º
        if seq_id not in self.sequence_histories:
            self.sequence_histories[seq_id] = {
                'created_time': seq.created_time,
                'status_history': [],
                'token_history': [],
                'preemption_history': []
            }
            self.metrics['total_sequences'] += 1

        # è®°å½•çŠ¶æ€å˜åŒ–
        history = self.sequence_histories[seq_id]
        if not history['status_history'] or history['status_history'][-1]['status'] != seq.status:
            history['status_history'].append({
                'status': seq.status,
                'timestamp': time.time(),
                'tokens': len(seq)
            })

        # è®°å½•tokenå†å²
        history['token_history'].append({
            'timestamp': time.time(),
            'token_count': len(seq)
        })

        # è®°å½•æŠ¢å å†å²
        if hasattr(seq, 'last_preemption_time'):
            history['preemption_history'].append(seq.last_preemption_time)

        # æ›´æ–°å…¨å±€æŒ‡æ ‡
        if seq.is_finished:
            self.metrics['completed_sequences'] += 1
            self.metrics['total_tokens'] += len(seq)
            self.metrics['total_time'] += seq.get_processing_time()

        # æ›´æ–°å³°å€¼å†…å­˜
        current_memory = seq.get_memory_usage()
        self.metrics['peak_memory'] = max(self.metrics['peak_memory'], current_memory)

    def get_performance_summary(self):
        """è·å–æ€§èƒ½æ‘˜è¦"""
        if self.metrics['completed_sequences'] == 0:
            return {}

        return {
            'completion_rate': self.metrics['completed_sequences'] / self.metrics['total_sequences'],
            'preemption_rate': self.metrics['preempted_sequences'] / self.metrics['total_sequences'],
            'avg_tokens_per_sequence': self.metrics['total_tokens'] / self.metrics['completed_sequences'],
            'avg_processing_time': self.metrics['total_time'] / self.metrics['completed_sequences'],
            'throughput': self.metrics['total_tokens'] / self.metrics['total_time'] if self.metrics['total_time'] > 0 else 0,
            'peak_memory_mb': self.metrics['peak_memory'] / (1024 * 1024)
        }

    def analyze_sequence_patterns(self):
        """åˆ†æåºåˆ—æ¨¡å¼"""
        patterns = {
            'length_distribution': {},
            'preemption_patterns': {},
            'processing_time_distribution': {}
        }

        for seq_id, history in self.sequence_histories.items():
            # é•¿åº¦åˆ†å¸ƒ
            final_length = history['token_history'][-1]['token_count']
            length_bucket = (final_length // 100) * 100  # 100ä¸º bucket size
            patterns['length_distribution'][length_bucket] = patterns['length_distribution'].get(length_bucket, 0) + 1

            # æŠ¢å æ¨¡å¼
            preemption_count = len(history['preemption_history'])
            patterns['preemption_patterns'][preemption_count] = patterns['preemption_patterns'].get(preemption_count, 0) + 1

        return patterns
```

---

## ğŸ”„ æ ¸å¿ƒç»„ä»¶é—´çš„åè°ƒæœºåˆ¶

### ç»„ä»¶äº¤äº’çš„æ—¶åºå›¾

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant LE as LLMEngine
    participant S as Scheduler
    participant BM as BlockManager
    participant MR as ModelRunner
    participant Seq as Sequence

    Note over U,Seq: è¯·æ±‚å¤„ç†æµç¨‹
    U->>LE: generate(prompts, params)
    LE->>Seq: åˆ›å»ºSequenceå¯¹è±¡
    LE->>S: add_request(sequence)

    Note over U,Seq: ä¸»æ¨ç†å¾ªç¯
    loop ç›´åˆ°æ‰€æœ‰è¯·æ±‚å®Œæˆ
        LE->>S: schedule()
        S->>BM: can_allocate(sequence)
        BM-->>S: True/False
        S->>BM: allocate(sequence)
        S-->>LE: return scheduled_seqs

        LE->>MR: run(scheduled_seqs, is_prefill)
        MR->>MR: prepare_inputs()
        MR->>Seq: æ›´æ–°tokençŠ¶æ€
        MR->>MR: model_forward()
        MR->>BM: è¯»å†™KV Cache
        MR-->>LE: return new_tokens

        LE->>S: postprocess(seqs, tokens)
        S->>Seq: æ›´æ–°å®ŒæˆçŠ¶æ€
        S->>BM: deallocate(completed_seqs)
    end

    LE-->>U: è¿”å›ç”Ÿæˆç»“æœ
```

### æ•°æ®æµå’Œæ§åˆ¶æµçš„è¯¦ç»†åˆ†æ

#### 1. è¯·æ±‚è°ƒåº¦çš„æ•°æ®æµ

```mermaid
flowchart TD
    A[ç”¨æˆ·è¯·æ±‚] --> B[LLMEngineæ¥æ”¶]
    B --> C[åˆ›å»ºSequenceå¯¹è±¡]
    C --> D[Scheduler.add_request]
    D --> E{ç­‰å¾…é˜Ÿåˆ—æ£€æŸ¥}

    E -->|æœ‰ç©ºé—´| F[ç›´æ¥åŠ å…¥ç­‰å¾…é˜Ÿåˆ—]
    E -->|é˜Ÿåˆ—æ»¡| G[è¯·æ±‚ç­‰å¾…æˆ–æ‹’ç»]

    F --> H[ç­‰å¾…è°ƒåº¦æ—¶æœº]
    H --> I[Scheduler.scheduleè°ƒç”¨]

    I --> J{èµ„æºæ£€æŸ¥}
    J -->|å†…å­˜å……è¶³| K[Prefillå¤„ç†]
    J -->|å†…å­˜ä¸è¶³| L[Decodeå¤„ç†æˆ–ç­‰å¾…]

    K --> M[BlockManager.allocate]
    M --> N[åˆ†é…KV Cache]
    N --> O[ç§»åŠ¨åˆ°è¿è¡Œé˜Ÿåˆ—]

    L --> P[æ£€æŸ¥æŠ¢å æ¡ä»¶]
    P -->|éœ€è¦æŠ¢å | Q[æŠ¢å é•¿åºåˆ—]
    Q --> R[é‡Šæ”¾èµ„æº]
    R --> S[é‡æ–°è°ƒåº¦]
```

#### 2. å†…å­˜ç®¡ç†çš„æ§åˆ¶æµ

```mermaid
stateDiagram-v2
    [*] --> ç©ºé—²: åˆå§‹åŒ–
    ç©ºé—² --> å·²åˆ†é…: åºåˆ—è¯·æ±‚
    å·²åˆ†é… --> ä½¿ç”¨ä¸­: å¼€å§‹å¤„ç†
    ä½¿ç”¨ä¸­ --> å…±äº«ä¸­: å…¶ä»–åºåˆ—å¤ç”¨
    å…±äº«ä¸­ --> ä½¿ç”¨ä¸­: åºåˆ—å®Œæˆ
    ä½¿ç”¨ä¸­ --> é‡Šæ”¾ä¸­: å¼•ç”¨è®¡æ•°å‡1
    é‡Šæ”¾ä¸­ --> ç©ºé—²: å¼•ç”¨è®¡æ•°å½’é›¶
    é‡Šæ”¾ä¸­ --> å·²åˆ†é…: ä»æœ‰å¼•ç”¨

    note right of å…±äº«ä¸­
        Prefix Cachingçš„
        æ ¸å¿ƒçŠ¶æ€
    end note
```

#### 3. æ€§èƒ½ä¼˜åŒ–çš„åè°ƒæœºåˆ¶

```python
class PerformanceCoordinator:
    """æ€§èƒ½ä¼˜åŒ–åè°ƒå™¨"""

    def __init__(self, llm_engine: LLMEngine):
        self.llm_engine = llm_engine
        self.scheduler = llm_engine.scheduler
        self.model_runner = llm_engine.model_runner

        # æ€§èƒ½ç›‘æ§
        self.monitor = PerformanceMonitor()
        self.optimizer = DynamicOptimizer()

    def coordinate_optimization(self):
        """åè°ƒå„ä¸ªç»„ä»¶çš„æ€§èƒ½ä¼˜åŒ–"""

        # 1. æ”¶é›†æ€§èƒ½æŒ‡æ ‡
        metrics = self.monitor.collect_metrics()

        # 2. åˆ†ææ€§èƒ½ç“¶é¢ˆ
        bottlenecks = self.analyze_bottlenecks(metrics)

        # 3. åè°ƒä¼˜åŒ–ç­–ç•¥
        for bottleneck in bottlenecks:
            if bottleneck.type == 'memory_pressure':
                self.optimize_memory_usage()
            elif bottleneck.type == 'low_throughput':
                self.optimize_throughput()
            elif bottleneck.type == 'high_latency':
                self.optimize_latency()

    def optimize_memory_usage(self):
        """å†…å­˜ä½¿ç”¨ä¼˜åŒ–"""
        # 1. æ¸…ç†æœªä½¿ç”¨çš„KV Cache
        self.scheduler.block_manager.cleanup_unused_blocks()

        # 2. è°ƒæ•´æ‰¹å¤„ç†å¤§å°
        current_batch_size = self.llm_engine.current_batch_size
        if current_batch_size > 16:
            self.llm_engine.adjust_batch_size(current_batch_size // 2)

        # 3. è§¦å‘åƒåœ¾å›æ”¶
        import gc
        gc.collect()

    def optimize_throughput(self):
        """ååé‡ä¼˜åŒ–"""
        # 1. å¢åŠ æ‰¹å¤„ç†å¤§å°
        current_batch_size = self.llm_engine.current_batch_size
        self.llm_engine.adjust_batch_size(min(current_batch_size * 1.5, 64))

        # 2. ä¼˜åŒ–è°ƒåº¦ç­–ç•¥
        self.scheduler.enable_aggressive_batching = True

        # 3. å¯ç”¨æ›´å¤šçš„CUDA Graph
        self.model_runner.enable_additional_graphs()

    def optimize_latency(self):
        """å»¶è¿Ÿä¼˜åŒ–"""
        # 1. å‡å°æ‰¹å¤„ç†å¤§å°
        current_batch_size = self.llm_engine.current_batch_size
        self.llm_engine.adjust_batch_size(max(current_batch_size // 2, 1))

        # 2. ä¼˜å…ˆå¤„ç†çŸ­åºåˆ—
        self.scheduler.enable_short_sequence_priority = True

        # 3. ç¦ç”¨ä¸€äº›ä¼˜åŒ–ä»¥å‡å°‘å»¶è¿Ÿ
        self.model_runner.disable_heavy_optimizations = True
```

---

## ğŸ’¡ æœ¬ç« æ€»ç»“

### å…³é”®è¦ç‚¹å›é¡¾

1. **LLMEngineçš„æ ¸å¿ƒä½œç”¨**ï¼š
   - ç»Ÿä¸€çš„ç”¨æˆ·æ¥å£å’Œæµç¨‹æ§åˆ¶
   - å¤šè¿›ç¨‹ç¯å¢ƒçš„åè°ƒå’Œç®¡ç†
   - æ€§èƒ½ç›‘æ§å’Œèµ„æºç®¡ç†

2. **Schedulerçš„æ™ºèƒ½è°ƒåº¦**ï¼š
   - ä¸¤é˜¶æ®µè°ƒåº¦ç­–ç•¥ï¼ˆPrefill/Decodeåˆ†ç¦»ï¼‰
   - æŠ¢å æœºåˆ¶ä¿è¯å…¬å¹³æ€§å’Œå“åº”æ€§
   - è‡ªé€‚åº”æ‰¹å¤„ç†ä¼˜åŒ–ååé‡

3. **ModelRunnerçš„é«˜æ•ˆæ‰§è¡Œ**ï¼š
   - CUDA Graphä¼˜åŒ–å‡å°‘æ‰§è¡Œå¼€é”€
   - å¼ é‡å¹¶è¡Œæ”¯æŒå¤šGPUè®¡ç®—
   - å†…å­˜ç®¡ç†å’ŒKV Cacheä¼˜åŒ–

4. **Sequenceçš„ç”Ÿå‘½å‘¨æœŸç®¡ç†**ï¼š
   - å®Œæ•´çš„çŠ¶æ€è½¬æ¢æœºåˆ¶
   - æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡
   - å†…å­˜é«˜æ•ˆçš„è¡¨ç¤º

### è®¾è®¡æ¨¡å¼æ€»ç»“

1. **å¤–è§‚æ¨¡å¼**ï¼šLLMEngineä¸ºå¤æ‚çš„å†…éƒ¨ç»„ä»¶æä¾›ç®€å•æ¥å£
2. **ç­–ç•¥æ¨¡å¼**ï¼šScheduleræ”¯æŒå¤šç§è°ƒåº¦ç­–ç•¥
3. **çŠ¶æ€æ¨¡å¼**ï¼šSequenceé€šè¿‡çŠ¶æ€æœºç®¡ç†ç”Ÿå‘½å‘¨æœŸ
4. **è§‚å¯Ÿè€…æ¨¡å¼**ï¼šæ€§èƒ½ç›‘æ§å’Œå›è°ƒæœºåˆ¶
5. **å·¥å‚æ¨¡å¼**ï¼šå„ç§ç»„ä»¶çš„åˆ›å»ºå’Œåˆå§‹åŒ–

### æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

1. **å†…å­˜ä¼˜åŒ–**ï¼šKV Cacheã€Prefix Cachingã€å¼•ç”¨è®¡æ•°
2. **è®¡ç®—ä¼˜åŒ–**ï¼šCUDA Graphã€å¼ é‡å¹¶è¡Œã€æ‰¹å¤„ç†
3. **è°ƒåº¦ä¼˜åŒ–**ï¼šä¸¤é˜¶æ®µè°ƒåº¦ã€æŠ¢å æœºåˆ¶ã€è‡ªé€‚åº”è°ƒæ•´
4. **æ•°æ®ç»“æ„ä¼˜åŒ–**ï¼šç¼“å­˜ã€å†…å­˜æ± ã€é«˜æ•ˆè®¿é—®æ¨¡å¼

### ä¸‹ä¸€æ­¥é¢„å‘Š

ä¸‹ä¸€ç« æˆ‘ä»¬å°†æ·±å…¥åˆ†æã€Šæ¨¡å‹å®ç°å±‚ã€‹ï¼ŒåŒ…æ‹¬ï¼š
- Qwen3æ¨¡å‹çš„æ¶æ„è®¾è®¡
- Transformerå±‚çš„å…·ä½“å®ç°
- æ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜åŒ–æŠ€æœ¯
- æ¨¡å‹åŠ è½½å’Œåˆå§‹åŒ–è¿‡ç¨‹

ç°åœ¨ä½ å·²ç»å¯¹nano-vLLMçš„æ ¸å¿ƒå¼•æ“æœ‰äº†æ·±å…¥çš„ç†è§£ï¼Œå‡†å¤‡å¥½è¿›å…¥æ¨¡å‹å±‚çš„åˆ†æäº†å—ï¼ŸğŸš€