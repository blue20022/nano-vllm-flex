# ç¬¬å››ç« ï¼šæ¨¡å‹å®ç°å±‚

## ğŸ“š æœ¬ç« ç›®æ ‡

é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œä½ å°†æ·±å…¥ç†è§£ï¼š
- Qwen3æ¨¡å‹çš„å®Œæ•´æ¶æ„è®¾è®¡å’Œå®ç°ç»†èŠ‚
- Transformerå±‚çš„å…·ä½“å®ç°å’Œä¼˜åŒ–æŠ€æœ¯
- æ³¨æ„åŠ›æœºåˆ¶çš„é«˜æ•ˆå®ç°å’ŒFlash Attentioné›†æˆ
- åµŒå…¥å±‚å’Œè¾“å‡ºå¤´çš„è®¾è®¡åŸç†
- æ¿€æ´»å‡½æ•°ã€å±‚å½’ä¸€åŒ–ç­‰åŸºç¡€å±‚çš„ä¼˜åŒ–
- å¼ é‡å¹¶è¡Œåœ¨æ¨¡å‹å±‚ä¸­çš„å…·ä½“åº”ç”¨
- æ¨¡å‹åŠ è½½å’Œåˆå§‹åŒ–çš„å®Œæ•´æµç¨‹

---

## ğŸ§  æ¨¡å‹å®ç°å±‚æ¦‚è§ˆ

nano-vLLMçš„æ¨¡å‹å®ç°å±‚æ˜¯æ•´ä¸ªç³»ç»Ÿçš„è®¡ç®—æ ¸å¿ƒï¼Œå®ƒè´Ÿè´£ï¼š
1. **æ¨¡å‹æ¶æ„å®šä¹‰**ï¼šå®ç°Qwen3çš„å®Œæ•´ç½‘ç»œç»“æ„
2. **é«˜æ•ˆå‰å‘è®¡ç®—**ï¼šä¼˜åŒ–æ¯å±‚çš„è®¡ç®—æ€§èƒ½
3. **å†…å­˜ç®¡ç†**ï¼šç²¾ç¡®æ§åˆ¶GPUå†…å­˜ä½¿ç”¨
4. **å¹¶è¡Œè®¡ç®—**ï¼šæ”¯æŒå¤šGPUå¼ é‡å¹¶è¡Œ

### æ¨¡å‹å±‚æ¶æ„å›¾

```mermaid
graph TB
    subgraph "æ¨¡å‹å®ç°å±‚æ¶æ„"
        Qwen3[Qwen3ForCausalLM<br/>æ ¹æ¨¡å‹ç±»]
        Embed[Embedding<br/>è¯åµŒå…¥å±‚]
        Layers[TransformerLayers<br/>å¤šå±‚Transformer]

        subgraph "å•ä¸ªTransformerå±‚"
            Attn[Qwen3Attention<br/>å¤šå¤´æ³¨æ„åŠ›]
            MLP[Qwen3MLP<br/>å‰é¦ˆç½‘ç»œ]
            Norm1[RMSNorm<br/>å±‚å½’ä¸€åŒ–1]
            Norm2[RMSNorm<br/>å±‚å½’ä¸€åŒ–2]
        end

        LMHead[LMHead<br/>è¾“å‡ºå¤´]
    end

    subgraph "è®¡ç®—æµç¨‹"
        Input[Input Tokens] --> Embed
        Embed --> Layers
        Layers --> LMHead
        LMHead --> Output[Output Logits]

        subgraph "å±‚å†…è®¡ç®—"
            Attn --> Norm1
            Norm1 --> MLP
            MLP --> Norm2
        end
    end

    subgraph "ä¼˜åŒ–æŠ€æœ¯"
        TP[å¼ é‡å¹¶è¡Œ]
        KV[KV Cache]
        FA[Flash Attention]
        CG[CUDA Graph]
    end

    Layers --> TP
    Layers --> KV
    Attn --> FA
    Layers --> CG
```

### å±‚æ¬¡åŒ–è®¾è®¡ç†å¿µ

```mermaid
mindmap
  root((æ¨¡å‹å±‚è®¾è®¡))
    æŠ½è±¡å±‚
      ç»Ÿä¸€æ¥å£
      é…ç½®é©±åŠ¨
      æ¨¡å—åŒ–è®¾è®¡
    å®ç°å±‚
      é«˜æ€§èƒ½è®¡ç®—
      å†…å­˜ä¼˜åŒ–
      å¹¶è¡Œæ”¯æŒ
    ä¼˜åŒ–å±‚
      CUDAæ ¸å‡½æ•°
      Flash Attention
      å¼ é‡å¹¶è¡Œ
    å·¥å…·å±‚
      æƒé‡åŠ è½½
      æ¨¡å‹éªŒè¯
      æ€§èƒ½åˆ†æ
```

---

## ğŸ—ï¸ Qwen3ForCausalLMï¼šæ ¹æ¨¡å‹ç±»

### Qwen3æ¨¡å‹çš„è®¾è®¡æ€æƒ³

Qwen3ForCausalLMæ˜¯nano-vLLMä¸­å®ç°çš„å…·ä½“æ¨¡å‹ï¼Œå®ƒç»§æ‰¿è‡ªHuggingFaceçš„è®¾è®¡æ¨¡å¼ï¼Œä½†é’ˆå¯¹æ¨ç†åœºæ™¯è¿›è¡Œäº†å¤§é‡ä¼˜åŒ–ã€‚

```python
from nanovllm.models.qwen3 import Qwen3ForCausalLM
```

### Qwen3ForCausalLMçš„å®Œæ•´å®ç°

```python
class Qwen3ForCausalLM(nn.Module):
    """Qwen3 Causal Language Model"""

    def __init__(self, config: Qwen3Config):
        super().__init__()
        self.config = config

        # === æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ– ===
        self.model = Qwen3Model(config)          # Transformeræ¨¡å‹ä¸»ä½“
        self.lm_head = ParallelLMHead(           # è¾“å‡ºå¤´ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰
            config.hidden_size,
            config.vocab_size,
            bias=False,
            tp_dim=0,  # è¯åµŒå…¥ç»´åº¦è¿›è¡Œåˆ‡åˆ†
            tp_size=get_world_size(),
        )

        # === åˆå§‹åŒ–æƒé‡ ===
        self.post_init()

    def forward(
        self,
        input_ids: torch.Tensor,              # [batch_size, seq_len]
        position_ids: torch.Tensor,           # [batch_size, seq_len]
        slot_mapping: torch.Tensor,           # [batch_size, seq_len]
    ) -> torch.Tensor:
        """å‰å‘æ¨ç†"""

        # 1. Transformerå‰å‘è®¡ç®—
        hidden_states = self.model(input_ids, position_ids, slot_mapping)  # [batch_size, seq_len, hidden_size]

        # 2. è¾“å‡ºå¤´æŠ•å½±åˆ°è¯è¡¨ç©ºé—´
        logits = self.lm_head(hidden_states)  # [batch_size, seq_len, vocab_size]

        return logits

    def post_init(self):
        """æ¨¡å‹åˆå§‹åŒ–åå¤„ç†"""

        # æƒé‡åˆå§‹åŒ–
        self.apply(self._init_weights)

        # è¯åµŒå…¥å’Œè¾“å‡ºå¤´çš„æƒé‡å…±äº«ï¼ˆå¯é€‰ï¼‰
        if self.config.tie_word_embeddings:
            self.lm_head.weight = self.model.embed_tokens.weight

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"æ¨¡å‹å‚æ•°é‡: {total_params:,} (å¯è®­ç»ƒ: {trainable_params:,})")

    def _init_weights(self, module):
        """æƒé‡åˆå§‹åŒ–å‡½æ•°"""

        if isinstance(module, nn.Linear):
            # çº¿æ€§å±‚æƒé‡åˆå§‹åŒ–
            torch.nn.init.normal_(
                module.weight,
                mean=0.0,
                std=self.config.initializer_range
            )
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)

        elif isinstance(module, nn.Embedding):
            # è¯åµŒå…¥å±‚åˆå§‹åŒ–
            torch.nn.init.normal_(
                module.weight,
                mean=0.0,
                std=self.config.initializer_range
            )

        elif isinstance(module, (RMSNorm, LlamaRMSNorm)):
            # RMSNormå±‚åˆå§‹åŒ–
            module.weight.data.fill_(1.0)
```

### Qwen3Modelï¼šæ¨¡å‹ä¸»ä½“å®ç°

```python
class Qwen3Model(nn.Module):
    """Qwen3æ¨¡å‹ä¸»ä½“"""

    def __init__(self, config: Qwen3Config):
        super().__init__()
        self.config = config

        # === åµŒå…¥å±‚ ===
        self.embed_tokens = VocabParallelEmbedding(
            config.vocab_size,
            config.hidden_size,
            tp_dim=0,  # è¯è¡¨ç»´åº¦åˆ‡åˆ†
            tp_size=get_world_size(),
        )

        # === Transformerå±‚ ===
        self.layers = nn.ModuleList([
            Qwen3DecoderLayer(config, layer_idx)
            for layer_idx in range(config.num_hidden_layers)
        ])

        # === æœ€ç»ˆå½’ä¸€åŒ–å±‚ ===
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        # === ç¼“å­˜ä½ç½®ç¼–ç  ===
        self.rotary_emb = get_rope(self.config)

    def forward(
        self,
        input_ids: torch.Tensor,
        position_ids: torch.Tensor,
        slot_mapping: torch.Tensor,
    ) -> torch.Tensor:
        """å‰å‘è®¡ç®—"""

        # 1. è¯åµŒå…¥
        hidden_states = self.embed_tokens(input_ids)  # [batch_size, seq_len, hidden_size]

        # 2. ä½ç½®ç¼–ç æ³¨å…¥ï¼ˆåœ¨attentionå±‚ä¸­ï¼‰
        # hidden_statesä¿æŒä¸å˜ï¼Œposition_idsä¼ é€’ç»™attentionå±‚

        # 3. é€å±‚å¤„ç†
        for idx, layer in enumerate(self.layers):
            # è·å–å¯¹åº”å±‚çš„æ—‹è½¬ä½ç½®ç¼–ç 
            cos, sin = self.rotary_emb(hidden_states, position_ids)

            # å‰å‘è®¡ç®—
            hidden_states = layer(
                hidden_states,
                position_ids,
                slot_mapping,
                cos,
                sin
            )

        # 4. æœ€ç»ˆå½’ä¸€åŒ–
        hidden_states = self.norm(hidden_states)

        return hidden_states
```

### è®¾è®¡å†³ç­–çš„æ·±åº¦åˆ†æ

#### 1. **è¯åµŒå…¥å’Œè¾“å‡ºå¤´çš„å…³ç³»**

```python
# è¯åµŒå…¥å’Œè¾“å‡ºå¤´æ˜¯å¦å…±äº«æƒé‡çš„å†³ç­–
if self.config.tie_word_embeddings:
    self.lm_head.weight = self.model.embed_tokens.weight

# ä¼˜åŠ¿ï¼š
# 1. å‡å°‘å‚æ•°é‡
# 2. æé«˜è®­ç»ƒç¨³å®šæ€§
# 3. ç¬¦åˆè¯­è¨€æ¨¡å‹çš„ç†è®ºåŸºç¡€
#
# åŠ£åŠ¿ï¼š
# 1. å¯èƒ½å½±å“è¡¨è¾¾èƒ½åŠ›
# 2. æ¨ç†æ—¶å¯èƒ½ä¸æ˜¯æœ€ä¼˜é€‰æ‹©
```

#### 2. **æ¨¡å—åŒ–è®¾è®¡çš„å¥½å¤„**

```python
# ä¸ºä»€ä¹ˆä½¿ç”¨ModuleListè€Œä¸æ˜¯å•ä¸ªå¤§æ¨¡å—ï¼Ÿ
self.layers = nn.ModuleList([...])
# ä¼˜åŠ¿ï¼š
# 1. ä¾¿äºå•ç‹¬è®¿é—®æ¯ä¸€å±‚
# 2. æ”¯æŒå±‚çº§çš„æ£€æŸ¥ç‚¹ä¿å­˜
# 3. ä¾¿äºè°ƒè¯•å’Œåˆ†æ
# 4. æ”¯æŒå±‚çº§çš„å¹¶è¡ŒåŒ–
```

#### 3. **ä½ç½®ç¼–ç çš„å¤„ç†ç­–ç•¥**

```python
# æ—‹è½¬ä½ç½®ç¼–ç åœ¨attentionå±‚ä¸­å¤„ç†
cos, sin = self.rotary_emb(hidden_states, position_ids)
hidden_states = layer(hidden_states, position_ids, cos, sin)

# ä¸ºä»€ä¹ˆä¸åœ¨æ¨¡å‹å¼€å§‹æ—¶å¤„ç†ï¼Ÿ
# 1. å‡å°‘å†…å­˜å ç”¨ï¼ˆåªå¤„ç†éœ€è¦çš„å±‚ï¼‰
# 2. æ”¯æŒä¸åŒå±‚ä½¿ç”¨ä¸åŒçš„ä½ç½®ç¼–ç 
# 3. ä¾¿äºæ”¯æŒä¸åŒé•¿åº¦åºåˆ—çš„å¤„ç†
```

---

## ğŸ”„ Qwen3DecoderLayerï¼šTransformerå±‚çš„æ·±åº¦è§£æ

### Transformerå±‚çš„è®¾è®¡æ¶æ„

æ¯ä¸ªTransformerå±‚æ˜¯æ¨¡å‹çš„åŸºæœ¬è®¡ç®—å•å…ƒï¼ŒåŒ…å«ï¼š
1. **å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼šå¤„ç†åºåˆ—é—´çš„å…³ç³»
2. **å‰é¦ˆç¥ç»ç½‘ç»œ**ï¼šéçº¿æ€§ç‰¹å¾å˜æ¢
3. **æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–**ï¼šç¨³å®šè®­ç»ƒå’Œæ¨ç†

### Qwen3DecoderLayerçš„å®Œæ•´å®ç°

```python
class Qwen3DecoderLayer(nn.Module):
    """Qwen3 Transformer Decoderå±‚"""

    def __init__(self, config: Qwen3Config, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx

        # === å¤šå¤´è‡ªæ³¨æ„åŠ› ===
        self.self_attn = Qwen3Attention(
            hidden_size=config.hidden_size,
            num_heads=config.num_attention_heads,
            num_kv_heads=config.num_key_value_heads,
            max_position=config.max_position_embeddings,
            rms_norm_eps=config.rms_norm_eps,
            rope_theta=config.rope_theta,
            rope_scaling=config.rope_scaling,
        )

        # === å‰é¦ˆç½‘ç»œ ===
        self.mlp = Qwen3MLP(
            hidden_size=config.hidden_size,
            intermediate_size=config.intermediate_size,
            hidden_act=config.hidden_act,
        )

        # === è¾“å…¥å½’ä¸€åŒ– ===
        self.input_layernorm = RMSNorm(
            config.hidden_size,
            eps=config.rms_norm_eps
        )

        # === è¾“å‡ºå½’ä¸€åŒ– ===
        self.post_attention_layernorm = RMSNorm(
            config.hidden_size,
            eps=config.rms_norm_eps
        )

    def forward(
        self,
        hidden_states: torch.Tensor,          # [batch_size, seq_len, hidden_size]
        position_ids: torch.Tensor,           # [batch_size, seq_len]
        slot_mapping: torch.Tensor,           # [batch_size, seq_len]
        cos: torch.Tensor,                    # æ—‹è½¬ä½ç½®ç¼–ç cos
        sin: torch.Tensor,                    # æ—‹è½¬ä½ç½®ç¼–ç sin
    ) -> torch.Tensor:
        """å‰å‘è®¡ç®—"""

        # === é¢„å½’ä¸€åŒ–æ³¨æ„åŠ›æœºåˆ¶ï¼ˆPre-Normï¼‰ ===
        # ä¸ºä»€ä¹ˆä½¿ç”¨Pre-Normè€Œä¸æ˜¯Post-Normï¼Ÿ
        # Pre-Norm: LayerNorm -> Attention -> Add
        # Post-Norm: Attention -> Add -> LayerNorm

        # Pre-Normçš„ä¼˜åŠ¿ï¼š
        # 1. è®­ç»ƒæ›´ç¨³å®š
        # 2. æ”¯æŒæ›´æ·±çš„ç½‘ç»œ
        # 3. æ¨ç†æ—¶æ€§èƒ½æ›´å¥½

        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)

        # === è‡ªæ³¨æ„åŠ›è®¡ç®— ===
        hidden_states, _ = self.self_attn(
            hidden_states=hidden_states,
            position_ids=position_ids,
            slot_mapping=slot_mapping,
            cos=cos,
            sin=sin
        )

        # === ç¬¬ä¸€ä¸ªæ®‹å·®è¿æ¥ ===
        hidden_states = hidden_states + residual

        # === ç¬¬äºŒä¸ªé¢„å½’ä¸€åŒ– ===
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)

        # === å‰é¦ˆç½‘ç»œè®¡ç®— ===
        hidden_states = self.mlp(hidden_states)

        # === ç¬¬äºŒä¸ªæ®‹å·®è¿æ¥ ===
        hidden_states = hidden_states + residual

        return hidden_states
```

### Pre-Norm vs Post-Normçš„è¯¦ç»†åˆ†æ

```python
def compare_norm_strategies():
    """æ¯”è¾ƒPre-Normå’ŒPost-Normçš„å½±å“"""

    # === Post-Normç»“æ„ ===
    class PostNormLayer(nn.Module):
        def forward(self, x):
            # æ³¨æ„åŠ›
            attn_out = self.attention(x)
            x = x + attn_out
            x = self.norm1(x)

            # å‰é¦ˆç½‘ç»œ
            mlp_out = self.mlp(x)
            x = x + mlp_out
            x = self.norm2(x)
            return x

    # === Pre-Normç»“æ„ ===
    class PreNormLayer(nn.Module):
        def forward(self, x):
            # æ³¨æ„åŠ›
            x = x + self.attention(self.norm1(x))

            # å‰é¦ˆç½‘ç»œ
            x = x + self.mlp(self.norm2(x))
            return x

    # === æ¯”è¾ƒåˆ†æ ===
    """
    Post-Normçš„é—®é¢˜ï¼š
    1. æ·±å±‚ç½‘ç»œæ—¶æ¢¯åº¦æ¶ˆå¤±
    2. è®­ç»ƒä¸ç¨³å®š
    3. éœ€è¦ç²¾å¿ƒè°ƒèŠ‚å­¦ä¹ ç‡

    Pre-Normçš„ä¼˜åŠ¿ï¼š
    1. æ¢¯åº¦æµæ›´ç¨³å®š
    2. æ”¯æŒæ›´æ·±çš„ç½‘ç»œï¼ˆ1000+å±‚ï¼‰
    3. è®­ç»ƒæ›´å®¹æ˜“æ”¶æ•›
    4. æ¨ç†æ€§èƒ½æ›´å¥½

    Qwen3é€‰æ‹©Pre-Normçš„åŸå› ï¼š
    - ç°ä»£å¤§æ¨¡å‹çš„æ ‡å‡†åšæ³•
    - æ›´å¥½çš„è®­ç»ƒç¨³å®šæ€§
    - æ¨ç†æ—¶æ€§èƒ½ä¼˜åŠ¿æ˜æ˜¾
    """
```

---

## ğŸ‘ï¸ Qwen3Attentionï¼šæ³¨æ„åŠ›æœºåˆ¶çš„æ·±åº¦è§£æ

### æ³¨æ„åŠ›æœºåˆ¶çš„è®¾è®¡ç†å¿µ

Qwen3Attentionå®ç°äº†å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶é›†æˆäº†å¤šé¡¹ä¼˜åŒ–æŠ€æœ¯ï¼š
1. **åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰**ï¼šå‡å°‘è®¡ç®—é‡
2. **Flash Attention**ï¼šå†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—
3. **æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰**ï¼šå¤„ç†ä½ç½®ä¿¡æ¯
4. **å¼ é‡å¹¶è¡Œ**ï¼šæ”¯æŒå¤šGPUè®¡ç®—

### Qwen3Attentionçš„å®Œæ•´å®ç°

```python
class Qwen3Attention(nn.Module):
    """Qwen3å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚"""

    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        num_kv_heads: int,
        max_position: int = 4096 * 32,
        head_dim: int | None = None,
        rms_norm_eps: float = 1e-6,
        qkv_bias: bool = False,
        rope_theta: float = 10000,
        rope_scaling: tuple | None = None,
    ):
        super().__init__()

        # æ³¨æ„åŠ›å¤´é…ç½®
        self.total_num_heads = num_heads
        self.total_num_kv_heads = num_kv_heads
        assert self.total_num_heads % get_world_size() == 0
        assert self.total_num_kv_heads % get_world_size() == 0

        # å¼ é‡å¹¶è¡Œè®¡ç®—
        self.num_heads = self.total_num_heads // get_world_size()
        self.num_kv_heads = self.total_num_kv_heads // get_world_size()

        self.head_dim = head_dim or hidden_size // self.total_num_heads
        self.q_size = self.num_heads * self.head_dim
        self.kv_size = self.num_kv_heads * self.head_dim
        self.scaling = self.head_dim ** -0.5
        self.qkv_bias = qkv_bias

        # === QKVæŠ•å½±å±‚ ===
        self.qkv_proj = QKVParallelLinear(
            hidden_size,
            self.head_dim,
            self.total_num_heads,
            self.total_num_kv_heads,
            bias=qkv_bias,
            tp_size=get_world_size(),
        )

        # === è¾“å‡ºæŠ•å½±å±‚ ===
        self.o_proj = RowParallelLinear(
            self.total_num_heads * self.head_dim,
            hidden_size,
            bias=False,
            tp_size=get_world_size(),
        )

        # === KV Cacheç®¡ç† ===
        self.k_cache = None
        self.v_cache = None

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_ids: torch.Tensor,
        slot_mapping: torch.Tensor,
        cos: torch.Tensor,
        sin: torch.Tensor,
    ) -> tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]:
        """å‰å‘è®¡ç®—"""

        batch_size, seq_len, _ = hidden_states.shape

        # === QKVæŠ•å½± ===
        # åˆå¹¶è®¡ç®—å‡å°‘å†…å­˜è®¿é—®
        qkv = self.qkv_proj(hidden_states)  # [batch_size, seq_len, q_size + 2*kv_size]

        # åˆ†ç¦»Q, K, V
        query_states = qkv[..., :self.q_size]            # [batch_size, seq_len, q_size]
        key_states = qkv[..., self.q_size:self.q_size + self.kv_size]  # [batch_size, seq_len, kv_size]
        value_states = qkv[..., self.q_size + self.kv_size:]        # [batch_size, seq_len, kv_size]

        # === é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼ ===
        query_states = query_states.view(
            batch_size, seq_len, self.num_heads, self.head_dim
        ).transpose(1, 2)  # [batch_size, num_heads, seq_len, head_dim]

        key_states = key_states.view(
            batch_size, seq_len, self.num_kv_heads, self.head_dim
        ).transpose(1, 2)  # [batch_size, num_kv_heads, seq_len, head_dim]

        value_states = value_states.view(
            batch_size, seq_len, self.num_kv_heads, self.head_dim
        ).transpose(1, 2)  # [batch_size, num_kv_heads, seq_len, head_dim]

        # === æ—‹è½¬ä½ç½®ç¼–ç  ===
        query_states, key_states = apply_rotary_pos_emb(
            query_states, key_states, cos, sin
        )

        # === KV Cacheå¤„ç† ===
        if self.training:
            # è®­ç»ƒæ¨¡å¼ï¼šç›´æ¥è®¡ç®—æ³¨æ„åŠ›
            attn_output = torch.matmul(query_states, key_states.transpose(-2, -1))
            attn_output = attn_output / self.scaling
            attn_output = torch.softmax(attn_output, dim=-1)
            attn_output = torch.matmul(attn_output, value_states)
        else:
            # æ¨ç†æ¨¡å¼ï¼šä½¿ç”¨KV Cacheå’ŒFlash Attention
            attn_output = flash_attn_varlen_func(
                q=query_states,
                k=key_states,
                v=value_states,
                cu_seqlens_q=None,
                cu_seqlens_k=None,
                max_seqlen_q=seq_len,
                max_seqlen_k=seq_len,
                dropout_p=0.0,
                softmax_scale=self.scaling,
                causal=True,
            )

        # === é‡å¡‘è¾“å‡º ===
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(
            batch_size, seq_len, self.num_heads * self.head_dim
        )

        # === è¾“å‡ºæŠ•å½± ===
        attn_output = self.o_proj(attn_output)

        return attn_output, (key_states, value_states)
```

### åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰çš„è¯¦ç»†å®ç°

```python
def implement_grouped_query_attention():
    """å®ç°åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›çš„è¯¦ç»†è¯´æ˜"""

    class GroupedQueryAttention(nn.Module):
        """åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›å®ç°"""

        def __init__(self, num_heads, num_kv_heads, head_dim):
            super().__init__()
            self.num_heads = num_heads
            self.num_kv_heads = num_kv_heads
            self.head_dim = head_dim

            # è®¡ç®—æ¯ç»„åŒ…å«å¤šå°‘ä¸ªæŸ¥è¯¢å¤´
            self.group_size = num_heads // num_kv_heads
            assert num_heads % num_kv_heads == 0, "num_heads must be divisible by num_kv_heads"

        def forward(self, q, k, v):
            """
            Args:
                q: [batch_size, num_heads, seq_len, head_dim]
                k: [batch_size, num_kv_heads, seq_len, head_dim]
                v: [batch_size, num_kv_heads, seq_len, head_dim]
            """
            batch_size, _, seq_len, _ = q.shape

            # === å…³é”®æ­¥éª¤ï¼šæ‰©å±•Kå’ŒVä»¥åŒ¹é…Qçš„å¤´æ•° ===
            # æ¯ä¸ªKVå¤´ä¼šè¢«é‡å¤self.group_sizeæ¬¡
            k_expanded = k.unsqueeze(2).expand(-1, -1, self.group_size, -1, -1)
            v_expanded = v.unsqueeze(2).expand(-1, -1, self.group_size, -1, -1)

            # é‡æ–°å¡‘å½¢ä»¥åŒ¹é…Qçš„å¤´æ•°
            k_expanded = k_expanded.reshape(
                batch_size, num_heads, seq_len, head_dim
            )
            v_expanded = v_expanded.reshape(
                batch_size, num_heads, seq_len, head_dim
            )

            # === è®¡ç®—æ³¨æ„åŠ› ===
            attn_scores = torch.matmul(q, k_expanded.transpose(-2, -1))
            attn_weights = torch.softmax(attn_scores / math.sqrt(self.head_dim), dim=-1)
            attn_output = torch.matmul(attn_weights, v_expanded)

            return attn_output

    # === GQAçš„è¯¦ç»†ç¤ºä¾‹ ===
    """
    å‡è®¾é…ç½®ï¼š
    - num_heads = 32
    - num_kv_heads = 8
    - group_size = 32 / 8 = 4

    æŸ¥è¯¢å¤´åˆ†ç»„ï¼š
    Q0-Q3 -> K0-V0 (ç¬¬1ç»„)
    Q4-Q7 -> K1-V1 (ç¬¬2ç»„)
    Q8-Q11 -> K2-V2 (ç¬¬3ç»„)
    Q12-Q15 -> K3-V3 (ç¬¬4ç»„)
    Q16-Q19 -> K4-V4 (ç¬¬5ç»„)
    Q20-Q23 -> K5-V5 (ç¬¬6ç»„)
    Q24-Q27 -> K6-V6 (ç¬¬7ç»„)
    Q28-Q31 -> K7-V7 (ç¬¬8ç»„)

    ä¼˜åŠ¿ï¼š
    1. å‡å°‘KV Cacheçš„å†…å­˜å ç”¨ (8/32 = 25%)
    2. ä¿æŒå¤§éƒ¨åˆ†çš„è¡¨è¾¾èƒ½åŠ›
    3. æ˜¾è‘—çš„æ¨ç†åŠ é€Ÿæ•ˆæœ
    """

    # === æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹ ===
    def compare_attention_performance():
        """æ¯”è¾ƒä¸åŒæ³¨æ„åŠ›æ–¹æ³•çš„æ€§èƒ½"""

        configs = {
            'MHA': {'num_heads': 32, 'num_kv_heads': 32},  # å¤šå¤´æ³¨æ„åŠ›
            'MQA': {'num_heads': 32, 'num_kv_heads': 1},   # å¤šæŸ¥è¯¢æ³¨æ„åŠ›
            'GQA': {'num_heads': 32, 'num_kv_heads': 8},   # åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›
        }

        for method, config in configs.items():
            num_heads = config['num_heads']
            num_kv_heads = config['num_kv_heads']
            memory_ratio = num_kv_heads / num_heads

            print(f"{method}:")
            print(f"  æŸ¥è¯¢å¤´æ•°: {num_heads}")
            print(f" KVå¤´æ•°: {num_kv_heads}")
            print(f" å†…å­˜å ç”¨æ¯”ä¾‹: {memory_ratio:.2%}")
            print(f" ç†è®ºåŠ é€Ÿæ¯”: {1/memory_ratio:.2f}x")
            print()

# è¿è¡Œæ€§èƒ½æ¯”è¾ƒ
compare_attention_performance()
```

### Flash Attentionçš„é›†æˆ

```python
def flash_attention_implementation():
    """Flash Attentionçš„å®ç°ç»†èŠ‚"""

    class FlashAttentionOptimized(nn.Module):
        """ä½¿ç”¨Flash Attentionä¼˜åŒ–çš„æ³¨æ„åŠ›å±‚"""

        def __init__(self, head_dim, causal=True, dropout_p=0.0):
            super().__init__()
            self.head_dim = head_dim
            self.causal = causal
            self.dropout_p = dropout_p
            self.softmax_scale = 1.0 / math.sqrt(head_dim)

        def forward(self, q, k, v, cu_seqlens_q=None, cu_seqlens_k=None,
                   max_seqlen_q=None, max_seqlen_k=None):
            """
            Flash Attentionçš„æ ¸å¿ƒä¼˜åŠ¿ï¼š
            1. ç®—æ³•å¤æ‚åº¦: O(N^2) -> O(N)
            2. å†…å­˜ä½¿ç”¨: ä¸ä¿å­˜å®Œæ•´çš„æ³¨æ„åŠ›çŸ©é˜µ
            3. æ•°å€¼ç¨³å®šæ€§: åœ¨çº¿softmaxè®¡ç®—
            """

            # === Flash Attentionè°ƒç”¨ ===
            # è¿™é‡Œçš„å…³é”®æ˜¯è¦å‡†å¤‡å¥½æ­£ç¡®çš„å‚æ•°
            attn_output = flash_attn_varlen_func(
                q,  # [total_q, num_heads, head_dim]
                k,  # [total_k, num_kv_heads, head_dim]
                v,  # [total_k, num_kv_heads, head_dim]
                cu_seqlens_q=cu_seqlens_q,  # æŸ¥è¯¢åºåˆ—çš„ç´¯ç§¯é•¿åº¦
                cu_seqlens_k=cu_seqlens_k,  # é”®å€¼åºåˆ—çš„ç´¯ç§¯é•¿åº¦
                max_seqlen_q=max_seqlen_q,  # æŸ¥è¯¢åºåˆ—æœ€å¤§é•¿åº¦
                max_seqlen_k=max_seqlen_k,  # é”®å€¼åºåˆ—æœ€å¤§é•¿åº¦
                dropout_p=self.dropout_p,
                softmax_scale=self.softmax_scale,
                causal=self.causal,
                return_attn_probs=False,  # ä¸è¿”å›æ³¨æ„åŠ›æƒé‡ä»¥èŠ‚çœå†…å­˜
            )

            return attn_output

    # === Flash Attention vs ä¼ ç»Ÿæ³¨æ„åŠ›çš„å¯¹æ¯” ===
    """
    ä¼ ç»Ÿæ³¨æ„åŠ›ï¼ˆO(N^2)å†…å­˜ï¼‰ï¼š
    attention_weights = softmax(Q @ K^T / sqrt(d))  # [N, N] çŸ©é˜µ
    output = attention_weights @ V                     # å¦ä¸€ä¸ª [N, N] çŸ©é˜µ

    é—®é¢˜ï¼š
    - å¯¹äºé•¿åºåˆ—ï¼Œå†…å­˜ä½¿ç”¨å‘ˆå¹³æ–¹å¢é•¿
    - æ— æ³•å¤„ç†è¶…è¿‡GPUå†…å­˜çš„åºåˆ—
    - è®¡ç®—å’Œå†…å­˜è®¿é—®ä¸å‹å¥½

    Flash Attentionï¼ˆO(N)å†…å­˜ï¼‰ï¼š
    - åˆ†å—è®¡ç®—ï¼Œé¿å…å­˜å‚¨å®Œæ•´æ³¨æ„åŠ›çŸ©é˜µ
    - åœ¨çº¿è®¡ç®—softmaxï¼Œå‡å°‘å†…å­˜è®¿é—®
    - æ”¯æŒæ›´é•¿çš„åºåˆ—å¤„ç†
    """

    def flash_attention_benefits():
        """Flash Attentionçš„æ€§èƒ½ä¼˜åŠ¿åˆ†æ"""

        # å†…å­˜ä½¿ç”¨å¯¹æ¯”
        seq_length = 8192  # 8Kåºåˆ—é•¿åº¦
        head_dim = 128
        num_heads = 32
        batch_size = 1

        # ä¼ ç»Ÿæ³¨æ„åŠ›å†…å­˜éœ€æ±‚
        traditional_memory = batch_size * num_heads * seq_length * seq_length * 4  # float32
        traditional_memory_gb = traditional_memory / (1024**3)

        # Flash Attentionå†…å­˜éœ€æ±‚
        flash_memory = batch_size * num_heads * seq_length * head_dim * 4 * 3  # Q+K+V
        flash_memory_gb = flash_memory / (1024**3)

        print(f"åºåˆ—é•¿åº¦: {seq_length}")
        print(f"ä¼ ç»Ÿæ³¨æ„åŠ›å†…å­˜: {traditional_memory_gb:.2f} GB")
        print(f"Flash Attentionå†…å­˜: {flash_memory_gb:.2f} GB")
        print(f"å†…å­˜èŠ‚çœ: {(1 - flash_memory_gb/traditional_memory_gb)*100:.1f}%")

# è¿è¡Œåˆ†æ
flash_attention_benefits()
```

### æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰çš„å®ç°

```python
def rotary_position_embedding_implementation():
    """æ—‹è½¬ä½ç½®ç¼–ç çš„è¯¦ç»†å®ç°"""

    class RotaryEmbedding(nn.Module):
        """æ—‹è½¬ä½ç½®ç¼–ç å®ç°"""

        def __init__(self, dim, max_position_embeddings=2048, base=10000):
            super().__init__()
            self.dim = dim
            self.max_position_embeddings = max_position_embeddings
            self.base = base

            # === è®¡ç®—é¢‘ç‡ ===
            inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
            self.register_buffer("inv_freq", inv_freq, persistent=False)

            # === é¢„è®¡ç®—coså’Œsin ===
            self._set_cos_sin_cache(seq_len=max_position_embeddings)

        def _set_cos_sin_cache(self, seq_len):
            """è®¾ç½®coså’Œsinç¼“å­˜"""
            self.max_seq_len_cached = seq_len
            t = torch.arange(seq_len, device=self.inv_freq.device)
            freqs = torch.einsum("i,j->ij", t, self.inv_freq)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos()[None, None, :, :]
            sin = emb.sin()[None, None, :, :]
            self.register_buffer("cos_cached", cos, persistent=False)
            self.register_buffer("sin_cached", sin, persistent=False)

        def forward(self, x, seq_len=None):
            """è·å–ä½ç½®ç¼–ç """
            if seq_len > self.max_seq_len_cached:
                self._set_cos_sin_cache(seq_len=seq_len)
            return (
                self.cos_cached[..., :seq_len, :],
                self.sin_cached[..., :seq_len, :],
            )

    def apply_rotary_pos_emb(q, k, cos, sin, position_ids):
        """åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç """

        def rotate_half(x):
            """æ—‹è½¬çŸ©é˜µçš„ä¸€åŠ"""
            x1 = x[..., : x.shape[-1] // 2]
            x2 = x[..., x.shape[-1] // 2:]
            return torch.cat([-x2, x1], dim=-1)

        # === æ ¹æ®ä½ç½®è·å–å¯¹åº”çš„coså’Œsin ===
        cos = cos[position_ids].unsqueeze(-1)  # [batch_size, seq_len, 1, head_dim/2]
        sin = sin[position_ids].unsqueeze(-1)  # [batch_size, seq_len, 1, head_dim/2]

        # === åº”ç”¨æ—‹è½¬ ===
        # q * cos + rotate_half(q) * sin
        q_embed = (q.float() * cos) + (rotate_half(q.float()) * sin)
        k_embed = (k.float() * cos) + (rotate_half(k.float()) * sin)

        return q_embed.type_as(q), k_embed.type_as(k)

    # === RoPEçš„æ•°å­¦åŸç† ===
    """
    æ—‹è½¬ä½ç½®ç¼–ç çš„æ•°å­¦è¡¨è¾¾å¼ï¼š

    RoPE(q_m, m) = q_m * cos(mÎ¸) + (rotate_half(q_m)) * sin(mÎ¸)

    å…¶ä¸­ï¼š
    - q_m: ç¬¬mä¸ªä½ç½®çš„æŸ¥è¯¢å‘é‡
    - Î¸: é¢‘ç‡å‘é‡ [Î¸â‚€, Î¸â‚, ..., Î¸_{d/2-1}]
    - rotate_half: æ—‹è½¬æ“ä½œ
    - cos(mÎ¸), sin(mÎ¸): æ—‹è½¬çŸ©é˜µçš„å…ƒç´ 

    ä¼˜åŠ¿ï¼š
    1. ç»å¯¹ä½ç½®ç¼–ç ï¼šä¸éœ€è¦ç›¸å¯¹ä½ç½®ä¿¡æ¯
    2. å¤–æ¨æ€§å¥½ï¼šå¯ä»¥å¤„ç†è®­ç»ƒæ—¶æ²¡è§è¿‡çš„é•¿åº¦
    3. è®¡ç®—é«˜æ•ˆï¼šåªéœ€å‘é‡æ—‹è½¬
    """

    def rope_example():
        """RoPEçš„å…·ä½“è®¡ç®—ç¤ºä¾‹"""

        # å‡è®¾æŸ¥è¯¢å‘é‡å’Œé¢‘ç‡
        q = torch.tensor([1.0, 2.0, 3.0, 4.0])  # 4ç»´æŸ¥è¯¢å‘é‡
        theta = torch.tensor([0.5, 1.0])         # é¢‘ç‡ [Î¸â‚€, Î¸â‚]
        position = 3                            # ä½ç½®m

        # === è®¡ç®—mÎ¸ ===
        m_theta = position * theta  # [1.5, 3.0]

        # === è®¡ç®—coså’Œsin ===
        cos_vals = torch.cos(m_theta)  # [cos(1.5), cos(3.0)]
        sin_vals = torch.sin(m_theta)  # [sin(1.5), sin(3.0)]

        # === æ—‹è½¬æŸ¥è¯¢å‘é‡ ===
        q_half1 = q[:2]   # [1.0, 2.0]
        q_half2 = q[2:]   # [3.0, 4.0]

        # rotate_halfæ“ä½œ
        rotated_half = torch.cat([-q_half2, q_half1])  # [-3.0, -4.0, 1.0, 2.0]

        # === åº”ç”¨æ—‹è½¬ç¼–ç  ===
        cos_expanded = torch.cat([cos_vals, cos_vals])  # [cos(1.5), cos(3.0), cos(1.5), cos(3.0)]
        sin_expanded = torch.cat([sin_vals, sin_vals])  # [sin(1.5), sin(3.0), sin(1.5), sin(3.0)]

        q_encoded = q * cos_expanded + rotated_half * sin_expanded

        print(f"åŸå§‹æŸ¥è¯¢å‘é‡: {q}")
        print(f"æ—‹è½¬ç¼–ç å: {q_encoded}")
        print(f"é¢‘ç‡å‘é‡: {theta}")
        print(f"ä½ç½®: {position}")

# è¿è¡Œç¤ºä¾‹
rope_example()
```

---

## ğŸŒ Qwen3MLPï¼šå‰é¦ˆç½‘ç»œçš„å®ç°

### MLPçš„è®¾è®¡å’Œä¼˜åŒ–

Qwen3MLPå®ç°äº†Transformerä¸­çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œé‡‡ç”¨äº†SwiGLUæ¿€æ´»å‡½æ•°å’Œå¼ é‡å¹¶è¡Œä¼˜åŒ–ã€‚

```python
class Qwen3MLP(nn.Module):
    """Qwen3å‰é¦ˆç½‘ç»œå±‚"""

    def __init__(
        self,
        hidden_size: int,
        intermediate_size: int,
        hidden_act: str = "silu",
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size

        # === é—¨æ§çº¿æ€§å±‚ï¼ˆç¬¬ä¸€ä¸ªçº¿æ€§å±‚ï¼‰ ===
        self.gate_proj = MergedColumnParallelLinear(
            input_size=hidden_size,
            output_sizes=[intermediate_size, intermediate_size],  # é—¨æ§å’ŒæŠ•å½±
            bias=False,
            tp_dim=0,
            tp_size=get_world_size(),
        )

        # === ç¬¬äºŒä¸ªçº¿æ€§å±‚ ===
        self.down_proj = RowParallelLinear(
            input_size=intermediate_size,
            output_size=hidden_size,
            bias=False,
            tp_dim=1,
            tp_size=get_world_size(),
        )

        # === æ¿€æ´»å‡½æ•° ===
        if hidden_act.lower() == "silu":
            self.act_fn = SiluAndMul()
        else:
            raise ValueError(f"Unsupported activation: {hidden_act}")

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘è®¡ç®—"""

        # === é—¨æ§æœºåˆ¶ ===
        # x ç»è¿‡ä¸¤ä¸ªå¹¶è¡Œçš„çº¿æ€§å˜æ¢ï¼Œç„¶åé€šè¿‡æ¿€æ´»å‡½æ•°
        gate_up = self.gate_proj(x)  # [batch_size, seq_len, 2*intermediate_size]

        # åˆ†ç¦»é—¨æ§å’Œä¸ŠæŠ•å½±
        gate, up = gate_up.chunk(2, dim=-1)  # å„è‡ªæ˜¯ [batch_size, seq_len, intermediate_size]

        # === SwiGLUæ¿€æ´»å‡½æ•° ===
        # SwiGLU(x) = Swish(gate) * up
        # Swish(x) = x * sigmoid(x)
        intermediate = self.act_fn(gate, up)  # [batch_size, seq_len, intermediate_size]

        # === ä¸‹æŠ•å½± ===
        output = self.down_proj(intermediate)  # [batch_size, seq_len, hidden_size]

        return output
```

### SwiGLUæ¿€æ´»å‡½æ•°çš„å®ç°

```python
class SiluAndMul(nn.Module):
    """SwiGLUæ¿€æ´»å‡½æ•°ï¼šSiLU(x) * y"""

    def __init__(self):
        super().__init__()

    @torch.compile
    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—SwiGLU"""
        # SiLU(x) = x * sigmoid(x)
        # SwiGLU(x, y) = SiLU(x) * y = x * sigmoid(x) * y

        # ä¼˜åŒ–å®ç°ï¼šç›´æ¥è®¡ç®—x * sigmoid(x) * y
        return F.silu(x) * y

def activation_function_comparison():
    """æ¯”è¾ƒä¸åŒæ¿€æ´»å‡½æ•°çš„ç‰¹æ€§"""

    # === SwiGLUä¸å…¶ä»–æ¿€æ´»å‡½æ•°çš„æ¯”è¾ƒ ===
    def compare_activations():
        x = torch.randn(1000, 256)  # éšæœºè¾“å…¥

        # ReLU
        relu_out = F.relu(x)

        # GELU
        gelu_out = F.gelu(x)

        # Swish
        swish_out = F.silu(x)

        # SwiGLU (éœ€è¦ä¸¤ä¸ªè¾“å…¥)
        y = torch.randn_like(x)
        swiglu_out = F.silu(x) * y

        print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
        print(f"ReLUè¾“å‡ºå‡å€¼: {relu_out.mean():.4f}")
        print(f"GELUè¾“å‡ºå‡å€¼: {gelu_out.mean():.4f}")
        print(f"Swishè¾“å‡ºå‡å€¼: {swish_out.mean():.4f}")
        print(f"SwiGLUè¾“å‡ºå‡å€¼: {swiglu_out.mean():.4f}")

    # === SwiGLUçš„ä¼˜åŠ¿åˆ†æ ===
    def swiglu_advantages():
        """
        SwiGLUåœ¨å¤§æ¨¡å‹ä¸­çš„ä¼˜åŠ¿ï¼š

        1. å¹³æ»‘æ€§ï¼š
           - æ²¡æœ‰ç¡¬è¾¹ç•Œï¼Œæ¢¯åº¦æµåŠ¨æ›´å¹³æ»‘
           - é¿å…ç¥ç»å…ƒ"æ­»äº¡"

        2. è¡¨è¾¾èƒ½åŠ›ï¼š
           - é—¨æ§æœºåˆ¶æä¾›æ›´å¤§çš„çµæ´»æ€§
           - å¯ä»¥å­¦ä¹ å¤æ‚çš„éçº¿æ€§å˜æ¢

        3. è®­ç»ƒç¨³å®šæ€§ï¼š
           - è‡ªå½’ä¸€åŒ–ç‰¹æ€§æœ‰åŠ©äºè®­ç»ƒç¨³å®š
           - æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜è¾ƒå°‘

        4. è®¡ç®—æ•ˆç‡ï¼š
           - å¯ä»¥é«˜æ•ˆå®ç°
           - ä¸ç°ä»£GPUæ¶æ„å…¼å®¹æ€§å¥½
        """

    # === ä¸ºä»€ä¹ˆåœ¨å¤§æ¨¡å‹ä¸­é¦–é€‰SwiGLU ===
    def why_swiglu_in_llms():
        """
        å¤§è¯­è¨€æ¨¡å‹é€‰æ‹©SwiGLUçš„åŸå› ï¼š

        1. å®éªŒè¯æ˜æ•ˆæœæ›´å¥½ï¼š
           - åœ¨ç›¸åŒå‚æ•°é‡ä¸‹ï¼ŒSwiGLUé€šå¸¸è¡¨ç°æ›´å¥½
           - ç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­ä¼˜åŠ¿æ˜æ˜¾

        2. å…¼å®¹æ€§å¥½ï¼š
           - ä¸å…¶ä»–ä¼˜åŒ–æŠ€æœ¯ï¼ˆå¦‚Flash Attentionï¼‰å…¼å®¹
           - ä¸ä¼šå½±å“å…¶ä»–ç»„ä»¶çš„è®¾è®¡

        3. å®ç°ç®€å•ï¼š
           - ç›¸å¯¹å®¹æ˜“å®ç°å’Œä¼˜åŒ–
           - è®¡ç®—å¼€é”€å¯æ§
        """

# è¿è¡Œæ¯”è¾ƒ
compare_activations()
swiglu_advantages()
why_swiglu_in_llms()
```

### å¼ é‡å¹¶è¡Œåœ¨MLPä¸­çš„å®ç°

```python
def tensor_parallel_in_mlp():
    """MLPä¸­çš„å¼ é‡å¹¶è¡Œå®ç°"""

    class TensorParallelMLP(nn.Module):
        """æ”¯æŒå¼ é‡å¹¶è¡Œçš„MLP"""

        def __init__(self, hidden_size, intermediate_size, tp_size, tp_rank):
            super().__init__()
            self.tp_size = tp_size
            self.tp_rank = tp_rank

            # === è¾“å…¥å±‚å¼ é‡å¹¶è¡Œ ===
            # ç¬¬ä¸€ä¸ªçº¿æ€§å±‚æŒ‰è¾“å…¥ç»´åº¦åˆ‡åˆ†
            self.gate_proj = ColumnParallelLinear(
                input_size=hidden_size,
                output_size=intermediate_size,
                bias=False,
                gather_output=False,  # ä¸èšåˆè¾“å‡ºï¼Œä¿æŒåˆ‡åˆ†çŠ¶æ€
            )

            self.up_proj = ColumnParallelLinear(
                input_size=hidden_size,
                output_size=intermediate_size,
                bias=False,
                gather_output=False,
            )

            # === è¾“å‡ºå±‚å¼ é‡å¹¶è¡Œ ===
            # ç¬¬äºŒä¸ªçº¿æ€§å±‚æŒ‰è¾“å‡ºç»´åº¦åˆ‡åˆ†
            self.down_proj = RowParallelLinear(
                input_size=intermediate_size,
                output_size=hidden_size,
                bias=False,
                input_is_parallel=True,  # è¾“å…¥å·²ç»æ˜¯åˆ‡åˆ†çŠ¶æ€
            )

        def forward(self, x):
            # === å‰å‘è®¡ç®—ï¼ˆå¼ é‡å¹¶è¡Œï¼‰===

            # è¾“å…¥å±‚ï¼šæ¯ä¸ªGPUå¤„ç†ä¸€éƒ¨åˆ†è¾“å…¥ç‰¹å¾
            gate = self.gate_proj(x)  # [batch, seq, intermediate_size/tp_size]
            up = self.up_proj(x)       # [batch, seq, intermediate_size/tp_size]

            # æ¿€æ´»å‡½æ•°ï¼šåœ¨æœ¬åœ°è®¡ç®—
            intermediate = F.silu(gate) * up  # [batch, seq, intermediate_size/tp_size]

            # è¾“å‡ºå±‚ï¼šè‡ªåŠ¨èšåˆæ¥è‡ªæ‰€æœ‰GPUçš„ç»“æœ
            output = self.down_proj(intermediate)  # [batch, seq, hidden_size]

            return output

    # === å¼ é‡å¹¶è¡Œçš„æ•°æ®æµç¤ºä¾‹ ===
    """
    å‡è®¾é…ç½®ï¼š
    - hidden_size = 4096
    - intermediate_size = 11008
    - tp_size = 2 (2ä¸ªGPU)

    æ•°æ®æµï¼š
    è¾“å…¥: [batch, seq, 4096]

    GPU 0:
      gate_proj: [batch, seq, 4096] -> [batch, seq, 11008/2] = [batch, seq, 5504]
      up_proj:   [batch, seq, 4096] -> [batch, seq, 5504]

    GPU 1:
      gate_proj: [batch, seq, 4096] -> [batch, seq, 5504]
      up_proj:   [batch, seq, 4096] -> [batch, seq, 5504]

    æ¿€æ´»å‡½æ•°åï¼š
      GPU 0: [batch, seq, 5504]
      GPU 1: [batch, seq, 5504]

    è¾“å‡ºå±‚ï¼ˆè‡ªåŠ¨èšåˆï¼‰ï¼š
      down_proj: [batch, seq, 5504] + [batch, seq, 5504] -> [batch, seq, 4096]
    """

    def tensor_parallel_benefits():
        """å¼ é‡å¹¶è¡Œçš„ä¼˜åŠ¿åˆ†æ"""

        configs = [
            {'model_size': '7B', 'hidden_size': 4096, 'intermediate_size': 11008},
            {'model_size': '13B', 'hidden_size': 5120, 'intermediate_size': 13824},
            {'model_size': '70B', 'hidden_size': 8192, 'intermediate_size': 28672},
        ]

        tp_sizes = [1, 2, 4, 8]  # 1, 2, 4, 8ä¸ªGPU

        for config in configs:
            hidden_size = config['hidden_size']
            intermediate_size = config['intermediate_size']

            print(f"\n{config['model_size']} æ¨¡å‹:")
            print(f"  éšè—å±‚å¤§å°: {hidden_size}")
            print(f"  ä¸­é—´å±‚å¤§å°: {intermediate_size}")

            for tp_size in tp_sizes:
                # è®¡ç®—æ¯ä¸ªGPUçš„å‚æ•°é‡
                gate_params_per_gpu = hidden_size * (intermediate_size // tp_size)
                up_params_per_gpu = hidden_size * (intermediate_size // tp_size)
                down_params_per_gpu = (intermediate_size // tp_size) * hidden_size

                total_per_gpu = gate_params_per_gpu + up_params_per_gpu + down_params_per_gpu

                print(f"    {tp_size} GPU: æ¯GPU {total_per_gpu:,} å‚æ•°")

# è¿è¡Œåˆ†æ
tensor_parallel_in_mlp()
```

---

## âš–ï¸ RMSNormï¼šå±‚å½’ä¸€åŒ–çš„å®ç°

### RMSNormçš„è®¾è®¡ç†å¿µ

RMSNormï¼ˆRoot Mean Square Normalizationï¼‰æ˜¯LayerNormçš„ä¼˜åŒ–ç‰ˆæœ¬ï¼Œä¸“é—¨ä¸ºå¤§è¯­è¨€æ¨¡å‹è®¾è®¡ã€‚

### RMSNormçš„å®Œæ•´å®ç°

```python
class RMSNorm(nn.Module):
    """å‡æ–¹æ ¹å½’ä¸€åŒ–"""

    def __init__(self, hidden_size: int, eps: float = 1e-6):
        """
        Args:
            hidden_size: éšè—å±‚å¤§å°
            eps: æ•°å€¼ç¨³å®šæ€§å‚æ•°ï¼Œé˜²æ­¢é™¤é›¶
        """
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(hidden_size))

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """
        RMSNormå…¬å¼ï¼š
        RMSNorm(x) = x / sqrt(mean(xÂ²) + eps) * weight

        ä¸LayerNormçš„åŒºåˆ«ï¼š
        - ä¸å‡å»å‡å€¼ï¼ˆå»ä¸­å¿ƒåŒ–ï¼‰
        - åªç”¨å‡æ–¹æ ¹ï¼Œä¸ç”¨æ ‡å‡†å·®
        - è®¡ç®—é‡æ›´å°
        """

        # è®¡ç®—å‡æ–¹æ ¹
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)

        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        rms = variance.rsqrt()  # å‡æ–¹æ ¹å€’æ•°ï¼Œç­‰ä»·äº 1/sqrt(variance)

        # åº”ç”¨å½’ä¸€åŒ–
        hidden_states = hidden_states * rms

        # åº”ç”¨æƒé‡
        hidden_states = hidden_states.to(input_dtype)
        return self.weight * hidden_states

def rmsnorm_vs_layernorm():
    """æ¯”è¾ƒRMSNormå’ŒLayerNormçš„æ€§èƒ½"""

    class LayerNorm(nn.Module):
        """æ ‡å‡†LayerNormå®ç°"""
        def __init__(self, hidden_size, eps=1e-6):
            super().__init__()
            self.eps = eps
            self.weight = nn.Parameter(torch.ones(hidden_size))
            self.bias = nn.Parameter(torch.zeros(hidden_size))

        def forward(self, x):
            mean = x.mean(-1, keepdim=True)
            var = x.var(-1, keepdim=True, unbiased=False)
            x_norm = (x - mean) / torch.sqrt(var + self.eps)
            return self.weight * x_norm + self.bias

    def compare_performance():
        """æ€§èƒ½å’Œæ•ˆæœå¯¹æ¯”"""

        # æµ‹è¯•æ•°æ®
        batch_size, seq_len, hidden_size = 32, 512, 4096
        x = torch.randn(batch_size, seq_len, hidden_size)

        # åˆ›å»ºå½’ä¸€åŒ–å±‚
        rms_norm = RMSNorm(hidden_size)
        layer_norm = LayerNorm(hidden_size)

        # æ€§èƒ½æµ‹è¯•
        import time

        # RMSNormæ€§èƒ½
        start_time = time.time()
        for _ in range(100):
            _ = rms_norm(x)
        rms_time = time.time() - start_time

        # LayerNormæ€§èƒ½
        start_time = time.time()
        for _ in range(100):
            _ = layer_norm(x)
        ln_time = time.time() - start_time

        # è®¡ç®—ç›¸å¯¹è¯¯å·®
        rms_output = rms_norm(x)
        ln_output = layer_norm(x)
        relative_error = torch.abs(rms_output - ln_output).mean().item()

        print(f"æ€§èƒ½å¯¹æ¯” (100æ¬¡å‰å‘ä¼ æ’­):")
        print(f"  RMSNorm: {rms_time:.4f}s")
        print(f"  LayerNorm: {ln_time:.4f}s")
        print(f"  åŠ é€Ÿæ¯”: {ln_time/rms_time:.2f}x")
        print(f"  ç›¸å¯¹è¯¯å·®: {relative_error:.6f}")

    def analyze_differences():
        """åˆ†æä¸¤ç§å½’ä¸€åŒ–çš„å·®å¼‚"""

        """
        RMSNorm vs LayerNorm çš„è¯¦ç»†å¯¹æ¯”ï¼š

        1. æ•°å­¦å…¬å¼ï¼š
           LayerNorm: (x - Î¼) / âˆš(ÏƒÂ² + Îµ) * Î³ + Î²
           RMSNorm:   x / âˆš(Î¼(xÂ²) + Îµ) * Î³

        2. å…³é”®å·®å¼‚ï¼š
           a) å»ä¸­å¿ƒåŒ–ï¼šLayerNormå‡å»å‡å€¼ï¼ŒRMSNormä¸å‡
           b) è®¡ç®—é‡ï¼šRMSNormæ›´ç®€å•
           c) å‚æ•°ï¼šRMSNormæ²¡æœ‰åç½®é¡¹Î²
           d) æ•ˆæœï¼šåœ¨å¤§æ¨¡å‹ä¸­å·®å¼‚å¾ˆå°

        3. RMSNormçš„ä¼˜åŠ¿ï¼š
           a) è®¡ç®—é‡å‡å°‘çº¦25-30%
           b) ä¸éœ€è¦å­˜å‚¨å‡å€¼ç»Ÿè®¡
           c) æ›´é€‚åˆå¤§æ¨¡å‹æ¨ç†
           d) æ•°å€¼ç¨³å®šæ€§æ›´å¥½

        4. ä¸ºä»€ä¹ˆå¤§æ¨¡å‹é€‰æ‹©RMSNormï¼š
           a) æ€§èƒ½æå‡æ˜¾è‘—
           b) ç²¾åº¦æŸå¤±å¾ˆå°
           c) å®ç°ç®€å•
           d) ä¸å…¶ä»–ä¼˜åŒ–æŠ€æœ¯å…¼å®¹
        """

# è¿è¡Œå¯¹æ¯”
compare_performance()
analyze_differences()
```

---

## ğŸ”¤ï¸ è¯åµŒå…¥å’Œè¾“å‡ºå¤´çš„å®ç°

### è¯åµŒå…¥çš„å¼ é‡å¹¶è¡Œ

```python
class VocabParallelEmbedding(nn.Module):
    """æ”¯æŒå¼ é‡å¹¶è¡Œçš„è¯åµŒå…¥å±‚"""

    def __init__(self, num_embeddings: int, embedding_dim: int, tp_dim: int = 0, tp_size: int = 1):
        """
        Args:
            num_embeddings: è¯è¡¨å¤§å°
            embedding_dim: åµŒå…¥ç»´åº¦
            tp_dim: å¼ é‡å¹¶è¡Œç»´åº¦ (0=è¯è¡¨ç»´åº¦, 1=åµŒå…¥ç»´åº¦)
            tp_size: å¼ é‡å¹¶è¡Œå¤§å°
        """
        super().__init__()
        self.tp_size = tp_size
        self.tp_dim = tp_dim

        # æ£€æŸ¥å¹¶è¡Œç»´åº¦æ˜¯å¦å¯æ•´é™¤
        if tp_dim == 0:  # è¯è¡¨ç»´åº¦åˆ‡åˆ†
            assert num_embeddings % tp_size == 0
            self.num_embeddings_per_partition = num_embeddings // tp_size
            self.embedding_dim = embedding_dim
        else:  # åµŒå…¥ç»´åº¦åˆ‡åˆ†
            assert embedding_dim % tp_size == 0
            self.num_embeddings_per_partition = num_embeddings
            self.embedding_dim = embedding_dim // tp_size

        # åˆ›å»ºåµŒå…¥å±‚
        self.weight = nn.Parameter(
            torch.empty(self.num_embeddings_per_partition, self.embedding_dim)
        )
        nn.init.normal_(self.weight, mean=0, std=0.02)

    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:
        """å‰å‘è®¡ç®—"""

        if self.tp_dim == 0:
            # è¯è¡¨ç»´åº¦åˆ‡åˆ†ï¼šæ¯ä¸ªGPUå¤„ç†ä¸€éƒ¨åˆ†è¯è¡¨
            # éœ€è¦è°ƒæ•´token IDs
            input_ids = input_ids % self.num_embeddings_per_partition
            embed = F.embedding(input_ids, self.weight)
        else:
            # åµŒå…¥ç»´åº¦åˆ‡åˆ†ï¼šç›´æ¥åµŒå…¥
            embed = F.embedding(input_ids, self.weight)

        return embed

class ParallelLMHead(nn.Module):
    """æ”¯æŒå¼ é‡å¹¶è¡Œçš„è¾“å‡ºå¤´"""

    def __init__(self, hidden_size: int, vocab_size: int, bias: bool = False,
                 tp_dim: int = 0, tp_size: int = 1):
        super().__init__()
        self.tp_size = tp_size
        self.tp_dim = tp_dim

        if tp_dim == 0:  # è¾“å‡ºç»´åº¦åˆ‡åˆ†
            assert vocab_size % tp_size == 0
            self.vocab_size_per_partition = vocab_size // tp_size
            self.hidden_size = hidden_size
        else:  # éšè—ç»´åº¦åˆ‡åˆ†
            assert hidden_size % tp_size == 0
            self.vocab_size_per_partition = vocab_size
            self.hidden_size = hidden_size // tp_size

        self.weight = nn.Parameter(
            torch.empty(self.vocab_size_per_partition, self.hidden_size)
        )
        if bias:
            self.bias = nn.Parameter(torch.zeros(self.vocab_size_per_partition))
        else:
            self.register_parameter("bias", None)

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """å‰å‘è®¡ç®—"""
        return F.linear(hidden_states, self.weight, self.bias)

def embedding_layer_optimization():
    """åµŒå…¥å±‚çš„ä¼˜åŒ–æŠ€æœ¯"""

    class OptimizedEmbedding(nn.Module):
        """ä¼˜åŒ–çš„åµŒå…¥å±‚å®ç°"""

        def __init__(self, num_embeddings, embedding_dim):
            super().__init__()
            self.num_embeddings = num_embeddings
            self.embedding_dim = embedding_dim

            # ä½¿ç”¨16ä½ç²¾åº¦å­˜å‚¨ä»¥èŠ‚çœå†…å­˜
            self.weight = nn.Parameter(
                torch.empty(num_embeddings, embedding_dim, dtype=torch.float16)
            )

            # ç¼“å­˜æœºåˆ¶
            self.embedding_cache = {}
            self.cache_size = 10000

            # é¢„è®¡ç®—å¸¸ç”¨çš„tokenåµŒå…¥
            self.common_tokens = set(range(100))  # å‡è®¾å‰100ä¸ªæ˜¯å¸¸ç”¨token

        def forward(self, input_ids):
            """å¸¦ç¼“å­˜çš„åµŒå…¥è®¡ç®—"""

            # æ£€æŸ¥æ˜¯å¦å¯ä»¥å‘½ä¸­ç¼“å­˜
            cache_key = tuple(input_ids.tolist())
            if cache_key in self.embedding_cache:
                return self.embedding_cache[cache_key]

            # è®¡ç®—åµŒå…¥
            embeddings = F.embedding(input_ids, self.weight)

            # ç¼“å­˜ç»“æœ
            if len(self.embedding_cache) < self.cache_size:
                self.embedding_cache[cache_key] = embeddings

            return embeddings

    def embedding_memory_analysis():
        """åµŒå…¥å±‚å†…å­˜ä½¿ç”¨åˆ†æ"""

        configs = [
            {'vocab_size': 50000, 'embed_dim': 1024, 'name': 'å°å‹æ¨¡å‹'},
            {'vocab_size': 100000, 'embed_dim': 4096, 'name': 'ä¸­å‹æ¨¡å‹'},
            {'vocab_size': 200000, 'embed_dim': 8192, 'name': 'å¤§å‹æ¨¡å‹'},
        ]

        for config in configs:
            vocab_size = config['vocab_size']
            embed_dim = config['embed_dim']

            # 32ä½æµ®ç‚¹æ•°å†…å­˜
            memory_32bit = vocab_size * embed_dim * 4  # bytes
            # 16ä½æµ®ç‚¹æ•°å†…å­˜
            memory_16bit = vocab_size * embed_dim * 2  # bytes

            print(f"{config['name']}:")
            print(f"  è¯è¡¨å¤§å°: {vocab_size:,}")
            print(f"  åµŒå…¥ç»´åº¦: {embed_dim}")
            print(f"  32ä½å†…å­˜: {memory_32bit / (1024**2):.1f} MB")
            print(f"  16ä½å†…å­˜: {memory_16bit / (1024**2):.1f} MB")
            print(f"  èŠ‚çœ: {((memory_32bit - memory_16bit) / memory_32bit * 100):.1f}%")
            print()

# è¿è¡Œåˆ†æ
embedding_memory_analysis()
```

---

## ğŸƒâ€â™‚ï¸ æ¨¡å‹åŠ è½½å’Œåˆå§‹åŒ–

### æ¨¡å‹æƒé‡çš„åŠ è½½æœºåˆ¶

```python
def model_loading_implementation():
    """æ¨¡å‹åŠ è½½çš„è¯¦ç»†å®ç°"""

    def load_model_weights(model, model_path, device="cuda"):
        """åŠ è½½æ¨¡å‹æƒé‡çš„ä¼˜åŒ–å®ç°"""

        # === æƒé‡æ–‡ä»¶è·¯å¾„ ===
        weight_file = os.path.join(model_path, "pytorch_model.bin")
        config_file = os.path.join(model_path, "config.json")

        if not os.path.exists(weight_file):
            raise FileNotFoundError(f"æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {weight_file}")

        # === æ£€æŸ¥æ¨¡å‹é…ç½®åŒ¹é… ===
        model_config = AutoConfig.from_pretrained(model_path)
        if not check_config_compatibility(model.config, model_config):
            raise ValueError("æ¨¡å‹é…ç½®ä¸åŒ¹é…")

        # === å†…å­˜å‹å¥½çš„æƒé‡åŠ è½½ ===
        print(f"å¼€å§‹åŠ è½½æ¨¡å‹æƒé‡: {weight_file}")

        # ä½¿ç”¨CPUåŠ è½½ä»¥é¿å…GPUå†…å­˜ä¸è¶³
        with open(weight_file, 'rb') as f:
            # ä½¿ç”¨pickleåŠ è½½ï¼ˆæ¯”torch.loadæ›´å¯æ§ï¼‰
            state_dict = pickle.load(f)

        # === æƒé‡é¢„å¤„ç† ===
        state_dict = preprocess_weights(state_dict)

        # === å¼ é‡å¹¶è¡Œå¤„ç† ===
        if get_world_size() > 1:
            state_dict = shard_weights_for_tensor_parallel(state_dict)

        # === åŠ è½½åˆ°æ¨¡å‹ ===
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)

        # === æƒé‡åŠ è½½æŠ¥å‘Š ===
        if missing_keys:
            print(f"ç¼ºå¤±çš„æƒé‡: {missing_keys}")
        if unexpected_keys:
            print(f"å¤šä½™çš„æƒé‡: {unexpected_keys}")

        # === æƒé‡ç²¾åº¦è½¬æ¢ ===
        convert_weights_to_target_precision(model)

        # === æ¨¡å‹éªŒè¯ ===
        validate_loaded_model(model)

        print(f"æ¨¡å‹æƒé‡åŠ è½½å®Œæˆï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

        return model

    def check_config_compatibility(config1, config2):
        """æ£€æŸ¥ä¸¤ä¸ªé…ç½®çš„å…¼å®¹æ€§"""
        compatible_keys = [
            'hidden_size', 'num_attention_heads', 'num_hidden_layers',
            'vocab_size', 'max_position_embeddings', 'rms_norm_eps'
        ]

        for key in compatible_keys:
            if getattr(config1, key, None) != getattr(config2, key, None):
                print(f"é…ç½®ä¸åŒ¹é…: {key} - {getattr(config1, key)} vs {getattr(config2, key)}")
                return False
        return True

    def preprocess_weights(state_dict):
        """æƒé‡é¢„å¤„ç†"""

        processed_dict = {}

        for key, weight in state_dict.items():
            # === ç²¾åº¦è½¬æ¢ ===
            if weight.dtype == torch.float64:
                weight = weight.float()
            elif weight.dtype == torch.bfloat16:
                weight = weight.float()  # è½¬æ¢ä¸ºfloat32ä»¥è·å¾—æ›´å¥½ç²¾åº¦

            # === å†…å­˜ä¼˜åŒ– ===
            if weight.is_sparse:
                weight = weight.to_dense()

            # === æƒé‡åˆå¹¶ï¼ˆå¦‚æœéœ€è¦ï¼‰===
            # æŸäº›æƒ…å†µä¸‹éœ€è¦åˆå¹¶åˆ†ç‰‡çš„æƒé‡
            if 'weight' in key and any(x in key for x in ['q_proj', 'k_proj', 'v_proj']):
                weight = merge_qkv_weights(key, weight, state_dict)

            processed_dict[key] = weight

        return processed_dict

    def shard_weights_for_tensor_parallel(state_dict):
        """ä¸ºå¼ é‡å¹¶è¡Œåˆ†ç‰‡æƒé‡"""

        world_size = get_world_size()
        rank = get_rank()

        sharded_dict = {}

        for key, weight in state_dict.items():
            if '.weight' in key and any(x in key for x in ['gate_proj', 'up_proj', 'down_proj']):
                # MLPå±‚çš„æƒé‡éœ€è¦åˆ†ç‰‡
                if 'down_proj' in key:
                    # è¾“å‡ºå±‚æŒ‰è¾“å…¥ç»´åº¦åˆ†ç‰‡
                    chunk_size = weight.size(1) // world_size
                    weight = weight[:, rank * chunk_size:(rank + 1) * chunk_size]
                else:
                    # è¾“å…¥å±‚æŒ‰è¾“å‡ºç»´åº¦åˆ†ç‰‡
                    chunk_size = weight.size(0) // world_size
                    weight = weight[rank * chunk_size:(rank + 1) * chunk_size, :]

            elif '.weight' in key and 'embed_tokens' in key:
                # è¯åµŒå…¥æŒ‰è¯è¡¨ç»´åº¦åˆ†ç‰‡
                chunk_size = weight.size(0) // world_size
                weight = weight[rank * chunk_size:(rank + 1) * chunk_size, :]

            sharded_dict[key] = weight

        return sharded_dict

def model_initialization_strategy():
    """æ¨¡å‹åˆå§‹åŒ–ç­–ç•¥"""

    def initialize_model_weights(module):
        """æ¨¡å‹æƒé‡åˆå§‹åŒ–ç­–ç•¥"""

        if isinstance(module, nn.Linear):
            # çº¿æ€§å±‚åˆå§‹åŒ–
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                nn.init.zeros_(module.bias)

        elif isinstance(module, nn.Embedding):
            # è¯åµŒå…¥åˆå§‹åŒ–
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.padding_idx is not None:
                with torch.no_grad():
                    module.weight[module.padding_idx].fill_(0)

        elif isinstance(module, RMSNorm):
            # RMSNormåˆå§‹åŒ–
            nn.init.ones_(module.weight)

    def weight_initialization_analysis():
        """æƒé‡åˆå§‹åŒ–çš„åˆ†æ"""

        """
        åˆå§‹åŒ–ç­–ç•¥çš„é€‰æ‹©è€ƒè™‘ï¼š

        1. Xavieråˆå§‹åŒ– (Glorot):
           - é€‚ç”¨äºsigmoid/tanhæ¿€æ´»å‡½æ•°
           - æ–¹å·®: 2 / (fan_in + fan_out)

        2. Heåˆå§‹åŒ–:
           - é€‚ç”¨äºReLUæ¿€æ´»å‡½æ•°
           - æ–¹å·®: 2 / fan_in

        3. æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–:
           - é€‚ç”¨äºæ·±åº¦ç½‘ç»œ
           - æ–¹å·®éœ€è¦ç²¾å¿ƒè°ƒèŠ‚

        4. å¤§è¯­è¨€æ¨¡å‹çš„ç‰¹æ®Šè€ƒè™‘ï¼š
           - æ®‹å·®è¿æ¥çš„å­˜åœ¨
           - LayerNormçš„ç¨³å®šåŒ–ä½œç”¨
           - æ·±åº¦å¸¦æ¥çš„æ¢¯åº¦é—®é¢˜

        Qwen3çš„é€‰æ‹©ï¼šstd=0.02çš„æ­£æ€åˆ†å¸ƒ
        - åŸºäºå¤§é‡å®éªŒç»“æœ
        - é€‚åˆæ·±åº¦Transformeræ¶æ„
        - ä¸å…¶ä»–è¶…å‚æ•°é…åˆè‰¯å¥½
        """

# è¿è¡Œåˆ†æ
model_loading_implementation()
model_initialization_strategy()
```

---

## ğŸ“Š æ¨¡å‹å±‚çš„æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

### CUDA Graphåœ¨æ¨¡å‹å±‚çš„åº”ç”¨

```python
def cudagraph_optimization_in_model():
    """æ¨¡å‹å±‚çš„CUDA Graphä¼˜åŒ–"""

    class GraphOptimizedModel(nn.Module):
        """ä½¿ç”¨CUDA Graphä¼˜åŒ–çš„æ¨¡å‹"""

        def __init__(self, base_model):
            super().__init__()
            self.base_model = base_model
            self.static_inputs = None
            self.graph = None

        def capture_graph(self, sample_batch, sample_seq_len):
            """æ•è·CUDA Graph"""
            print("å¼€å§‹æ•è·æ¨¡å‹CUDA Graph...")

            # å‡†å¤‡é™æ€è¾“å…¥
            self.static_inputs = {
                'input_ids': torch.zeros(sample_batch, sample_seq_len,
                                        dtype=torch.long, device="cuda"),
                'position_ids': torch.zeros(sample_batch, sample_seq_len,
                                            dtype=torch.long, device="cuda"),
                'slot_mapping': torch.zeros(sample_batch, sample_seq_len,
                                             dtype=torch.long, device="cuda"),
            }

            # ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆwarmupï¼‰
            with torch.no_grad():
                output = self.base_model(**self.static_inputs)

            # æ•è·è®¡ç®—å›¾
            self.graph = torch.cuda.CUDAGraph()
            with torch.cuda.graph(self.graph):
                with torch.no_grad():
                    self.graph_output = self.base_model(**self.static_inputs)

            print("CUDA Graphæ•è·å®Œæˆ")

        def forward(self, input_ids, position_ids, slot_mapping):
            """ä½¿ç”¨CUDA Graphçš„å‰å‘è®¡ç®—"""

            # æ£€æŸ¥è¾“å…¥å¤§å°æ˜¯å¦åŒ¹é…
            if (input_ids.shape[0] <= self.static_inputs['input_ids'].shape[0] and
                input_ids.shape[1] <= self.static_inputs['input_ids'].shape[1]):

                # ä½¿ç”¨CUDA Graph
                # å¤åˆ¶æ•°æ®åˆ°é™æ€è¾“å…¥ç¼“å†²åŒº
                self.static_inputs['input_ids'][:input_ids.shape[0], :input_ids.shape[1]] = input_ids
                self.static_inputs['position_ids'][:position_ids.shape[0], :position_ids.shape[1]] = position_ids
                self.static_inputs['slot_mapping'][:slot_mapping.shape[0], :slot_mapping.shape[1]] = slot_mapping

                return self.graph_output[:input_ids.shape[0], :input_ids.shape[1], :]
            else:
                # è¾“å…¥å¤ªå¤§ï¼Œä½¿ç”¨æ™®é€šæ¨¡å¼
                return self.base_model(input_ids, position_ids, slot_mapping)

    def cudagraph_benefits():
        """CUDA Graphçš„ä¼˜åŠ¿åˆ†æ"""

        """
        CUDA Graphåœ¨æ¨¡å‹å±‚ä¸­çš„åº”ç”¨ä¼˜åŠ¿ï¼š

        1. æ¶ˆé™¤CPU-GPUåŒæ­¥ï¼š
           - å‡å°‘kernelå¯åŠ¨å¼€é”€
           - æé«˜GPUåˆ©ç”¨ç‡

        2. å†…å­˜ä¼˜åŒ–ï¼š
           - é¢„åˆ†é…å†…å­˜
           - å‡å°‘å†…å­˜åˆ†é…å¼€é”€

        3. æ‰§è¡Œä¼˜åŒ–ï¼š
           - ç®—å­èåˆ
           - å‡å°‘ä¸­é—´ç»“æœå­˜å‚¨

        4. ç¼ºç‚¹ï¼š
           - è¾“å…¥å¤§å°å›ºå®š
           - éœ€è¦å¤šä¸ªGraphæ”¯æŒä¸åŒå¤§å°
           - è°ƒè¯•å›°éš¾
        """

# è¿è¡Œåˆ†æ
cudagraph_optimization_in_model()
```

---

## ğŸ’¡ æœ¬ç« æ€»ç»“

### å…³é”®è¦ç‚¹å›é¡¾

1. **Qwen3æ¨¡å‹çš„å®Œæ•´æ¶æ„**ï¼š
   - æ¨¡å—åŒ–è®¾è®¡ï¼Œä¾¿äºæ‰©å±•å’Œç»´æŠ¤
   - æ”¯æŒå¼ é‡å¹¶è¡Œï¼Œæå‡è®¡ç®—æ•ˆç‡
   - ä¼˜åŒ–çš„å¤§æ¨¡å‹æ¶æ„è®¾è®¡

2. **Transformerå±‚çš„å®ç°**ï¼š
   - Pre-Normç»“æ„ä¿è¯è®­ç»ƒç¨³å®š
   - æ®‹å·®è¿æ¥ä¿ƒè¿›æ¢¯åº¦æµåŠ¨
   - æ¨¡å—åŒ–çš„ç»„ä»¶è®¾è®¡

3. **æ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜åŒ–**ï¼š
   - åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰å‡å°‘è®¡ç®—é‡
   - Flash Attentionä¼˜åŒ–å†…å­˜ä½¿ç”¨
   - æ—‹è½¬ä½ç½®ç¼–ç å¤„ç†ä½ç½®ä¿¡æ¯

4. **å‰é¦ˆç½‘ç»œçš„è®¾è®¡**ï¼š
   - SwiGLUæ¿€æ´»å‡½æ•°æå‡è¡¨è¾¾èƒ½åŠ›
   - å¼ é‡å¹¶è¡Œæ”¯æŒå¤šGPUè®¡ç®—
   - é«˜æ•ˆçš„é—¨æ§æœºåˆ¶å®ç°

5. **åŸºç¡€å±‚çš„ä¼˜åŒ–**ï¼š
   - RMSNormå‡å°‘è®¡ç®—é‡
   - è¯åµŒå…¥å’Œè¾“å‡ºå¤´çš„å¹¶è¡ŒåŒ–
   - ç²¾åº¦ä¼˜åŒ–å’Œå†…å­˜ç®¡ç†

### æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯æ€»ç»“

1. **å†…å­˜ä¼˜åŒ–**ï¼š
   - KV Cacheç®¡ç†
   - æ··åˆç²¾åº¦è®¡ç®—
   - å¼ é‡å¹¶è¡Œ

2. **è®¡ç®—ä¼˜åŒ–**ï¼š
   - CUDA Graph
   - Flash Attention
   - æ ¸å‡½æ•°èåˆ

3. **æ¶æ„ä¼˜åŒ–**ï¼š
   - åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›
   - Pre-Normç»“æ„
   - SwiGLUæ¿€æ´»å‡½æ•°

### å®ç°ç»éªŒæ€»ç»“

1. **æ¨¡å—åŒ–è®¾è®¡çš„é‡è¦æ€§**ï¼š
   - ä¾¿äºè°ƒè¯•å’Œä¿®æ”¹
   - æ”¯æŒç»„ä»¶æ›¿æ¢
   - æé«˜ä»£ç å¯ç»´æŠ¤æ€§

2. **æ€§èƒ½ä¼˜åŒ–çš„æƒè¡¡**ï¼š
   - å†…å­˜vsè®¡ç®—
   - ç²¾åº¦vsé€Ÿåº¦
   - å¤æ‚åº¦vsæ•ˆæœ

3. **å¼ é‡å¹¶è¡Œçš„å®ç°ç»†èŠ‚**ï¼š
   - æƒé‡åˆ†ç‰‡ç­–ç•¥
   - é€šä¿¡ä¼˜åŒ–
   - æ•°å€¼ç¨³å®šæ€§

### ä¸‹ä¸€æ­¥é¢„å‘Š

ä¸‹ä¸€ç« æˆ‘ä»¬å°†æ·±å…¥åˆ†æã€Šç®—å­ä¼˜åŒ–å±‚ã€‹ï¼ŒåŒ…æ‹¬ï¼š
- æ¿€æ´»å‡½æ•°çš„é«˜æ•ˆå®ç°
- çº¿æ€§å±‚çš„å¹¶è¡Œä¼˜åŒ–
- é‡‡æ ·ç®—æ³•çš„å¿«é€Ÿå®ç°
- Tritonæ ¸å‡½æ•°ä¼˜åŒ–
- å†…å­˜ç®¡ç†å·¥å…·

ç°åœ¨ä½ å·²ç»å¯¹nano-vLLMçš„æ¨¡å‹å®ç°å±‚æœ‰äº†æ·±å…¥çš„ç†è§£ï¼Œå‡†å¤‡å¥½è¿›å…¥ç®—å­å±‚çš„åˆ†æäº†å—ï¼ŸğŸš€